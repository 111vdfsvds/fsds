{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Data\n",
    "\n",
    "## The importance of exploration\n",
    "\n",
    "One of the first things that we do when working with any new data set is to familiarise ourselves with it. There are a _huge_ number of ways to do this, but there are no shortcuts to:\n",
    "* Reading about the data (how it was collected, what the sample size was, etc.)\n",
    "* Reviewing any accompanying metadata (data about the data, column specs, etc.)\n",
    "* Looking at the data itself at the row- and column-levels\n",
    "* Producing descriptive statistics \n",
    "* Visualising the data using plots \n",
    "In fact, you should use _all_ of these together to really understand where the data came from, how it was handled, and whether there are gaps or other problems. If you're wondering which comes first, I've always liked this approach: _start with a chart_. We're _not_ going to do that here because, first, I want you to get a handle on pandas itself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Revisiting Last Week's Data with Pandas\n",
    "\n",
    "Pandas stands for 'Python Data Analysis Library'; it is designed to provide data scientists working in Python with a set of powerful tools to load, transform, and process large-ish data sets. As a result, it has become something of a *de facto* standard for online tutorials and many of the lessons that you can find online will make use of pandas at some point.\n",
    "\n",
    "You will want to bookmark [the documentation](http://pandas.pydata.org/pandas-docs/stable/) since you will undoubtedly need to refer to it fairly regularly. _Note_: this link is to the most recent release. Over time there will be updates published and you _may_ find that you no longer have the most up-to-date version. If you find that you are now using an older version of pandas then you'll need to track down the _specific_ version of the documentation that you need from the [home page](http://pandas.pydata.org).\n",
    "\n",
    "You can always check what version you have installed like this:\n",
    "```python\n",
    "import pandas as pd\n",
    "print pd.__version__\n",
    "```\n",
    "*Note*: this approach isn't guaranteed to work with _every_ package, but it will work with most of them. Remember that variables and methods starting and ending with '`__`' are **private** and any interaction with them should be approached very, very carefully.\n",
    "\n",
    "Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "help(pd.DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On second thought, let's never do that again. Well, at least not _that_ way! You'll have noticed that the help documentation for the DataFrame is not just a bit longer than anything we've seen before, it's massively longer. There's probably quite a lot of intimidating terminology in there too... Right from the start we get things like \"Two-dimensional size-mutable, potentially heterogeneous tabular data structure with labeled axes (rows and columns).\" \n",
    "\n",
    "## You've already invented pandas!\n",
    "\n",
    "Here's the thing: in the [last notebook](https://raw.githubusercontent.com/kingsgeocomp/geocomputation/master/Practical-4-Functions%2C%20Packages%20and%20Methods.ipynb) we came close to writing something like pandas from scratch. That's because pandas takes a column-view of data in the same way that our Dictionary-of-Lists did, it's just that it's got a lot more features than our 'simple' tool does. That's why the documentation is so much more forbidding and why pandas is so much more powerful.\n",
    "\n",
    "But at its heart, a pandas data frame ('df' for short) is just a collection of data series (i.e. columns) with an index. Each Series is like one of our column-lists from the last notebook. And the df is like the overarching dictionary that held the collection of data series (serieses?) together. OK? You've seen this before. So you already _know_ what's going on, or at least have an _analogy_ that you can use to make sense of it:\n",
    "```python\n",
    "myDataFrame = [\n",
    "    '<column name 1>': <Series 1>,\n",
    "    '<column name 2>': <Series 2>,\n",
    "    '<column name 3>': <Series 3>\n",
    "]\n",
    "```\n",
    "\n",
    "Let's try using pandas with last week's data!\n",
    "\n",
    "## Load a Remote CSV file in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('http://www.reades.com/CitiesWithWikipediaData.csv') # Read the remote CSV file\n",
    "print(type(df)) # What is 'df'?\n",
    "df.head() # What did we get?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check it out!\n",
    "\n",
    "Instead of having to write a 'readRemoteCSV' function and then manually create a Dictionary-of-Lists from that remote file, we just told pandas to read it for us and it automagically converted it to a data structure that we could view. You'll notice that it even figured out where the column names were. \n",
    "\n",
    "All we did with `df.head()` was to ask it to print out the first 5 rows of data. If we wanted to only see the first two rows it would be `df.head(2)`. This is pretty handy, right? \n",
    "\n",
    "Also, it deliberately mimics the Unix command-line tool `head` (i.e. `head -5 CitiesWithWikipediaData.csv`). So you've learned two tools for the price of one!\n",
    "\n",
    "## Describing Numerical Data in Pandas\n",
    "\n",
    "Let's try a few more things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll probably have seen a fairly prominent warning (\"Invalid value encountered in percentile\"), and if you look closely you'll see that there are some fields that report '`NaN`' in some of the rows. NaN is 'Not A Number', and if you were to look at the original data you'd see that we're missing the Area and Density for some of the cities in the data set -- pandas uses NaN because it wants us to know that there was _no_ data there. \n",
    "\n",
    "**_Why wouldn't we want to pandas to default to `0` for 'no data'?_**\n",
    "\n",
    "### Recap\n",
    "\n",
    "Let's take a step back for a second to appreciate where we're at: in one function call we loaded a remote data set, parsed it to turn it into data, and then produced a 7-figure summary for _most_ of the columns in the data! That's a bit faster than trying to do it all in Excel, right?\n",
    "\n",
    "So, just by calling `describe`...\n",
    "1. We've asked Python to describe the data frame and it has returned a set of columns with descriptive metrics for each.\n",
    "2. Note what is _missing_ from this list: where are 'Name', 'MetroArea', and a couple of the other columns? Can you think why they weren't reported in the descriptives?\n",
    "\n",
    "Of course, maybe you don't want the report for all columns, maybe you're just interested in one column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.Population.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have the same information, but only for the Population column. We have to do this a _little_ differently because describing the DataFrame does some clever formatting when you're using Jupyter notebooks, and describing a Series requires us to print out the result. Also notice that `dtype` at the end: that tells us the _data type_ is a 64-bit float. You can have strings, floats, integers, booleans, etc. in a DataFrame.\n",
    "\n",
    "But the really crucial thing is that this introduces _one_ of the two ways that we access a Series in pandas: `<data frame>.<series name>.method`. So we could get similar information on the Name column with:\n",
    "```python\n",
    "df.Name.describe()\n",
    "```\n",
    "And so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print df.Name.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that describing a text column gives us an 'object' data type because a String is a complex object, not a simple float or int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"The mean population of the cities in the data is: \" + str(df.Population.mean())\n",
    "print \"The median population of the cities in the data is: \" + str(df.Population.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Challenge for You!\n",
    "\n",
    "If all of this has made some kind of sense, why not spend a four of five minutes (at most) exploring the CSV data from last week using pandas. Try the following:\n",
    "\n",
    "1. What's the standard deviation of the population? _[Hint: look in the help for a Series, or Google it.]_\n",
    "2. What's the highest rank (i.e. smallest city) in the data set? _[Hint: you are looking for the maximum value in the id column.]_\n",
    "\n",
    "Use the coding block below for your exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And notice to that we can ask the data frame to quickly work out a derived variable (such as the mean) just by asking the Series to do the work for us: `<data frame>.<series>.method()`. You might want to have a [look at the documentation](http://pandas.pydata.org/pandas-docs/stable/api.html#series) to see what other methods are available for a data series. It's rather a long list, but most of your descriptive stats are here in [Cumulative / Descriptive Stats](http://pandas.pydata.org/pandas-docs/stable/api.html#computations-descriptive-stats) as are things about [strings](http://pandas.pydata.org/pandas-docs/stable/api.html#string-handling), [categorical data](http://pandas.pydata.org/pandas-docs/stable/api.html#categorical), and [plotting](http://pandas.pydata.org/pandas-docs/stable/api.html#plotting).\n",
    "\n",
    "## Quick and Dirty Plotting\n",
    "\n",
    "No, this isn't about this man: ![Niccolo Machiavelli](http://a5.files.biography.com/image/upload/c_fill,cs_srgb,dpr_1.0,g_face,h_300,q_80,w_300/MTE5NDg0MDU1MDQ5MzA3NjYz.jpg \"Niccolo Machiavelli\") (that's Niccolo Machiavelli) this is about making graphs of our data. This is going to be a long class, so I want you to recognise why it's worth getting the weather data below _into_ a pandas data frame before we go through the pain of working with the MetOffice API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "df.Population.plot.hist() # Population histogram. What's the outlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.Population.plot.box() # Population histogram. More than one outlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.Latitude.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.Latitude.plot.density()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kind of handy, no? These aren't the _best_ looking plots, but they are all being generated on-the-fly for you by pandas with no more than a cheery `<data frame>.<data series>.plot.<plot type>`! Since those plots are all just method calls, many of them take optional parameters to change the colour, the notation (scientific or not), and other options. This is why we like pandas: it allows us to be _constructively lazy_. We don't need to know _how_ a draw a KDE plot (though it always helps if you don't see what you expected), we just need to know that pandas provides a method that will do it for you. And _that_ is why it's always worth having a [look at the documentation](http://pandas.pydata.org/pandas-docs/stable/api.html#plotting).\n",
    "\n",
    "### Looking Ahead\n",
    "\n",
    "This is a very basic introduction to pandas just so that you're familiar with the bare bones of examining a pandas data frame to work out what it contains, how to calculate a few descriptive variables, and how to plot. Now it gets real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas with _Real_ Data\n",
    "\n",
    "That's the last of the 'toy' data, for the remainder of this module we're going to be working with two types of data: data about people (Socio-economic Classifcation) and data about the environment (weather). \n",
    "\n",
    "We've selected these two very different types of data on purpose:\n",
    "\n",
    "1. Because we know that some of you have interests in the human environment, and others in the natural\n",
    "2. Because these are very different types of data with very different properties\n",
    "3. Because we'll see that _similar_ workflows can be used with each!\n",
    "\n",
    "What we want to highlight is that computational approaches are _highly transferrable_ between contexts. The mean or median is not _less_ relevant in one context than another, it's just more or less appropriate as a tool for understanding the data! \n",
    "\n",
    "We'll see the Socio-economic Classification data next week and focus on the weather API data this week. This week is much harder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Data \n",
    "\n",
    "The UK's Met Office is a world-leading weather and climate research centre, and even if it doesn't always seem like their forecasts are very accurate that's because Britain's weather is inherently _unpredictable_. They've also done a lot of work to make their weather data widely available to people like us.\n",
    "\n",
    "I probably don't need to say a _lot_ about weather data because you've probably been making use of forecasts for much of your life! But it's _still_ worth understanding something about how weather data is gathered and reported: many organisations operate weather stations where data on wind speed, temperature, rain, and amount of sun are collected and then transmitted to a server to be integrated into a larger data set of weather _observations_ at a national or global scale. Of course, any _one_ station might be in the 'wrong' place (somewhere shady or protected from the rain) or it might even break down, but the idea is that if you have enough of them you can collect a pretty good range of data for the country and begin to look for patterns and, potentially, make predictions.\n",
    "\n",
    "We will be accessing data from the MetOffice from weather stations where observations, such as the ones below, are collected:\n",
    "* <Param name=\"F\" units=\"C\">Feels Like Temperature (units: degrees Celsius)\n",
    "* <Param name=\"G\" units=\"mph\">Wind Gust (units: mph)</Param>\n",
    "* <Param name=\"H\" units=\"%\">Screen Relative Humidity (units: percent)</Param> \n",
    "* <Param name=\"T\" units=\"C\">Temperature (units: degrees Celsius)</Param> \n",
    "* <Param name=\"V\" units=\"\">Visibility (units: km?)</Param> \n",
    "* <Param name=\"D\" units=\"compass\">Wind Direction (units: compass degrees)</Param>  \n",
    "* <Param name=\"S\" units=\"mph\">Wind Speed (units: mph)</Param> \n",
    "* <Param name=\"U\" units=\"\">Max UV Index (units: index value)</Param> \n",
    "* <Param name=\"W\" units=\"\">Weather Type (units: categorical)</Param> \n",
    "* <Param name=\"Pp\" units=\"%\">Precipitation Probability (units: percent)</Param>\n",
    "\n",
    "These observations are associated with a particular station (where did we see these values/where _will_ we see these values?), they will also be associated with _either_ a particular time in the past (when were they collected?) or, if they're forecasts, with a particular time in the future (when do we expect to see them?). \n",
    "\n",
    "So although weather data might seem more 'objective' than data on social class (though for obvious reasons it turns out that both are just attempts to capture data about reality, not reality itself), it may also turn out to be very complex to store and manage beccause of the temporal element _and_ the fact that it's not just a count of one thing, each of these observations uses a very different set of units.\n",
    "\n",
    "To really get to grips with the MetOffice API you will need to RTM (Read The Manual): http://www.metoffice.gov.uk/media/pdf/3/0/DataPoint_API_reference.pdf. The ruder version of that, which you will sometimes see on StackOverflow and elsewhere, is: RTFM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Weather Data via an API\n",
    "\n",
    "Because the weather is changing all the time, so is the data! And, 'worse', it's becoming obsolete: the forecast from 2 years ago isn't particularly useful to us now. *And* asking for \"yesterday's weather\" depends on the day that we're asking! When you have data that is always changing from minute to minute or day to day then you use an API (Application Programming Interface) to access it: the API knows that \"yesterday's weather\" means \"work out what day it is right now and then get the weather from the day before\", and it also knows that \"give me the current weather from station X\" means \"look up station X and find the latest weather report that I've received\". In other words, an API is  designed with programmatic, dynamic interaction in mind right from the start.\n",
    "\n",
    "## So What _is_ an API?\n",
    "\n",
    "There's a nice, friendly introduction to APIs over at [Free Code Camp](https://medium.freecodecamp.com/what-is-an-api-in-english-please-b880a3214a82#.rmjnli2nn). You can combine this with watching a video from MuleSoft that compares an API with a waiter in a restaurant:\n",
    "\n",
    "[![What is an API?](http://img.youtube.com/vi/s7wmiS2mSXY/0.jpg)](https://www.youtube.com/watch?v=s7wmiS2mSXY)\n",
    "\n",
    "Helpfully, the MetOffice provides a lot of documentation about their API (I'd suggest bookmarking it): http://www.metoffice.gov.uk/datapoint/support/api-reference\n",
    "\n",
    "This type of data requires a lot more research up front to work with, but it's very flexible once you know how to 'speak API' because you can _customise_ the API request so that the server responds with _only_ the data we're interested in instead of being 'stuck' with what the provider wants to give us.\n",
    "\n",
    "## Obtaining an API Key\n",
    "\n",
    "The first step to working with the API from the MetOffice is to obtain an API key: [do that here](http://www.metoffice.gov.uk/datapoint/API). \n",
    "\n",
    "Make a note of this key in your notebook. Right here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "api_key   = \"???\" # your API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `<`just double-click this cell to make it editable`>`.\n",
    "\n",
    "## Making an API Request\n",
    "\n",
    "We have to use the key as part of an API request: the process by which we _ask_ for data. Think of the key as being _your_ unique identifier: no two people share the same key and that way the MetOffice can cut off people who abuse the system or look at which APIs are popular with lots of users, or just a couple... Twitter and Facebook do the same thing.\n",
    "\n",
    "We're going to show you the code and output _first_ and then we'll step through the steps involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json, requests\n",
    "\n",
    "api_url   = \"http://datapoint.metoffice.gov.uk/public/data/\" # base URL\n",
    "obs_json  = \"val/wxobs/all/json/\" # observations URL\n",
    "fcs_json  = \"val/wxfcs/all/json/\" # forecasts URL\n",
    "\n",
    "heathrow = str(3772)  # heathrow airport weather station\n",
    "\n",
    "payload = {'res': 'hourly', 'key': api_key}\n",
    "r = requests.get(api_url + obs_json + heathrow, params=payload)\n",
    "\n",
    "# check the call\n",
    "print(r.url) # This is our _request_ to the MetOffice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check the output\n",
    "print(r.json()) # This is our _reply_ from the MetOffice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now let's make sense of this:\n",
    "```python\n",
    "import json, requests\n",
    "\n",
    "api_url   = \"http://datapoint.metoffice.gov.uk/public/data/\" # base URL\n",
    "obs_json  = \"val/wxobs/all/json/\" # observations URL\n",
    "fcs_json  = \"val/wxfcs/all/json/\" # forecasts URL\n",
    "```\n",
    "So, first we import two new modules: one that makes requests to a web server, and one that will parse JSON* responses from the server in order to turn them into something that we can use.\n",
    "\n",
    "\\* We'll get to what JSON is in a second.\n",
    "\n",
    "Then we set up some default variables (`api_key`, `api_url`, `obs_url`, `fcs_url`) that will help us to build our request to the MetOffice's server. The comments help us to remember what each of these variables is.\n",
    "\n",
    "Now let's do the actual work:\n",
    "```python\n",
    "heathrow = str(3772)  # heathrow airport weather station\n",
    "\n",
    "payload = {'res': 'hourly', 'key': api_key}\n",
    "r = requests.get(api_url + obs_json + heathrow, params=payload)\n",
    "\n",
    "# Check the call - need some proper try, except stuff here\n",
    "print(r.url)\n",
    "\n",
    "# Check the output\n",
    "print(r.json())\n",
    "```\n",
    "To get the data for Heathrow Airport we have to request it using a unique identifier (3772 in this case). Why? Because that's easier for the computer to handle than a long, potentially ambiguous string; for instance, if you asked for 'London' what would you get? The City of London? Greater London? Maybe there's another London in Wales? We don't know, so the computer is asking us to be specific in our request.\n",
    "\n",
    "We can then assemble a URL request by combining the site name, the observations URL, and the parameters. In this case that's: the type of 'resource' (the hourly observations), and our API key.\n",
    "\n",
    "### What's JSON?\n",
    "\n",
    "It's pretty hard to figure out what that 'JSON' reply means, but it's actually just a kind of dictionary containing lists and other dictionaries. That's it. It looks like a mess, but it _is_ a dictionary and the only thing that is entirely new is the fact that every string has the letter 'u' in front of it. That 'u' means 'Unicode' and it just a special kind of string that supports accents, Chinese characters, emojis, and just about anything else that you can think of... so to access this information in a useful way we need to know what the keys are that will give us access to the crucial bits of insight.\n",
    "\n",
    "It might be a little easier to read if we just look at the description associated with the _Site Report_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdesc = r.json()['SiteRep']['Wx']\n",
    "print(pdesc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you skip over the distracting little 'u's, then it's pretty clear that we have a dictionary of _parameters_: Wind Gust, Visibility, Wind Speed, etc. For each of these parameters, we have a short-name ('G', 'V', 'S', etc.), and the units in which they are measured. So this is _metadata_: data _about_ the data we've just been given. It makes sense too: if we just had a field called 'Wind Speed' we'd have no idea if it was in m/s, kph, mph, or something else entirely!\n",
    "\n",
    "And we can look at the data (DV = data values) the API returned using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdat  = r.json()['SiteRep']['DV']\n",
    "print(pdat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the above also looks a lot like a mix of Python dictionaries and lists: '{' and '['.\n",
    "\n",
    "### Retrieving Data for Another Site\n",
    "\n",
    "It's easy enough to get data for another site if you know how to ask the API nicely for a list of available sites. We do that with JSON as well, but it will be easier for _us_ to read if we ask for the answer in XML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "api_url = \"http://datapoint.metoffice.gov.uk/public/data/\" # base URL\n",
    "xml_url = \"val/wxfcs/all/xml/sitelist\" # sites API URL\n",
    "\n",
    "payload = {'res': 'daily', 'key': api_key}\n",
    "s = requests.get(api_url + xml_url, params=payload)\n",
    "\n",
    "# check the call\n",
    "print(s.url) # Click on the link below to see it nicely formatted automatically!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Recursion to Explore JSON Data\n",
    "\n",
    "We've seen dictionaries-of-lists and dictionaries-of-dictionaries before! We know how these work, but they've never been very easy to work with because we had to write lots and lots of nested loops:\n",
    "\n",
    "```python\n",
    "for key1 in bigDictionary:\n",
    "    for key2 in bigDictionary[key1]:\n",
    "        for key3 in bigDictionary[key1][key2]:\n",
    "            ... And so on ...\n",
    "```\n",
    "\n",
    "And if we have to add checks on each one of these `keys` to see if it is a list, a dictionary, or a simple float/int then this code would explode in complexity and become very, very hard to follow.\n",
    "\n",
    "But there is another way. It's a concept called _recursion_. \n",
    "\n",
    "Let's imagine that we have to deal with lists-of-lists (because those are a bit simpler to think about) but we don't know in advance how many lists there are inside of each list; e.g.:\n",
    "```python\n",
    "myList = [\n",
    "    ['Value 1',\n",
    "        ['Value 1.a.i', 'Value 1.a.ii'],\n",
    "        ['Value 1.b.i', 'Value 1.b.ii', \n",
    "            ['Value 1.b.ii.I', 'Value 1.b.ii.II'],\n",
    "        'Value 1.c'],\n",
    "    ['Value 2'],\n",
    "    'Value 3'\n",
    "]\n",
    "```\n",
    "What a nightmare! That's hard to even _read_, let alone know how to process! But recursion allows us reframe this problem as something that is _almost_ simple (it's certainly elegant): we need a function that steps through a list one element at a time and then: \n",
    "\n",
    "* if the element is a simple value (float, int or string) then it prints it out, \n",
    "* if the element is a list then the function _calls itself_ on the nested list! \n",
    "\n",
    "In other words, when our list-reading-function finds a new list _inside_ the list it is currently reading, then it calls itself and passes in the list-inside-the-list.\n",
    "\n",
    "That explanation probably _still_ doesn't take much sense, but take a look at the code below. \n",
    "\n",
    "**Really, really look**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def outputList(lst, depth=0): \n",
    "    for value in lst:\n",
    "        if type(value) is list:\n",
    "            outputList(value, depth+1)\n",
    "        elif type(value) is dict: \n",
    "            outputDict(value, depth+1)\n",
    "        else:\n",
    "            print \"\\t\" * depth + \"l-Value: \" + value\n",
    "    print \"\\n\"\n",
    "\n",
    "def outputDict(dct, depth=0):\n",
    "    for key, value in dct.iteritems():\n",
    "        print \"\\t\" * depth + \"d-Key: \" + key\n",
    "        if type(value) is list:\n",
    "            outputList(value, depth+1)\n",
    "        elif type(value) is dict:\n",
    "            outputDict(value, depth+1)\n",
    "        else:\n",
    "            print \"\\t\" * depth + \"  d-Value: \" + value\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, `outputList` takes a list `l` and then steps through each element of that list. If it encounters an element that is a list, it calls `outputList` and passes it the list that it just found. If it encounters an element that is a dictionary, it calls `outputDict` and passes it the dictionary that it just found. If it encounters a simple value (the `else`) then it just prints it out.\n",
    "\n",
    "`outputDict` works the same way.\n",
    "\n",
    "Now, what's going on with `depth`? That is what allows to demonstrate actual recursion, but it contains two _new_ ideas that you'll need to make sense of:\n",
    "\n",
    "1. It demonstrates that you can have _default values_ in a function. In this case, a call to the `outputList` function _has_ to include a list, but the parameter _depth_ is optional: if you don't specify a depth, then it defaults to 0.\n",
    "2. You can see that when `outputList` and `outputDict` call _themselves_ they increment the `depth` variable by one.\n",
    "\n",
    "Let's look at a simpler example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def showRecursion(depth=0):\n",
    "    print(\"\\t\" * depth + \"I'm now at depth \" + str(depth))\n",
    "    if depth < 3:\n",
    "        showRecursion(depth+1)\n",
    "    print(\"\\t\" * depth + \"I'm done with depth \" + str(depth))\n",
    "\n",
    "showRecursion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `\"\\t\" * depth` as part of our print statement to indent the output because it multiplies the tab character (`\\t`) by the depth (e.g. 1, 2, etc.) to print out 1, 2, 3 tabs. You'll also notice that every time we recurse we increment (increase) depth by 1. So:\n",
    "\n",
    "* We start at depth 0 with `showRecursion()`.\n",
    "* We print out \"I'm now at depth 0\".\n",
    "* We call `showRecursion(0+1)` (because that's what `depth+1` does)\n",
    "* We print out \"I'm now at depth 1\".\n",
    "* We call `showRecursion(1+1)` (because that's what `depth+1` does)\n",
    "* ...\n",
    "* We reach `depth == 3` and so _don't_ call `showRecursion(3+1)` because depth is no longer less than 3.\n",
    "* _Now_ we print out \"I'm done with depth 3\" (that's the end of `showRecursion(2+1)`)\n",
    "* _Now_ we print out \"I'm done with depth 2\" (that's the end of `showRecursion(1+1)`)\n",
    "* _Now_ we print out \"I'm done with depth 1\" (that's the end of `showRecursion(0+1)`)\n",
    "* _Now_ we print out \"I'm done with depth 0\" (that's the end of `showRecursion()`)\n",
    "\n",
    "That's a _lot_ to get your head around, but it's the same as we're doing with `outputList` or `outputDict`. The recursive function helps us to make the formatting legible, so it's probably better that we just see it action... In our case we know that we're starting with a dictionary so we would ask `outputDict` to start outputting the content of `pdesc` (the rePly DESCription). `outputDict` then takes each of the key/value pairs in turn, looks at the value to see if _it_ is a dictionary or list or (by default) string and takes appropriate action. Don't get too stressed out if it doesn't make sense just yet, but it's such a powerful concept that it's definitely worth getting to grips with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputDict(pdesc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so what we have is:\n",
    "\n",
    "* A dictionary saved in the variable `pdesc` (parameter-description)\n",
    "* It contains one key only: `Param`\n",
    "* `pdesc['Param']` is a list of dictionaries\n",
    "\n",
    "How do I know this? I investigated..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Type for pdesc['Param']: \" + str(type(pdesc['Param'])))\n",
    "\n",
    "print(\"Type for pdesc['Param'][0]: \" + str(type(pdesc['Param'][0])))\n",
    "\n",
    "print(\"Contents of pdesc['Param'][0]: \" + str(pdesc['Param'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the point here is that we know have little bundles of information about the data the MetOffice is giving back to us: the parameter description dictionary tells us, for instance, that the name 'G' in the data-part of the reply is data about 'wind gusts' given in miles per hour ('mph'). We can do the same for every other parameter.\n",
    "\n",
    "Now, let's see what we get when we look at the reply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputDict(pdat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, so that's a lot more complex isn't it? But we can make sense of it in the same incremental way...\n",
    "\n",
    "We can start off by noticing that there are some useful _generic_ fields:\n",
    "\n",
    "* `pdat['dataDate']` will give us the date and time of the data in the reply.\n",
    "* `pdat['type']` tells us that we're looking at _Obs_-ervations\n",
    "\n",
    "And so on. The really interesting one in there is the 'Location'... let's investigate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(pdat['Location']['name'])\n",
    "print(pdat['Location']['elevation'])\n",
    "print(pdat['Location']['lat'])\n",
    "print(pdat['Location']['lon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That leaves us with the rather nasty-looking `pdat['Location']['Period']`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdat['Location']['Period']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, however, if we don't panic then we can make sense of it! First, let's look at the big pieces:\n",
    "\n",
    "* It's pretty obvious that there's a set of dictionaries in there -- we can see the '{...}'!\n",
    "* We can also see things that look like readings: 'D', 'Dp', 'H', 'P'...\n",
    "* We can also see two rather useful-looking bits of information: something that says 'Day' and something that looks like a timestamp (e.g. '2016-10-14Z')\n",
    "\n",
    "Let's work on this some more by trial-and-error, starting from the point that the data structure must be a list because of the '[...]':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputDict(pdat['Location']['Period'][0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputDict(pdat['Location']['Period'][1], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now we know that `pdat['Location']['Period']` is a list of daily reports. How do I know that? Because when I asked for the first item in the list I got an answer with yesterday's date, and when I asked for the second item in the list I got something containing today's date! And _within_ each of those is _another_ list that contains a set of reports about the weather at Heathrow!\n",
    "\n",
    "The _last_ clue in there is that one of the parameters is changing in an unusal way: we can guess what H (Humidity), P (Pressure) and most of the rest are from having output `pdesc` above, but the '$' is always in multiples of 60. Can you guess why?\n",
    "\n",
    "Let's see if we can turn this into something useful... Fix the '???' so that prints out the temperature reading at Heathrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for d in pdat['Location']['Period']: # d is short for day\n",
    "    print(\"Date: \" + d['value'])\n",
    "    for i in d['Rep']: # i is short for time interval\n",
    "        print(\"\\tTime: \" + str(i['$']))\n",
    "        print(\"\\t\\tTemperature is: \" + str(i[???]))\n",
    "        print(\"\\t\\tHumidity is:    \" + str(i[???]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you explain why there are two days in there and why the '$' values don't overal?\n",
    "\n",
    "Use the coding area below to print out the other values over the same period of time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turning API data into a Pandas DataFrame\n",
    "\n",
    "I've done a little searching online and no one has posted code to do this for us, so we'll have to put together everything that we learned in the past few weeks _as well as_ some new ideas about how to deal with new types of data... I am not expecting you to be able to make sense of everything that happens in the next few blocks on the first go, but I _am_ expecting you to try and I am also expecting you to be able to make use of this in the future...\n",
    "\n",
    "First, we need to know how to work with dates and times. \n",
    "\n",
    "## Working with Time Data\n",
    "\n",
    "Naturally, Python has a library that can help with that so let's first have a look at the obvious date in there: `dataDate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta \n",
    "\n",
    "# Ignore the time part of 'dataDate' as we're \n",
    "# getting API data with values in minutes after\n",
    "# midnight. Given that, we want to set this \n",
    "# starting point to 00:00:00Z\n",
    "obsDate = datetime.strptime(pdat['dataDate'].split(\"T\")[0],'%Y-%m-%d')\n",
    "\n",
    "print(obsDate)\n",
    "print(type(obsDate)) # It's a new type of object... beyond float, int, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that's a start, now let's see if we can do two more things:\n",
    "\n",
    "1. Do 'date math' so that we can add the number of minutes since midnight associated with the observation to the datetime object extracted from `dataDate`.\n",
    "2. Print out _one_ observation from the full range.\n",
    "\n",
    "We do this as a 'nested' loop: we know that we have one 'period' for each day contained in the data. And we know that within each day we have one 'report' for each hour. We can't (and don't want to) go through these in a random order so we use two `for` loops:\n",
    "\n",
    "```python\n",
    "for d in <day>:\n",
    "    for h in <hour>:\n",
    "        ...do something...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for d in pdat['Location']['Period']: # d is for day of observations\n",
    "    \n",
    "    dataDate = datetime.strptime(d['value'],'%Y-%m-%dZ') # Convert date to datetime object\n",
    "    print(\"Observation date: \" + str(dataDate)) # Print for debugging\n",
    "    \n",
    "    for h in d['Rep']: # h is for hourly reports\n",
    "        obsDate = dataDate + timedelta(minutes = int(h['$'])) # Add the time in minutes to the datetime\n",
    "        # print(obsDate) # Debug!\n",
    "        \n",
    "        print(\"Temperature at \" + str(obsDate) + \" is \" + str(h['T']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how that worked? We did the following:\n",
    "\n",
    "1. For each day `d` in the data...\n",
    "2. We used the `value` parameter to initialise (i.e. set) a variable called `dataDate`\n",
    "3. We then retrieved the 'hours since midnight' from the \\$ parameter and added it to dataDate using the `timedelta` function.\n",
    "4. And then we found the temperature using the `T` key.\n",
    "\n",
    "## Making a Function to Handle API Data\n",
    "\n",
    "Let's try to moving this into a more generic function because we're likely to have to do this more than once..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def processMetOfficeObservations(loc): \n",
    "    \"\"\"\n",
    "    Process a series of 'reports' for a single\n",
    "    location using the datetime object as the \n",
    "    reference time against which to build the \n",
    "    timedelta (i.e. we start from midnight and \n",
    "    the timedelta is the number of minutes past \n",
    "    midnight)\n",
    "    \"\"\"\n",
    "    observations = {}\n",
    "    \n",
    "    for d in loc['Period']: # d for day\n",
    "        dt = datetime.strptime(d['value'],'%Y-%m-%dZ') # Convert date to datetime object\n",
    "    \n",
    "        # Now deal with the actual observations (i.e. 'Reports')\n",
    "        for report in d['Rep']:\n",
    "            \n",
    "            # Find the timestampe and add it to the date\n",
    "            minutes_after_midnight = int(report['$'])\n",
    "            ts = dt + timedelta(minutes=minutes_after_midnight)\n",
    "            \n",
    "            # For each of the possible values, set a default value\n",
    "            # if the weather station doesn't actually collect that\n",
    "            # parameter... can you see a problem with our defaults?\n",
    "            if 'ts' not in observations:\n",
    "                observations['ts'] = []\n",
    "            observations['ts'].append( str(ts) )\n",
    "            for key in ['D','Pt']:\n",
    "                if key not in report:\n",
    "                    report[key] = u\"\"\n",
    "                if key not in observations:\n",
    "                    observations[key] = []\n",
    "                observations[key].append(report[key])\n",
    "            for key in ['W','V','S','G']:\n",
    "                if key not in report or report[key] == \"\":\n",
    "                    report[key] = 0\n",
    "                if key not in observations:\n",
    "                    observations[key] = []\n",
    "                observations[key].append(report[key])\n",
    "            for key in ['T','Dp','H']:\n",
    "                if key not in report or report[key] == \"\":\n",
    "                    report[key] = 0.0\n",
    "                if key not in observations:\n",
    "                    observations[key] = []\n",
    "                observations[key].append(report[key])\n",
    "            \n",
    "    return observations\n",
    "\n",
    "data = processMetOfficeObservations(pdat['Location'])\n",
    "outputDict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And now this should be looking rather familiar to you... perhaps?\n",
    "\n",
    "Hang on a moment! It's a dictionary-of-lists!!!\n",
    "\n",
    "## Creating a DataFrame from a Dictionary\n",
    "\n",
    "And that, of course is exactly what we want to work with in pandas! \n",
    "\n",
    "So the _last_ step here is to figure out how to create a new data frame from this dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that was tricky... but did it really do what we expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head() # Check that it did what we expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get to plotting (again) we have a few more steps:\n",
    "\n",
    "1. To rename the columns to something a little more useful.\n",
    "2. To turn the 'ts' field into an _actual_ timeseries so that pandas understands what it is.\n",
    "3. To convert all of the other series to the right numerical/categorical format.\n",
    "\n",
    "Let's do this in several stages... remember that we printed out the column names earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# D  = Wind Direction\n",
    "# Dp = Dew Point\n",
    "# G  = Wind Gust\n",
    "# H  = Humidity\n",
    "# Pt = Pressure Tendency\n",
    "# S  = Wind Speed\n",
    "# T  = Temperature\n",
    "# V  = Visibility\n",
    "# W  = Weather Type\n",
    "# ts = Time of Day\n",
    "\n",
    "# Given this, how would you set new, easier to read column names?\n",
    "df.??? = ['WindDirection','DewPoint','WindGust','Humidity','PressureTendency','WindSpeed','Temperature','Visibility','WeatherType','ts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's looking a lot more useful, but now we need to make sure that 'ts' is treated as a time series... again, Google is your friend here: `\"pandas convert datetime to time series\"`. \n",
    "\n",
    "Given that we are creating a _new_ column from an _existing_ column, what do you think needs to replace the '???'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Time'] = pd.to_datetime(df.???, infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.Time.describe() # A quick check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.Temperature.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing column types\n",
    "\n",
    "Hmmm, you might notice that the description of numeric columns (like Temperature) doesn't seem much like what we had before -- shouldn't we get the 7-figure summary for numeric columns? The problem is that pandas didn't know what we expected the columns to be, so it's treated them all as 'objects' (basically: strings) and not as numeric data types (where possible).\n",
    "\n",
    "So we need to fix that now... there's a function called `'astype'` that allows to convert between data types where it's fairly easy for pandas to figure out what we want to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in ['WindDirection','WeatherType','PressureTendency']:\n",
    "    df[c] = df[c].astype('category')\n",
    "for c in ['DewPoint','Humidity','Temperature']:\n",
    "    df[c] = df[c].astype('float')\n",
    "for c in ['WindGust','Visibility']:\n",
    "    df[c] = df[c].astype('int')\n",
    "del df['ts'] # And delete a column!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's more like it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting!\n",
    "\n",
    "This has been a long, slow build towards something more exciting: plotting! In a way, this has been a lot of effort just to make a graph, but let's recognise where we're at:\n",
    "\n",
    "* We can request data for _any_ lcoation in Britain by changing the location id.\n",
    "* We can get new data _any_ time we feel like it.\n",
    "* We can (in a minute) create a plot of that data.\n",
    "* We can update it continuously in the future!\n",
    "\n",
    "That's pretty awesome, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This command tells Jupyter that we want \n",
    "# the plots to be shown inline (on this \n",
    "# web page). You'll always need to do this\n",
    "# *once* on a notebook.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.Humidity.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.Temperature.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.Temperature.plot(kind='box') # Not just line plots..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.WindDirection.plot() # Ooops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tend to think that that's quite enough to be coping with for one session... over to the script!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
