{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interlude: all models are wrong, but some are useful\n",
    "\n",
    "The statistician George Box [once said](https://en.wikipedia.org/wiki/All_models_are_wrong) \"all models are wrong but some are useful\". Now you might think that 'wrong' _implies_ uselessness, but this aphorism is a lot more interesting than that: if you're following this course in-person, then you'll have come across the idea of statistics as the study of a '[data-generating process](http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9639.2012.00524.x/full)'. \n",
    "\n",
    "The data that we work with is a _representation_ of reality, it is not reality itself. Just because I can say that the height of human beings is normally distributed with a mean of, say, 170cm and standard deviation of 8cm doesn't mean that I've _explained_ that process. What I have done is to say that reality can be reasonably well approximated by a normal distribution with those features. \n",
    "\n",
    "Given that understanding, I know that someone with a heigh of 2m is very, very improbable. Not impossible, just highly unlikely. So if I meet someone that tall then that's quite exciting! But my use of a normal distribution to represent that reality doesn't mean that I think that height is _actually_ distributed randomly amongst all human beings like some gigantic lottery system: some parts of the world are typically shorter, other parts typically taller; some families are typically shorter, while others are typically taller...\n",
    "\n",
    "So the _real_ reason for someone's height is to do with, I assume, a mix of genetics and nutrition. However, across large groups of people it's possible to _represent_ the cumulative impact of those individual realities with a simple normal distribution. And using that simplified data-generating process allows me to do things like estimate the likelihood of meeting someone 2m tall (which is why I'd be excited to do so).\n",
    "\n",
    "In the same way, real individuals earn different incomes for all sorts of reasons: skills, education, negotiation ability... and, of course, systematic discrimination or bias. Because of wide variations in individual lived experience, it's quite hard to _prove_ that any _one_ person has been discriminated against unless you have the 'smoking gun' of an email or other direct evidence that this has happened. \n",
    "\n",
    "But if I have data on _many_ men and women (from either inside or outside of a company) to work with, then I can take a look at what data-generting processes best describe what I've observed. And I can also create a data generating process that would describe what I'd _expect_ to see if no systematic discrimination were taking place at all. I make that sound simple but, of course, it's really hard to do this properly: do you account for the fact that discrimination has probably happened throughout someone's lifetime, not just in their most recent job? And so on.\n",
    "\n",
    "But, when we have created a data-generating process that captures this at what we feel is an appropriat level, then the analytical process becomes about testing to see if there is a _significant_ difference between what I expected and what I actually observed. Once we've done that, then we can start to rule out claims that 'there are no good candidates' and the other defences of the indefensible. It is always theoretically _possible_ that a company had trouble finding qualified candidates, but as you put together the evidence from you rmodel it may well  become increasingly _improbable_.\n",
    "\n",
    "So, always remember that the data is not reality, it is a very useful abstraction of reality that allows us to make certain claims about what we expected to see. Linking our observations to what we know about the characteristics of the data generating process then allows us to look at the _significance_ of our results or to search for outliers in the data that seem highly improbable and, consequently, worth further investigation.\n",
    "\n",
    "Here's a (slightly terrifying) video that tries to explain it in a different way:\n",
    "\n",
    "[![From reality to make-believe](http://img.youtube.com/vi/HAfI0g_S9oo/0.jpg)](https://www.youtube.com/watch?v=HAfI0g_S9oo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Socio-economic Classification Data\n",
    "\n",
    "Many governments publish large data sets drawn from a national Census that takes place every 5 or 10 years. And we know that things like education and income affect people's life chances in the long-run: higher levels of education, higher incomes, more senior jobs, and owning a house (instead of renting) tend to be linked to better health and other long-term advantages. \n",
    "\n",
    "So if we want to understand the distribution of these advantages in the population as a whole it might be handy to derive a 'class' for every household based on their levels of education, their job, their, health, and so forth. We're not saying that these 'classes' are objective realities since you could clearly slice and dice income, profession, and so on in a nearly infinite number of ways; but we can look in the data for 'natural breaks' or other meanignful thresholds (given our assumptions about the 'data generating process'). \n",
    "\n",
    "We can then look at how those classes are distributed across an area to see whether or not there is mixing (generally thought to be a good thing), or to select an area for further investigation as part of a case study. In the UK this [classifcation](http://www.statistics.gov.uk/downloads/theme_compendia/ESRC_Review.pdf) is known as the National Statistics Socio-economic Classifcation (NS-SeC). You really should [read the documentation](http://webarchive.nationalarchives.gov.uk/20160105160709/http://www.ons.gov.uk/ons/guide-method/classifications/current-standard-classifications/soc2010/soc2010-volume-3-ns-sec--rebased-on-soc2010--user-manual/index.html) before trying to work with this data for research (or an end-of-term essay) but the main point is the 8-fold classifcation:\n",
    "\n",
    "| Analytic Class | Description |\n",
    "| --- | --- |\n",
    "| 1 | Higher managerial and professional occupations |\n",
    "| 2 | Lower managerial and professional occupations |\n",
    "| 3 | Intermediate occupations (clerical, sales, service) |\n",
    "| 4 | Small employers and own account workers |\n",
    "| 5 | Lower supervisory and technical occupations |\n",
    "| 6 | Semi-routine occupations |\n",
    "| 7 | Routine occupations |\n",
    "| 8 | Never worked or long-term unemployed |\n",
    "| NC | Not classified elsewhere |\n",
    "\n",
    "These can be grouped into a 3-fold classification:\n",
    "\n",
    "| Class | Description |\n",
    "| --- | --- |\n",
    "| 1 | Higher occupations |\n",
    "| 2 | Intermediate occupations |\n",
    "| 3 | Lower occupations |\n",
    "\n",
    "And they can also be disaggregated into more detailed groups a limited way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Getting the NS-SeC Data\n",
    "\n",
    "## Using InFuse\n",
    "To obtain the NS-SeC data you'll have to visit the [InFuse](http://infuse.mimas.ac.uk) web site. Here's what you'll need to do:\n",
    "1. Select the 2011 Census data\n",
    "2. Scroll down the 'Filters' on the left-hand side of the page until you find 'NS-Sec' (*note*: there are _four_ options)\n",
    "3. Select the first NS-SeC filter: \"NS-SeC (National Statistics Socio-economic Classification) of household reference person\"\n",
    "4. Select the first option (Usual resident population; NS-SeC) from the range of data sets offered\n",
    "5. Read about the selected topic and then click 'Next'\n",
    "6. Select 'Total', classes 1–8, and Not classified using the checkboxes.\n",
    "7. Click 'Add' to add these to the 'Selected category combinations' below (it should say \"You have selected 10 category combinations)\n",
    "8. Click 'Next'\n",
    "9. Select 'Middle Super Output Areas' (8,480 areas) from the checkboxes and then click 'Add'\n",
    "10. Click 'Next', review the data extract information, and then click 'Get the data'\n",
    "11. When the big red 'Download the data' button appears after a few seconds click 'Download'\n",
    "12. Save the data to the _same folder_ as this notebook.\n",
    "\n",
    "If you've followed the steps above carefully, you should now have a Zip file showing in the 'Home' tab of your browser (the tab from which you launched this notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the data\n",
    "\n",
    "You could always unzip the downloaded file manually by double-clicking the zipfile, but given that we're programming in Python perhaps we could do it a different way?\n",
    "\n",
    "Perhaps [Google](http://www.google.com/) has the answer to 'unzipping zip files python'? And perhaps the first answer in that list will be something from [StackOverflow](http://stackoverflow.com/questions/3451111/unzipping-files-in-python)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip_ref = ???\n",
    "zip_ref.extractall('./Data/')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all has gone well you should now have a folder called 'Data' inside the folder where this notebook sits. The Data folder contains three files:\n",
    "1. citations.rtf – this is information about how to reference this data set so that other people know where it came from. \n",
    "2. Data_NSSHRP_UNIT_URESPOP.csv – this looks promising...\n",
    "3. Meta_NSSHRP_UNIT_URESPOP.csv – this file takes a little getting used to but it's very, very important since it's the metadata that describes each column in our downloaded data file!\n",
    "\n",
    "The hardest thing about the metadata file is that several rows can relate to _one_ column in the data file. That's because each row is giving us _different_ information about what the column contains: what's it about, where did it come from, what format is it in? Etc.\n",
    "\n",
    "Let's be _bit_ naughty and just see what happens if we try to load the data file directly into pandas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./Data/Data_NSSHRP_UNIT_URESPOP.csv')\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be pretty obvious that this isn't quite what we want: although we did get the variable names from the first row of the file, row 0 of our data frame still contains metadata! It's time to see what we can do in pandas to tidy this up _when_ we do the import.\n",
    "\n",
    "I'd suggest looking at the [pandas documentation](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) for `read_csv`, and in particular at the following options that can be _passed_ to `read_csv()`:\n",
    "* header\n",
    "* names\n",
    "* usecols\n",
    "* skiprows\n",
    "* nrows\n",
    "\n",
    "By way of guidance:\n",
    "* I would _suggest_ that you skip one row\n",
    "* I would _suggest_ that you drop one of the columns immediately after the import\n",
    "* I would _suggest_ that you specify your own column names only after loading the data\n",
    "* I would _suggest_ that while you get this right you only read a small number of rows\n",
    "\n",
    "Answer at the end of this notebook, but try it yourself first by adding one parameter at a time to _build_ a working import statement (don't try all of these options at once!). This is critical: you _will not get it right first time_, so you need to take an incremental approach and assemble the code a little bit at a time.\n",
    "\n",
    "But by way of a hint to get you started and to ensure that you end up with the same column names that I do:\n",
    "```python\n",
    "colnames = ['CDU','GeoCode','GeoLabel','GeoType','GeoType2','Total']\n",
    "for i in range(1,9):\n",
    "    colnames.append('Group' + str(i))\n",
    "colnames.append('NC')\n",
    "```\n",
    "Now over to you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've now written the components of a Python script that can take a file downloaded from InFuse, automatically extract the contents from a Zip archive, and then load the data into pandas automatically. Now that we've done it for _one_ file, we can work out how to do it for _any_ file. That's what we mean by scalability: yes, the column names will be different for other files downloaded from InFuse, but the _process_ is the same: we could create a function that handles all of this for us and the only thing it would need is the names that we want to use for the columns! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation\n",
    "\n",
    "One of the most frequent applications of adding a new series (i.e. column) to a data set is when we want to transform the original data in some way. Transformations are useful when a data series has features that make comparisons or analysis difficult, or that affect our ability to intuit meaningful difference. By manipulating the data using one or more mathematical operations we can sometimes make it more *tractable* for subsequent analysis.\n",
    "\n",
    "Here's an example: let's say that we want to understand how student heights are distributed within a class. It's not at all easy if all you have to go on is a list of raw heights: 160cm, 158cm, 150cm, 185cm, 172cm, 175cm, 166cm...\n",
    "\n",
    "An obvious first step to understanding this student data would be to calculate a mean ($\\mu$) from the data since that tells us the _average_ height of all students in the class. But wouldn't it be handy to be able to examine in more detail how tall students are _relative_ to the mean? Is there someone from the basketball team taking this course? Or maybe the cox from a crew? How could we make it easy to compare the difference between each student and the overall class average in order to spot these 'special cases'?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try writing this out as Python code, in other words:\n",
    "```python\n",
    "# Create an empty data frame to \n",
    "# hold our height data\n",
    "df = pd.DataFrame() \n",
    "\n",
    "# Create and add a series\n",
    "df['Heights'] = pd.Series(\n",
    "    [160, 158, 150, 185, 172, 175, 166],\n",
    "    index = ['Judy','Frank', 'Alice', 'Eve', 'Bob', 'Carlos', 'Dan']\n",
    ")\n",
    "\n",
    "# Look at the results\n",
    "df.describe()\n",
    "```\n",
    "To recap: looking at the heights of the students (whether in code or in the notebook generally) it's hard to tell how far each student is from average, and who might be especially (*significantly*) tall or short. When this happens we can _transform_ the raw data in order to make it easier to see and interpret this variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Heights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>166.57143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.77366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>150.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>159.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>166.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>173.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>185.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Heights\n",
       "count   7.00000\n",
       "mean  166.57143\n",
       "std    11.77366\n",
       "min   150.00000\n",
       "25%   159.00000\n",
       "50%   166.00000\n",
       "75%   173.50000\n",
       "max   185.00000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame() \n",
    "\n",
    "# Create and add a series\n",
    "df['Heights'] = pd.Series(\n",
    "    [160, 158, 150, 185, 172, 175, 166],\n",
    "    index = ['Judy','Frank', 'Alice', 'Eve', 'Bob', 'Carlos', 'Dan']\n",
    ")\n",
    "\n",
    "# Look at the results\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtracting the Mean\n",
    "\n",
    "In many cases, the best way to make this comparison is to _subtract the mean_. Why is that? What have we achieved?\n",
    "\n",
    "Let's think it out:\n",
    "1. If a student is shorter _than average_ then their transformed height is less than 0\n",
    "2. If a student is taller _than average_ then their transformed height is more than 0\n",
    "3. The distance from 0 (e.g. -20 vs -3) gives us _some_ sense of how short or how tall someone is\n",
    "\n",
    "In a mathematical form we'd write this transformation as:\n",
    "$$\n",
    "x - \\mu\n",
    "$$\n",
    "\n",
    "In pandas we can express this transformation as:\n",
    "```python\n",
    "df['<NewColumn'] = df.<column>-df.<column>.mean())\n",
    "df.describe()\n",
    "```\n",
    "\n",
    "We can break this apart as:\n",
    "\n",
    "* df.\\<column\\> – this is the _entire_ data series\n",
    "* df.\\<column\\>.mean() – this calculates the mean ($\\mu$) of the data series\n",
    "* We perform this calculation and then use the results to create a new data series\n",
    "* We assign this series to a new column in the data frame called `<NewColumn>`.\n",
    "\n",
    "Pandas is smart enough to know that it needs to take _each_ student height and then subtract the mean height of all students from that value. So even though it looks like we're performing a single calculation, we're actually performing as many calculations as there are rows in the data frame but without needing to write any tricky code!\n",
    "\n",
    "*Remember*: subtracting the mean is a linear transformation (unlike the log-transform).\n",
    "\n",
    "Try the transformation in the coding area below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Heights</th>\n",
       "      <th>TransformedHeights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7.00000</td>\n",
       "      <td>7.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>166.57143</td>\n",
       "      <td>-0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.77366</td>\n",
       "      <td>11.77366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>150.00000</td>\n",
       "      <td>-16.57143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>159.00000</td>\n",
       "      <td>-7.57143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>166.00000</td>\n",
       "      <td>-0.57143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>173.50000</td>\n",
       "      <td>6.92857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>185.00000</td>\n",
       "      <td>18.42857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Heights  TransformedHeights\n",
       "count   7.00000             7.00000\n",
       "mean  166.57143            -0.00000\n",
       "std    11.77366            11.77366\n",
       "min   150.00000           -16.57143\n",
       "25%   159.00000            -7.57143\n",
       "50%   166.00000            -0.57143\n",
       "75%   173.50000             6.92857\n",
       "max   185.00000            18.42857"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['TransformedHeights'] = df.???-df.???\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the Transformed Heights mean? Does the notation make any sense to you? It may help to remember that Python, like nearly all programming languages, can rack up very, very minute errors when working with floating point numbers.\n",
    "\n",
    "So the output is in something called scientific notation, where numbers are represented using an exponent for accuracy and consistency in the formatting. To change this, try a Google search on \"python data frame force non-scientific output\". You should find a solution in the first few results; however, `lambda` is something we haven't seen before. It is creating something called an *anonymous* function, meaning that we can define a function without giving it a name. Why is this useful? [Read more about lambda](https://pythonconquerstheuniverse.wordpress.com/2011/08/29/lambda_tutorial/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at the Effect of a Transformation\n",
    "\n",
    "Now let’s see what effect this transformation has on real-world data. This is data on Socioeconomic Class from the UK, and the code below is designed in a way that is _parameterised_, meaning that we can quickly change the series from one column to another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               CDU       Total     Group1     Group2     Group3     Group4  \\\n",
      "count   8480.00000  8480.00000 8480.00000 8480.00000 8480.00000 8480.00000   \n",
      "mean   64955.04658  7107.01639  956.58231 1611.15354  744.77429  893.86521   \n",
      "std   121862.17041  1964.26375  612.89004  626.90345  255.34084  404.24643   \n",
      "min     9937.00000  1867.00000   33.00000  198.00000  108.00000   86.00000   \n",
      "25%    12056.75000  5908.75000  489.00000 1155.00000  564.00000  620.00000   \n",
      "50%    14176.50000  7137.00000  821.50000 1572.00000  729.00000  852.00000   \n",
      "75%    16296.25000  8312.00000 1308.00000 2006.00000  911.00000 1115.00000   \n",
      "max   354691.00000 14876.00000 4208.00000 4662.00000 2020.00000 3297.00000   \n",
      "\n",
      "          Group5     Group6     Group7     Group8         NC  \n",
      "count 8480.00000 8480.00000 8480.00000 8480.00000 7201.00000  \n",
      "mean   638.62972  934.78420  885.49068  293.44139  156.62686  \n",
      "std    250.01969  422.03243  485.96687  271.83294  373.68203  \n",
      "min     56.00000   78.00000   22.00000    6.00000    0.00000  \n",
      "25%    456.00000  610.00000  515.00000  113.00000        nan  \n",
      "50%    623.00000  882.00000  796.00000  195.50000        nan  \n",
      "75%    795.00000 1210.00000 1178.00000  381.00000        nan  \n",
      "max   2134.00000 2927.00000 3406.00000 3031.00000 8225.00000  \n",
      "Summarising Group1...\n",
      "count   8480.00000\n",
      "mean     956.58231\n",
      "std      612.89004\n",
      "min       33.00000\n",
      "25%      489.00000\n",
      "50%      821.50000\n",
      "75%     1308.00000\n",
      "max     4208.00000\n",
      "Name: Group1, dtype: float64\n",
      "\n",
      "\n",
      "Prettily formatted metrics from Group1...\n",
      "\tMedian:   821.50\n",
      "\tLQ:       489.00\n",
      "\tUQ:       1308.00\n",
      "\tRange:    4175.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create our own column names to replace\n",
    "# the ones provided in the data\n",
    "colnames = ['CDU','GeoCode','GeoLabel','GeoType','GeoType2','Total']\n",
    "for i in range(1,9):\n",
    "    colnames.append('Group' + str(i))\n",
    "colnames.append('NC')\n",
    "\n",
    "# Load the CSV file into pandas\n",
    "df = pd.read_csv('./Data/Data_NSSHRP_UNIT_URESPOP.csv', skiprows=[1])\n",
    "# Delete the extra column\n",
    "del df['Unnamed: 15']\n",
    "# Update the column names\n",
    "df.columns = colnames\n",
    "\n",
    "# Print summary for group\n",
    "series = df.Group1\n",
    "print \"Summarising \" + series.name + \"...\"\n",
    "print series.describe()\n",
    "print \"\\n\"\n",
    "\n",
    "# Or we can do pretty numbers!\n",
    "print \"Prettily formatted metrics from \" + series.name + \"...\"\n",
    "print \"\\tMedian:  {0:> 7.2f}\".format(series.median())\n",
    "print \"\\tLQ:      {0:> 7.2f}\".format(series.quantile(0.25))\n",
    "print \"\\tUQ:      {0:> 7.2f}\".format(series.quantile(0.75))\n",
    "print \"\\tRange:   {0:> 7.2f}\".format(series.max()-series.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the coding cell below, why don't you use what we've just seen above to calculate a transformed value for Group1, assign it to a new series called `Group1LessMean`, and then print out the pretty-printed summary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prettily formatted metrics from Group1LessMean...\n",
      "\tMedian:   -15.77\n",
      "\tLQ:      -180.77\n",
      "\tUQ:       166.23\n",
      "\tRange:    1912.00\n"
     ]
    }
   ],
   "source": [
    "# Calculate and assign a transformed variable\n",
    "# to the data frame...\n",
    "df['Group1LessMean'] = ???\n",
    "\n",
    "# Which series are we working with?\n",
    "???\n",
    "\n",
    "# Or we can do pretty numbers!\n",
    "print \"Prettily formatted metrics from \" + series.name + \"...\"\n",
    "print \"\\tMedian:  {0:> 7.2f}\".format(series.median())\n",
    "print \"\\tLQ:      {0:> 7.2f}\".format(series.quantile(0.25))\n",
    "print \"\\tUQ:      {0:> 7.2f}\".format(series.quantile(0.75))\n",
    "print \"\\tRange:   {0:> 7.2f}\".format(series.max()-series.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compare the results how important are the changes? Which metrics have changed, and which remain unaffected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Formatting for Pretty-Printed Numbers \n",
    "\n",
    "Notice also the `<string>.format()` command I’ve used here: `{0:> 7.4f}`. In order to understand how this works for formatting the results in a nice, systematic way you will need to read  [the documentation](http://www.python.org/dev/peps/pep-3101/)\n",
    "\n",
    "The 'pep' tells you that:\n",
    "* `{0}` tells Python to grab the first value inside the parentheses (`format(... values ...)`) and to stick it into the string at this point, but `:...` tells Python that we also format the string in a particular way specified in the `...`.\n",
    "* `>` tells Python that the string should be right-aligned.\n",
    "* The space (' ') next to the > says that any 'fill' should be done with whitespace (you could also do it with a 0).\n",
    "* `7.4f` tells Python to treat anything it gets as a float (even if the variable is an int) and to format it for having 4 significant digits after the full-stop, and a total of 7 digits in all (which ties us back to the right-alignment up above). If you give it a number that has more than 3 digits to the left of the full-stop then it will still print them out, same as if it has less.\n",
    "\n",
    "Here are some suggestions to better-understand what’s going on:\n",
    "* Try changing the > to a < for some of the printouts\n",
    "* Try changing the .4 to a .0 for some of the printouts\n",
    "Do this in the coding area below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMedian:  -16      \n",
      "\tLQ:       -180.77\n",
      "\tUQ:      166.2257\n",
      "\tRange:   1912.000000\n"
     ]
    }
   ],
   "source": [
    "print \"\\tMedian:  {0:???f}\".format(series.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logarithmic Transformation\n",
    "\n",
    "Let’s do one more simple transformation: taking the natural logarithm of the Group1 values. If you don’t remember what a logarithm is try these:\n",
    "* https://www.youtube.com/watch?v=zzu2POfYv0Y \n",
    "* https://www.youtube.com/watch?v=akXXXx2ahW0 \n",
    "* https://www.youtube.com/watch?v=0fKBhvDjuy0 (made by Ray & Charles Eames, two of the 20th Century’s most famous designers).\n",
    "\n",
    "Note that logarithms are non-linear transformations -- can you think why this is?\n",
    "\n",
    "### Logarithmic Transforms in Pandas\n",
    "\n",
    "To create a new series in the data frame containing the natural log of the original value it’s a similar process to what we've done before; since pandas doesn't provide a log-transform operator (i.e. you can’t call `df.Group1.log()` and it makes no sense why it would) we need to use the `numpy` package again:\n",
    "```python\n",
    "import numpy as np\n",
    "df['Group1Log'] = pd.Series(np.log(df.Group1))\n",
    "```\n",
    "Try performing the transformation and then printing out the same summary measures as above in the coding area below. Is it more clear to you now why a log-transform is a non-linear transformation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardisation\n",
    "\n",
    "## Proportional Standardisation\n",
    "\n",
    "Clearly, a proportion (e.g. a percentage) is one way of standardising data since, unless you're measuring change, it limits the range to between 0% and 100%. Programmers and statisticians almost always write a proportion in a decimal format so the range is between 0.0 and 1.0. \n",
    "\n",
    "Mathematically, however, the notation is a little more forbidding:\n",
    "$$\n",
    "p_{i} = \\frac{x}{\\sum_{i=1}^{n} x}\n",
    "$$\n",
    "However, it's important that you begin to familiarise yourself with the mathematical notation since many papers on computational geography will make use of this form. Let's break it down:\n",
    "\n",
    "1. It's a fraction: x divided by, errrr, something involving x\n",
    "2. The numerator is easy\n",
    "3. The denominator is hard\n",
    "4. The key is in the $i=1$ and $n$, which tells us that the sum is for 'all i' (i.e. summing for every value in the data set) \n",
    "\n",
    "In other words, we take the `sum()` of the column of `x` as the divisor for *each* observation of `x`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proportional Standardisation in Pandas\n",
    "\n",
    "We can calculate the proportion of people in each area who come from Group 1 using a similar format to what we've seen before in the Transformation section:\n",
    "```python\n",
    "df['Group1Pct'] = df.Group1/df.Group1.sum()\n",
    "```\n",
    "You might recognise that this is that dictionary style of key/value pairs again, so `'Group1Pct'` is the key, and new data series is the value. You can see the link here between the mathematical notation and the computational operation, right?\n",
    "\n",
    "Try printing out various summary metrics for the new column and comparing them to the raw values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count   8480.00000\n",
       "mean       0.13376\n",
       "std        0.07563\n",
       "min        0.00950\n",
       "25%        0.07411\n",
       "50%        0.11908\n",
       "75%        0.18180\n",
       "max        0.47962\n",
       "Name: Group1Pct, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Group1Pct'] = df.Group1/df.Total\n",
    "df.Group1Pct.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unfortunately**, in this particular case, this form of standardisation is also not what we want. Can you think why we might not be _that_ interested in the share of _all_ Group1 people in a particular LSOA? If that's _not_ what we want, then what proportion do we want? I'd suggest having a look at the other columns that are available to see... \n",
    "\n",
    "Try updating the coding block above so that it gives you a more useful answer. *Hint*: the right answer for _us_ in _this particular case_ will have the following output <span style=\"color:red\">(Check output before publishing)</span>:\n",
    "```python\n",
    "count   8480.00000\n",
    "mean       0.13376\n",
    "std        0.07563\n",
    "min        0.00950\n",
    "25%        0.07411\n",
    "50%        0.11908\n",
    "75%        0.18180\n",
    "max        0.47962\n",
    "Name: Group1Pct, dtype: float64\n",
    "```\n",
    "You can find out _where_ the maximum value occurs with this code:\n",
    "```python\n",
    "print df[df.Group1Pct == df.Group1Pct.max()]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z-Score Standardisation\n",
    "\n",
    "The z-score is a common type of standardisation, but it's a little more complex than a simple proportion; however, both are designed to enable us to compare data across different groups of observations. We can easily compare two percentages to know which one is more, and which less (e.g. I got 80% on one exam and 70% on the other).\n",
    "\n",
    "But let's think about that 'which exam did I do better on?' question a little bit more: what if you got 80% on an exam where everyone else got 85%? Suddenly that doesn't look quite so good right? And what if your 70% was on an exam where the average score was 50%? The z-score is designed to help you perform this comparison in a numerical way.\n",
    "\n",
    "As a reminder, the z-score looks like this:\n",
    "$$\n",
    "z=(x-\\mu)/\\sigma\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-Score Standardisation in Pandas\n",
    "\n",
    "Let’s figure out how to translate this into a new columns called `Group1ZStd`!\n",
    "\n",
    "We’ve already done the first part of this calculation up above in `Group1LessMean`. That series value is the same as $x-\\mu$, so all we need to do is divide by the standard deviation.\n",
    "\n",
    "So one way to do this is:\n",
    "```python\n",
    "df['Group1ZStd'] = \n",
    "     df.Group1LessMean/df.Group1.std()\n",
    "```\n",
    "That works exactly the same way as what we did when we subtracted the mean in the first place: we’re just taking the results from the previous equation and passing them on to this one. \n",
    "\n",
    "We could also do it all in one go as:\n",
    "```python\n",
    "df['Group1ZStd'] = \n",
    "     (df.Group1 - df.Group1.mean()) / df.Group1.std()\n",
    "```\n",
    "Do you see how we can begin to build increasingly complicated equations into the process of creating a new data series.\n",
    "\n",
    "### Using the Z-Score\n",
    "\n",
    "In the previous notebook, there was a long digression about 'data generating processes'; here, we can start to bring this to life. The first thing to do with the z-score is to look at what it implies:\n",
    "\n",
    "1. Subtracting the mean implies that the mean _is a useful measure of centrality_: in other words, the only reason to subtract the mean is if the mean is _meaningful_. If you are dealing with highly skewed data then the mean is not going to be very useful and, by implication, neither is the z-score.\n",
    "2. Dividing by the Standard Deviation implies that this _is a useful measure of distribution_: again, if the data is skewed or follows some exponential distribution then the standard deviation is going to be near-useless as an analytical tool.\n",
    "\n",
    "So the z-score is _most_ relevant when we are dealing with something that look vaguely like a normal distribution (the z-scored version of which has mean=0 and standard deviation=1). In those cases, anything with a z-score more than 1.96 standard deviations from the mean is in the 5% significance zone. \n",
    "\n",
    "But note: we can't really say _why_ one particular area has a high concentration of employment or why one individual is over 2m tall. All we are saying is that this standardised value is a pretty unlikely outcome _when compared to our expectation that employment is randomly distributed across the region_. We _know_ that employment isn't randomly distributed in the same way that we know that height isn't genuinely random becuase of the influence of genetics, nutrition, etc. But we need a way to pick out what counts as _**significant**_ over- or under-concentration (or height) from the wide range of 'a bit more' or 'a bit less' than 'normal'.\n",
    "\n",
    "And here we get to the crux of the issue, most frequentist statistics boils down to this: subtracting **what you expected** from **what you got**, and then dividing by **some measure of spread** to control for the range of the data. We then look at what's _left_ to see if we think the gap between expectations and observations was _meaningful_ or whether it falls within the range of 'noise'.\n",
    "\n",
    "It should be obvious that it's the _**expected**_ part of that equation that is absolutely crucial: we come up with a process that generates data that is _resembles_ the important dimensions of reality without thinking that the process has explained them. It's the first step, not the lsat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count   8480.00000\n",
       "mean      -0.00000\n",
       "std        1.00000\n",
       "min       -2.49382\n",
       "25%       -0.70797\n",
       "50%       -0.06178\n",
       "75%        0.65100\n",
       "max        4.99421\n",
       "Name: ZStd, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location Quotient Standardisation (LQ)\n",
    "\n",
    "This 'LQ' is used in geography to measure the concentration of something a sub-region to the overall concentration in the wider region of which that sub-region is a part. One of the most common applications is in economic geography where we are trying to find concentrations (or absences) of employment that seem significant. \n",
    "\n",
    "For example, let's pretend that we have a (very small) country composed of three regions:\n",
    "- Region 1: 400 employees; 200 in steel\n",
    "- Region 2: 200 employees; 150 in steel\n",
    "- Region 3: 300 employees; 80 in steel\n",
    "  \n",
    "*Question*: which region has the greatest concentration of steelworkers?\n",
    "\n",
    "*Answer*: depends what you mean by concentration. \n",
    "\n",
    "There are _more_ steelworkers in Region 1 overall, but their _density_ in Zone 2 is higher (because `150/200 > 200/400`). We use the density (which is a simple proportion) to ensure that we are comparing like-for-like; the proportion controls for the fact that each of the regions has a different number of total employees. \n",
    "\n",
    "Let's put it another way: cities like London and New York are gigantic. If you want to know where there are the _most_ bankers or _most_ bagel factories then your answer will almost always be... London and New York. But what if there's a small town where 95% of people are bankers, and another town where 80% of people are bakers? Surely that's pretty interesting too, no? What's going on that a place can support way more of these professions that we'd expect...\n",
    "\n",
    "_Expect_, there's that word again. How do we define what we _expect_ so that we can compare it to what we _got_? Well, the mean is one way of defining our expectations -- if you had to guess the height of a new student in your class, your _expectation_ would be based on the heights of the existing students, and the best guess that you could make would be the _average_ height of those students because the majority of students are about that tall.\n",
    "\n",
    "In the case of our little steel-producing country, we would need to define our expectation a little differently. We're already using a proportion (steelworkers in a region/all workers in a region) to control for the fact that our regions have different sizes. Why not use a _second_ proportion (all steelworkers in the country/all workers in the country) to set our expectations about how concentrated steel employment will be in each area?\n",
    "\n",
    "That's the LQ.\n",
    "\n",
    "$$\n",
    "LQ = \\frac{{Employment}_{sR}/{Employment}_{eR}}{{Employment}_{sA}/{Employment}_{eA}}\n",
    "$$\n",
    "\n",
    "In this formula $sR$ is the count of steelworkers in a Region, and $eR$ is the count of employees in all industries. Similarly, $sA$ is the count of steelworkers in All Areas (i.e. the country), while $eA$ is total employment in the country.\n",
    "\n",
    "What we do by using the proportion across the entire country is to set our _expectation_ that employment should be distributed evenly across all regions. If steelworkers don't have any specialist needs then each region would be expected to have a proportion of steelworkers in line with the proportion of steelworkers in the country as a whole. So if we find areas that are way above or below this then that might be something worth digging into.\n",
    "\n",
    "### Calculating the LQ in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         allemp  steel\n",
      "Region1     400    200\n",
      "Region2     200    150\n",
      "Region3     300     80\n",
      "\n",
      "\n",
      "Proportion in each region: \n",
      "Region1    0.500000\n",
      "Region2    0.750000\n",
      "Region3    0.266667\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "Proportion in entire country: \n",
      "0\n",
      "\n",
      "\n",
      "LQs for each region: \n",
      "Region1    inf\n",
      "Region2    inf\n",
      "Region3    inf\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Notice how we can create a data frame 'by hand'\n",
    "# Why do we set these up as floats, not ints?\n",
    "d = {\n",
    "  'allemp': pd.Series([400,200,300], index=['Region1','Region2','Region3']),\n",
    "  'steel' : pd.Series([200,150,80], index=['Region1','Region2','Region3']),\n",
    "}\n",
    "df = pd.DataFrame(d)\n",
    "\n",
    "# Look at what we've done\n",
    "print df\n",
    "\n",
    "print \"\\n\"\n",
    "\n",
    "print \"Proportion in each region: \"\n",
    "print df.steel / df.allemp\n",
    "\n",
    "print \"\\n\"\n",
    "\n",
    "print \"Proportion in entire country: \"\n",
    "print df.steel.sum() / df.allemp.sum()\n",
    "\n",
    "print \"\\n\"\n",
    "\n",
    "print \"LQs for each region: \"\n",
    "print (df.steel / df.allemp) / (df.steel.sum() / df.allemp.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debugging\n",
    "\n",
    "Huh, what's going on here? Why is the LQ `inf` (i.e. infinite)? That's not very helpful! Have a look at the results preceding the non-sensical LQ, can you see where this 'bug' arises? \n",
    "\n",
    "*Hint*: what do you get if you divide an integer by another integer in Python? What do you get if you divide an integer by a float (or a float by an integer)?\n",
    "\n",
    "Fix the code block above so that you get three LQs in the range between 0.558 and 1.57.\n",
    "\n",
    "It can be really useful to manually double-check the results of a calculation using a calculator or just by typing the numbers into a separate line in Python's interpreter -- if you were to get the LQ wrong here then all of the analysis that you do later would be wrong too. And while the replicability of code helps (i.e. you can fix the mistake and then re-run the entire analysis very quickly), you should never assume that you got it exactly right until you’ve double-checked.\n",
    "\n",
    "### Interpreting Your Results\n",
    "\n",
    "How do we interpret the LQ results? Let's start with the two simplest results: 0 and 1. If the LQ is 0 then that means that the denominator (top half of the LQ formula) was 0; there is no employment in the sector of interest in that region. If the LQ is 1 then that means that the top and bottom of the LQ equation were the same; the density of employment in the region is _the same_ as the density in the country as a whole. Anything more than one means a greater density than in the country as a whole.[1]\n",
    "\n",
    "[1] _Note_: you don't have to do this analysis at the country level, you could have your 'country' be a city and your regions be neighbourhoods or districts... basically, any time you have smaller zones nested within a larger one.\n",
    "\n",
    "From there, the most straightforward way to think about it more deeply is, perhaps surprisingly, using numbers. What do we get if the LQ is: $\\frac{0.5}{0.25}$? In this case, we'd be saying that 50% of employment in the region of interest is in our sector of interest, while in the country as a whole we would expect 25% of employment to be in that sector. That gives us a LQ of 2, and we can read that directly as being _twice_ as concentrated! If we had the reverse: $\\frac{0.25}{0.5}$ then we'd get 0.5 as the result and we'd know that employment was _half_ as concentrated..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis & VDQI\n",
    "\n",
    "If we weren't learning how to program at the same time as we learn to do data analysis then my recommendation would have been this: **start with a chart**. There is _no_ better tool for understanding what is going on in your data than to visualise it, but we couldn't show you how to make a plot without first teaching you how to load data and perform some basic operations on a data frame! Now we can get to grips with VDQI (the Visual Display of Quantitative Information) and how this supports our exploration of the data.\n",
    "\n",
    "## Why Seaborn\n",
    "\n",
    "For data visualisation we are going to use the [Seaborn package](http://stanford.edu/~mwaskom/software/seaborn/) because it provides a lot of quite complex functionality (and very pretty pictures) at quite low cost. There are, however, other options out there; the two that you are most likely to hear mentioned are: [Bokeh](http://bokeh.pydata.org/en/latest/) and matplotlib. Bokeh is, like, Seaborn designed to make it easy for you to create good-looking plots with minimal effort. \n",
    "\n",
    "Matplotlib is a different beast: it is actually the _underlying_ package that supports the majority of plotting (drawing graphs) in Python. So Seaborn and Bokeh both make use of the matplotlib library to create their plots, and if you want to customise a figure from either of these two libraries you will eventually need to get to grips with matplotlib. The reason we don't teach matplotlib is that it's much harder to make a good plot and the syntax is much more complex.\n",
    "\n",
    "A more recent entrant is ŷhat's ggplot library, which deliberately mimics R’s ggplot2 (http://ggplot.yhathq.com) -- this has become the dominant way of creating plots in the R programming language and it uses a 'visualisation' grammar that many people find incredibly powerful and highly customisable. Unfortunately, ggplot on Python does not currently support mapping (which R does in ggplot2).\n",
    "\n",
    "## Loading Seaborn \n",
    "\n",
    "As with other libraries that we’ve used, we’ll import Seaborn using an alias:\n",
    "```python\n",
    "import seaborn as sns\n",
    "```\n",
    "So to access Seaborn's functions we will now always just write `sns.<function name>()` (where `<function name>` would be something like `distplot`). Try importing Seaborn..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named seaborn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-ed9806ce3570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named seaborn"
     ]
    }
   ],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've not made any changes to your Anaconda distribution then you probably got:\n",
    "```\n",
    "---------------------------------------------------------------------------\n",
    "ImportError                               Traceback (most recent call last)\n",
    "<ipython-input-43-ed9806ce3570> in <module>()\n",
    "----> 1 import seaborn as sns\n",
    "\n",
    "ImportError: No module named seaborn\n",
    "```\n",
    "So, what's the error?\n",
    "\n",
    "To fix it, we'll need to install Seaborn manually. The best way to do this is to open up the Terminal.\n",
    "\n",
    "<span style=\"color:red;size:bigger\">Needs screen capture of using Terminal</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a Distribution Plot\n",
    "\n",
    "One of the most useful ways to get a sense of a data series is simply to look at its overall distribution. Something like this:\n",
    "```python\n",
    "%pylab inline\n",
    "...\n",
    "sns.distplot(<data series>)\n",
    "```\n",
    "The `%pylab inline` command only need to be run _once_ in a jupyter notebook; it tells jupyter to show the plots as part of the web page, rather than trying to show them in a separate window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1153970d0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAECCAYAAAAb5qc/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115314bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pylab inline\n",
    "import seaborn as sns\n",
    "\n",
    "df.Group1Lq = (df.Group1 / df.Total) / (float(df.Group1.sum()) / df.Total.sum())\n",
    "\n",
    "df.Group1Lq.describe()\n",
    "\n",
    "sns.distplot(df.Group1Lq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all at went well you should see a nice distribution plot for the Group 1 Location Quotient! \n",
    "\n",
    "OK, I want you to take a second here: although there was a lot of setup work that needed to be done, we just created a distribution plot in one line of code. One line. This is a more sophisticated plot than you could ever create in Excel and you just created it in one line of code. Try producing similar plots for some of the other groups. Do you understand more about the data and its distribution now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a Plot\n",
    "\n",
    "Saving a plot isn’t quite as easy as creating one... but wouldn’t be it a lot easier if we could save our plot automatically and not have to even touch a button to do so? This is where we need to use matplotlib syntax (and where you'll see why we opted not to spend too much time on it):\n",
    "```python\n",
    "# Simple save\n",
    "# Plotting library needed by seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(series.name)\n",
    "sb.distplot(series)\n",
    "fig = plt.gcf()\n",
    "plt.savefig(\"{0}-Test.pdf\".format(series.name), bbox_inches=\"tight\"\n",
    "plt.close()\n",
    "\n",
    "```\n",
    "To explain what's happening here: \n",
    "1. We import a package that gives us access to all kind of useful functions.\n",
    "2. We create a 'figure' object into which Seaborn can 'print' its graph\n",
    "3. We call Seaborn and ask it to print (it doesn't really need to know that it's printing to something, it just nees to know what 'device' is should use for output).\n",
    "\n",
    " \n",
    "The plot should have been saved to your working directory with the name of the data series that you were working with (so that you can try doing the same for other data series without overwriting your results!). If you think back a little bit to where we started 6 weeks ago, you’ll see just how far we’ve come, and just how far you can go now: the ability to automatically load, clean, process (e.g. standardise and filter), and print out data in a variety of forms is incredibly powerful. You could repeat an entire analysis simply by changing the input file (assuming the format doesn’t change too much). It’s the automation component that sits at the heart of geocomputational techniques -- you still need individuals and their judgement to figure out what to do and what to produce, but once you’ve done that you can make the computer do the boring stuff while you focus on the interpretation and the meaning of the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automation with Loops & Functions\n",
    "\n",
    "When we're undertaking an analysis of a data set, we often have to perform the same (or at least similar) tasks for each weather station or socio-economic class or ethnic group. We *could* copy and paste the code, and then just change the variable names to update the analysis... but that would be a definite instance of what Larry Wall would have called 'false laziness': it seems like a time-saving device in the short run, but in the long run you've made your code less readily maintainable (what if you want to _add_ to your analysis or find a bug?) and less easy to understand.\n",
    "\n",
    "There are nearly always two things that you should look at if you find yourself repeating the same code: \n",
    "1. write a for loop; \n",
    "2. consider writing a function.\n",
    "\n",
    "Why these two strategies?\n",
    "\n",
    "## Automating Summaries\n",
    "\n",
    "Let’s start with a for loop to generate summary statistics and a distribution plot for several of our data columns – you’ve already typed all of this code more than once so all we need to do is take the right bits of it and put them in a for loop like this:\n",
    "```python\n",
    "for series in ...:\n",
    "    print 'Sumarizing ' + series.name\n",
    "    \n",
    "    series.desribe()\n",
    "    \n",
    "    print \"\\tMean:    {0:> 7.2f}\".format(series.mean())\n",
    "    print \"\\tMedian:  {0:> 7.2f}\".format(series.median())\n",
    "```\n",
    " \n",
    "All you need to do is figure what goes into the for loop in the first place! You can get quite a lot from just looking at the code itself: where do we get a Series object from? Try typing this: type(df.Group1).\n",
    "Automating Analysis\n",
    "It is actually much, much harder to automate the analysis, but there are certain things that we can do to make life easier – transformation and standardisation is one of the easiest things to do as part of automation!\n",
    "Here’s some code to generate the additional columns for our analysis on the pattern that we created for Group 1:\n",
    " \n",
    "Do you see how that works – we use the group names as keys to access the data in the data frames (using the df[colName] syntax), but we also use them as strings to create the new column names by doing: groupName + typeOfTransformation.\n",
    "Make sure that you understand how this works before moving on to the next bit, which is harder.\n",
    "Improving our Automated Summaries\n",
    "Seaborn is a handy imaging library because it makes a lot of difficult charts quite easy, but to help us make sense of the data it will be useful to add some additional information to our distribution plots: lines to show the location of the mean, median, and outlier thresholds.\n",
    "To do this, we need to get at the library that Seaborn itself uses: matplotlib.\n",
    "Find the code that you wrote to create and save a distribution plot (5.1.3), and update it so that it looks like this:\n",
    " \n",
    "Remember that you type help(plt.vlines) to discover what parameters that function takes. \n",
    "Try changing the style of the median to a solid green line to make it easier to distinguish from the line marking the mean.\n",
    "Why do you think I put an if condition on the outlier vertical lines? Would this always be appropriate? Why? Why not?\n",
    "Bonus (Part 1)\n",
    "Finally, let’s combine all of the code from 5.3.3 and 5.1.3 into a function – this will allow you to tidy up your code a lot because instead of having lots of copies of the same code all over your script, you can just call the function instead and it will generate the outputs you need.\n",
    "Since this is bonus material I won’t give you a lot of pointers, but recall that you will want to put all of the code for 5.4.1 inside a function definition:\n",
    "def generateSummary(series):\n",
    "     … code here …\n",
    "You could do the same thing for generating the transformed and standardised data columns for each of the original NS-SeC groups.\n",
    "Bonus (Part 2)\n",
    "Taking a Cut of the Data\n",
    "For some types of plots we can be overwhelmed by the data – trying to show the distribution of Group 1 people as grouped by borough would be tricky.\n",
    "So let’s take a ‘cut’ of the data by selecting only some boroughs for further analysis – we’re going to arbitrarily select K&C, Hackney and Barking because I know they’re quite different boroughs, but you could also use the data to make this selection, right? For instance, I could ask pandas to help me pick the three boroughs that are the furthest apart in terms of their Group 1 means…\n",
    "Anyway, here’s the code to select a cut:\n",
    " \n",
    "You’ll notice two unusual bits of code in there that need some explanation:\n",
    "\tTo ‘select  multiple’ we need to write: dataFrame.Series.isin([…]), so we can’t just write dataFrame.Series==[…] unfortunately.\n",
    "\tWe also have this line:\n",
    "sdf.Borough = sdf.Borough.cat.remove_unused_categories()\n",
    "Which should be fairly self-explanatory, but it’s because be default pandas doesn’t update the list of valid categories (i.e. boroughs) just because we filtered out boroughs that weren’t of interest. We therefore need to update the Series so that Seaborn doesn’t include a bunch of empty categories in when we make our plots.\n",
    "Other Types of Plots\n",
    "Let’s have a look at some other types of plots…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Plots\n",
    "Finally, I wanted to show you how to create a 3D scatter plot – in this case the plot doesn’t add a lot to our understanding of the data, but there are cases where it might and it does illustrate how pandas, seaborn, and matplotlib work together to produce some pretty incredible outputs.\n",
    "Here’s what you will get:\n",
    " \n",
    "Here’s the code:\n",
    "  \n",
    "I would encourage you to look into the options in more detail:\n",
    "\tCan you change the colour map used to indicate which Borough each LSOA is drawn from?\n",
    "\tCan you change the icons used to mark each Borough so that they are different?\n",
    "\tCan you add a legend to indicate which marker is for which Borough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Resources\n",
    "\n",
    "* Geocomputation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
