{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "<h1>Transforming Data</h1>\n",
    "<h2>7SSG2059 Geocomputation 2017/18</h2>\n",
    "<h3 style=\"font-style:italic\">Jon Reades</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Week’s Overview\n",
    "\n",
    "This week we're going to explore how visualising data (using pandas and seaborn) helps us to make more sense of our data. We're then going to move on to automating this process because coding isn't _just_ about being able to load lots of data, it's also about being able to be constructively lazy with the data once it's loaded.\n",
    "\n",
    "## Learning Outcomes\n",
    "\n",
    "By the end of this practical you should:\n",
    "- Have created a set of different plots using Seaborn\n",
    "- Have automated the presentation of data for a number of columns\n",
    "- Have grasped how a mix of graphs and numbers can help you make sense of your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics as Judgement\n",
    "\n",
    "We hope that you'll go on to make lots of use of what you're learning here, but the single most important thing that you can take away from the remaining weeks of the class is the idea that statistics is not _truth_, it is _judgement_.\n",
    "\n",
    "[![Are you above average?](http://img.youtube.com/vi/hQLCWHww9OQ/0.jpg)](http://www.youtube.com/watch?v=hQLCWHww9OQ)\n",
    "\n",
    "### Data as Representation of Reality\n",
    "\n",
    "The statistician George Box [once said](https://en.wikipedia.org/wiki/All_models_are_wrong) \"all models are wrong but some are useful\". Now you might think that 'wrong' _necessarily implies_ uselessness, but this aphorism is a lot more interesting than that: let's review the idea of statistics as the study of a '[data-generating process](http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9639.2012.00524.x/full)'. \n",
    "\n",
    "The data that we work with is a _representation_ of reality, it is not reality itself. Just because I can say that the height of human beings is normally distributed with a mean of, say, 170cm and standard deviation of 8cm doesn't mean that I've _explained_ that process. What I have done is to say that reality can be reasonably well approximated by a data-generating process that uses the normal distribution. \n",
    "\n",
    "Given that understanding of height distributions, I know that someone with a height of 2m is very, very improbable. Not impossible, just highly unlikely. So if I meet someone that tall then that's quite exciting! But my use of a normal distribution to represent the empirical reality of human height doesn't mean that I think our height is _actually_ distributed randomly using a gigantic lottery system: some parts of the world are typically shorter, other parts typically taller; some families are typically shorter, while others are typically taller...\n",
    "\n",
    "So the _real_ reason for someone's height is to do with, I assume, a mix of genetics and nutrition. However, across large groups of people it's possible to _represent_ the cumulative impact of those individual realities with a simple normal distribution. And using that simplified data-generating process allows me to do things like estimate the likelihood of meeting someone more than 2m tall (which is why I'd be excited to do so, though not as excited as the guy in the next video...).\n",
    "\n",
    "Here's a (genuinely terrifying) video that tries to explain this whole idea in a different way:\n",
    "\n",
    "[![From reality to make-believe](http://img.youtube.com/vi/HAfI0g_S9oo/0.jpg)](https://www.youtube.com/watch?v=HAfI0g_S9oo)\n",
    "\n",
    "In the same way, real individuals earn different incomes for all sorts of reasons: skills, education, negotiation ability... and, of course, systematic discrimination or bias. Because of wide variations in individual lived experience, it's quite hard to _prove_ that any _one_ person has been discriminated against unless you have the 'smoking gun' of an email or other direct evidence that this has happened. \n",
    "\n",
    "But if I have data on _many_ men and women (from a company, industry, or society) to work with, then I can take a look at what data-generting processes best describe what I've observed. And I can also create a data generating process that would describe what I'd _expect_ to see if no systematic discrimination were taking place at all. I make that sound simple but, of course, it's really hard to do this properly: do you account for the fact that discrimination has probably happened throughout someone's lifetime, not just in their most recent job? And so on.\n",
    "\n",
    "### What is _Significant_?\n",
    "\n",
    "But, when we have created a data-generating process that captures this at what we feel is an appropriate level, then the analytical process becomes about testing to see if there is a _significant_ difference between what I expected and what I observed. Once we've done that, then we can start to rule out claims that 'there are no good candidates' and the other defences of the indefensible. It is always theoretically _possible_ that a company had trouble finding qualified candidates, but as you put together the evidence from your model it may well  become increasingly _improbable_.\n",
    "\n",
    "So, always remember that, while the data is not reality, it _is_ a very useful abstraction of reality that allows us to make certain claims about what we expected to see. Linking our observations to what we know about the characteristics of the data generating process then allows us to look at the _significance_ of our results or to search for outliers in the data that seem highly improbable and, consequently, worth further investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.use('TkAgg')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pysal as ps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "import urllib\n",
    "import zipfile\n",
    "import re\n",
    "import os\n",
    "\n",
    "import random\n",
    "random.seed(20171107)\n",
    "np.random.seed(20171107)\n",
    "\n",
    "def dld_merge_gsa_data(dst='merged.csv.gz'):\n",
    "    \"\"\"\n",
    "    Function to automatically build the merged LSOA and air quality data set.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    dst -- a local path to use as a destination (default merged.csv.gz)\n",
    "    \"\"\"\n",
    "    dst_compression='gzip'\n",
    "    \n",
    "    # Check if local copy exists...\n",
    "    if os.path.exists(dst):\n",
    "        # Yes, then just read and return the data frame\n",
    "        print(\"File already downloaded.\")\n",
    "        # The 'low memory' option means pandas doesn't guess data types\n",
    "        df = pd.read_csv(dst, compression=dst_compression, low_memory=False)\n",
    "        \n",
    "    else:\n",
    "        # No, then download and save a local copy\n",
    "        print(\"Downloading remote data...\")\n",
    "        \n",
    "        # It is possible to have a remote file with no\n",
    "        # compression so this is just a piece of forward\n",
    "        # looking code....\n",
    "        df1 = pd.read_csv(\n",
    "            'https://github.com/kingsgeocomp/geocomputation/blob/master/data/LSOA_AirQuality.csv.gz?raw=true',\n",
    "            compression='gzip', low_memory=False\n",
    "        ) # The 'low memory' option means pandas doesn't guess data types\n",
    "        df2 = pd.read_csv(\n",
    "            'https://github.com/kingsgeocomp/geocomputation/blob/master/data/LSOA%20Data.csv.gz?raw=true',\n",
    "            compression='gzip', low_memory=False\n",
    "        )\n",
    "        df = pd.merge(df1, df2, how='left', on='LSOA11CD')\n",
    "        # And save it\n",
    "        print(\"Writing to local file...\")\n",
    "        df.to_csv(dst, compression=dst_compression, index=False)\n",
    "    print(\"Done.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = dld_merge_gsa_data()\n",
    "df.set_index(['LSOA11CD'], drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Transformation\n",
    "\n",
    "Transformations are useful when a data series has features that make comparisons or analysis difficult, or that affect our ability to intuit meaningful difference. By manipulating the data using one or more mathematical operations we can sometimes make it more *tractable* for subsequent analysis. In other words, it's all about the _context_ of our data.\n",
    "\n",
    "[![How tall is tall?](http://img.youtube.com/vi/-VjcCr4uM6w/0.jpg)](http://www.youtube.com/watch?v=-VjcCr4uM6w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Demonstration Data\n",
    "\n",
    "Here's an example from one of our earlier lessons, but now written as Python code: let's say that we want to understand how student heights (and the visiting Mr. G.) are distributed within a class. It's not at all easy if all you have to go on is a list of raw heights: 160cm, 158cm, 150cm, 185cm, 172cm, 175cm, 166cm... and, of course, Mr. Bill Gates himself, to know how big a range you've got on your hands and where people fall relative to the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try writing this out as Python code:\n",
    "```python\n",
    "# Create an empty data frame to \n",
    "# hold our height data\n",
    "df2 = pd.DataFrame() \n",
    "```\n",
    "Here we've created an empty data frame – as yet it contains no data.\n",
    "```python\n",
    "# Create and add a series\n",
    "df2['Height'] = pd.Series(\n",
    "    [160, 158, 150, 185, 172, 175, 166, 168],\n",
    "    index = ['Judy','Frank', 'Alice', 'Eve', 'Bob', 'Carlos', 'Dan', 'Bill G.']\n",
    ")\n",
    "```\n",
    "Then we create a new data series: the data for the series is the heights, the index is the name of the student. We assign this new data series to 'Heights' in the data frame.\n",
    "```python\n",
    "# Look at the results\n",
    "df2.describe()\n",
    "```\n",
    "And, as always, a good next step is to check that you got what you expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame() \n",
    "\n",
    "# Create and add a series\n",
    "df2['Height'] = pd.Series(\n",
    "    [160, 158, 150, 185, 172, 175, 166, 168],\n",
    "    index = ['Judy','Frank', 'Alice', 'Eve', 'Bob', 'Carlos', 'Dan', 'Bill G.']\n",
    ")\n",
    "\n",
    "# Look at the results\n",
    "df2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add their wealth..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2['Wealth'] = pd.Series([28300, 21258, 37234, 32748, 18536, 75093, 124382, 5124398742348], index=df2.index)\n",
    "\n",
    "# Check the results\n",
    "df2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An obvious first step to understanding this student data would be to use the mean ($\\mu$) from the data since that tells us the _average_ height and wealth of all students in the class. But wouldn't it be even _more_ interesting to be able to look at how tall students are _relative_ to the mean? Is there someone from the basketball team taking this course? Or maybe the cox from a crew? How could we make it easy to compare the difference between each student and the overall class average in order to spot these 'special cases'?\n",
    "\n",
    "In many cases, the best way to make this comparison is to _subtract the mean_. Why is that? What does it achieve?\n",
    "\n",
    "Let's think it out:\n",
    "1. If a student is shorter _than average_ then their transformed height is less than 0\n",
    "2. If a student is taller _than average_ then their transformed height is more than 0\n",
    "3. The distance from 0 (e.g. -20 vs -3) gives us _some_ sense of how short or how tall someone is relative to that mean\n",
    "\n",
    "In a mathematical form we'd write this transformation as:\n",
    "$$\n",
    "x - \\mu\n",
    "$$\n",
    "\n",
    "In pandas we can express this transformation as:\n",
    "```python\n",
    "df['<new column name>'] = df.<column>-df.<column>.mean())\n",
    "df.describe()\n",
    "```\n",
    "\n",
    "We can break this apart as:\n",
    "\n",
    "* `df.<column>` – this is the _entire_ data series\n",
    "* `df.<column>.mean()` – this calculates the mean ($\\mu$) of the data series\n",
    "* We perform this calculation and then use the results to create a new data series\n",
    "* We assign this series to a new column in the data frame called `<new column name>`.\n",
    "\n",
    "Pandas is smart enough to know that it needs to take _each_ student height and then subtract the mean height of all students from that value. So even though it looks like we're performing a single calculation, we're actually performing as many calculations as there are rows in the data frame but without needing to write any tricky code!\n",
    "\n",
    "To recap: looking at the heights of the students (whether in code or in the notebook generally) it's hard to tell how far each student is from average, and who might be especially (*significantly*) tall or short. When this happens we can _transform_ the raw data in order to make it easier to see and interpret this variation.\n",
    "\n",
    "*Remember*: subtracting the mean is a linear transformation (unlike the log-transform).\n",
    "\n",
    "Try the transformation in the coding area below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2['TransformedHeights'] = df2.Height-df2.Height.mean()\n",
    "df2['TransformedWealth']  = df2.Wealth-df2.Wealth.mean()\n",
    "df2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the Transformed Heights mean? Does the notation make any sense to you? It may help to remember that Python, like nearly all programming languages, can rack up very, very minute errors when working with floating point numbers.\n",
    "\n",
    "So the output is in something called scientific notation, where numbers are represented using an exponent for accuracy and consistency in the formatting. To change this, try a Google search on `\"python data frame force non-scientific output\"`. You should find a solution in the first few results. \n",
    "\n",
    "The concept of `lambda` is something we haven't seen before: previously, when we wanted to define a function we _had_ to use `def <function name>:`, but sometimes we just need a tiny snippet of code to do something useful in a function-like way and don't want to have to write a full definition. That's where `labmda` comes in: it is creating something called an *anonymous* function, meaning that we can define a function without giving it a name. Why is this useful? [Read more about lambda](https://pythonconquerstheuniverse.wordpress.com/2011/08/29/lambda_tutorial/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the Right Transformation\n",
    "\n",
    "Now let’s have a look at our real-world data (or should that be \"The Real World\"?). We've got quite a few columns to work with, but they describe quite different types of phenomena. Let's see how this plays out in the distributions and, consequently, the types of transformations that area appropriate..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add comments to explain what this function\n",
    "# is doing.\n",
    "def normal_from_dist(series): \n",
    "    mu = series.mean()\n",
    "    sd = series.std()\n",
    "    n  = len(series)\n",
    "    s = np.random.normal(mu, sd, n)\n",
    "    return s\n",
    "\n",
    "# Now use the function above to plot both a distribution plot \n",
    "# with both histogram and KDE, and then add a _second_ overplot\n",
    "# distplot to the same fig. The overplot should not have a histogram\n",
    "# and the color should be red. You should be able to check the \n",
    "# last figure (Owned) against the next block of questions to see\n",
    "# if you got the right answer.\n",
    "for c in ['RoadsArea','NOxmax','Owned']:\n",
    "    fig1 = plt.figure()\n",
    "    ax1 = fig1.add_subplot(111)\n",
    "    sns.distplot(???)\n",
    "    sns.distplot(normal_from_dist(???), hist=???, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**: For which of the columns are the mean and standard deviation _most_ appropriate as measures of centrality and spread?\n",
    "\n",
    "**A1**: _Your answer **and rationale** here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making use of a transformation\n",
    "\n",
    "Let's focus on the 'Owned' (_i.e._ home ownership) column. Without allowing for the fact that different areas have different numbers of households to begin with (we'll get to _that_ later) how do we go about setting a threshold for areas with high concentrations of home ownership? Let's look at the chart again (also a good way to check you got the right code in the block above).\n",
    "\n",
    "![Ownership Distribution](https://github.com/kingsgeocomp/geocomputation/blob/master/img/Owned.png?raw=true)\n",
    "\n",
    "We _know_ that the middle is somewhere about 350 (we can get the exact number using `<series>.describe()`) but is 400 extreme? What about 600? Or 500? \n",
    "\n",
    "#### The Standard Normal\n",
    "\n",
    "The standard normal distribution has some useful properties; here's a refresher:\n",
    "\n",
    "![Standard Normal](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Boxplot_vs_PDF.svg/500px-Boxplot_vs_PDF.svg.png)\n",
    "\n",
    "The cut-offs that we're usually the most interested in are:\n",
    "\n",
    "| Confidence Level | z\\*-value |\n",
    "|------------------|-----------|\n",
    "| 50% | 0.675 |\n",
    "| 80% | 1.28 |\n",
    "| 90% | 1.645 (by convention) |\n",
    "| 95% | 1.96 |\n",
    "| 98% | 2.33 |\n",
    "| 99% | 2.58 |\n",
    "| Outlier | 2.698 | \n",
    "\n",
    "What you'll notice is that we can have _any_ cut-off we want. By convention the 95% confidence interval had become the standard for a 'significant' result, but over the past 10-15 years the importance of publications for researchers (and the fact that reviewers were using 95% CIs as an _heuristic_ for determining whether the research had 'succeeded') led the rise of _p-hacking_ (or _p-value hacking_) in which authors basically played with their data until their results surpassed this magical threshold. As the starting example shows, you can say _anything_ with data if you want.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<h3>“If you torture the data long enough, it will confess.”</h3>\n",
    "<h4>― Ronald H. Coase, Essays on Economics and Economists</h4>\n",
    "</div>\n",
    "\n",
    "### Using the transformation to select 'outliers'\n",
    "\n",
    "Using common sense, the 95% CI applied to the `Owned` data series, and a mix of code we've used above _and_ in earlier weeks:\n",
    "1. How many LSOAs are _above_ the upper CI?\n",
    "2. How many LSOAs are _below_ the lower CI?\n",
    "3. What are the LSOA names for the first three LSOAs just _above_ the upper CI?\n",
    "4. What are the LSOA names for the first three LSOAs just _below_ the lower CI?\n",
    "5. Can you plot the extreme values on the same plot?\n",
    "\n",
    "The output I'm looking for is below, but I'll settle for anything _close_ (_i.e._ you have the right results even if the formatting if off) and I've included some comments and snippets of code to get you started..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First, we need the mean and standard deviation of the data\n",
    "mu = df.Owned.???\n",
    "sd = df.???\n",
    "\n",
    "# Then we need our upper and lower CIs\n",
    "uci = mu + 1.96 * sd\n",
    "lci = mu - 1.96 * sd\n",
    "\n",
    "# Then we need to select the LSOAs falling above\n",
    "# and below our CI values (you can sort the results\n",
    "# at the same time, or do this as a separate command)\n",
    "udf = df.loc[df.Owned > uci].sort_values(by='Owned', ascending=True) # udf == upper df\n",
    "ldf = df.loc[???].sort_values(by='Owned', ascending=???) # ldf == lower df\n",
    "\n",
    "# Output main results\n",
    "print(\"There are {0} LSOAs ({1:.2f}%) above the upper CI of {2:.2f}\".format(len(udf),(float(len(udf))/len(df))*100,uci))\n",
    "print(\"There are {0} LSOAs ({1:.2f}%) below the lower CI of {2:.2f}\".format(???))\n",
    "print(\"The upper LSOAs are: \" + \", \".join(udf.LSOA11NM.head(3).values))\n",
    "print(\"The lower LSOAs are: \" + \", \".join(ldf.LSOA11NM.head(3).values))\n",
    "\n",
    "# And plot the results\n",
    "fig1 = plt.figure()\n",
    "plt.title(\"Ownership Extremes\")\n",
    "ax1 = fig1.add_subplot(111)\n",
    "sns.distplot(???, color='green') # ???\n",
    "sns.distplot(???, color='red') # ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what I got:\n",
    "```shell\n",
    "There are 62 LSOAs (1.28%) above the upper CI of 591.19\n",
    "There are 62 LSOAs (1.28%) below the lower CI of 60.81\n",
    "The first three upper LSOAs are: Sutton 009A, Richmond upon Thames 011D, Richmond upon Thames 012D\n",
    "The first three lower LSOAs are: Kensington and Chelsea 005D, Lambeth 009B, Tower Hamlets 018A\n",
    "```\n",
    "\n",
    "![Ownership Extremes](https://github.com/kingsgeocomp/geocomputation/blob/master/img/Owned_Extremes.png?raw=true)\n",
    "\n",
    "**Q2.** Can you think why we only seem to have 2.56% of the LSOAs falling outside of our 95% CI? Shouldn't it be 5%?\n",
    "\n",
    "**A2.** _Your answer here_\n",
    "\n",
    "**Q2 (Bonus Part 1).** Can you think how to get a count of LSOAs by borough for each of the two extreme ends of the distribution? Write your code below. _Hint:_ You want to look at the highest-voted answer on [this page](https://stackoverflow.com/questions/19384532/how-to-count-number-of-rows-in-a-group-in-pandas-group-by-object).\n",
    "\n",
    "**Q2 (Bonus Part 2).** What do you notice about these upper and lower outliers? Is there a geographical pattern to suggest that this might not be quite what you want?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Upper outliers: \")\n",
    "print(udf[['LAD11NM']].???)\n",
    "print(\"\\nLower outliers: \")\n",
    "print(ldf[['LAD11NM']].???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logarithmic Transformation\n",
    "\n",
    "Logarithmic transformations are also considered fairly simple, but they are _non_-linear transformations and so they do change the relationships in your data in important ways. Although you _could_ use any logarithm, the natural log is considered the most useful since both the mean and standard deviation retain _some_ meaning (though you probably wouldn't report these as such). If you don’t remember what a logarithm is try these:\n",
    "\n",
    "[![From reality to make-believe](http://img.youtube.com/vi/zzu2POfYv0Y/0.jpg)](https://www.youtube.com/watch?v=zzu2POfYv0Y)\n",
    "\n",
    "[![From reality to make-believe](http://img.youtube.com/vi/akXXXx2ahW0/0.jpg)](https://www.youtube.com/watch?v=akXXXx2ahW0)\n",
    "\n",
    "[![From reality to make-believe](http://img.youtube.com/vi/0fKBhvDjuy0/0.jpg)](https://www.youtube.com/watch?v=0fKBhvDjuy0)\n",
    "\n",
    "The last video was made by Ray & Charles Eames, two of the 20th Century’s most famous designers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.** Why are log-transforms considered non-linear?\n",
    "\n",
    "**A3.** _Your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's imagine...\n",
    "\n",
    "Let's imagine that:\n",
    "\n",
    "1. I think that there is relationship between pollution and the presence of major roads.\n",
    "2. I don't know what kind of relationship there might be.\n",
    "\n",
    "We're going to start with the simplest approach: a chart!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.jointplot(x=df.RoadsArea, y=df.NOxmax, size=6)\n",
    "g.fig.suptitle('NOx (Max) against Roads Area\"', fontsize=18,color=\"r\",alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm, that didn't help very much. It looks like our RoadsArea data is very heavily skewed and that there are a lot of very low values in the data. There also seems to a be at least one major outlier that is _so_ different in scale that I'd want to know whether to even keep it in the analysis -- it looks like what would be called a 'leverage point' in a regression model: something that is so 'out of whack' that it alters the entire regression! \n",
    "\n",
    "Let's try stripping that out and running this code again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"The outlier LSOAs are: \" + \",\".join(df[df.NOxmax > 4000]['LSOA11NM'].values))\n",
    "df = df[df.NOxmax < 4000]\n",
    "\n",
    "g = sns.jointplot(x=df.RoadsArea, y=df.NOxmax, size=6)\n",
    "g.fig.suptitle('NOx (Max) against Roads Area\"', fontsize=18, color=\"r\",alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite a change right? Notice that the correlation has dropped markedly and that the new chart presents a much more complex picture of NOx and Roads Areas. \n",
    "\n",
    "Regardless, on the whole in this presentation it's _still_ really hard to tell what's going because of the heavy skew in the Roads data... Perhaps a different transformation might help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logarithmic Transforms in Pandas\n",
    "\n",
    "To create a new series in the data frame containing the natural log of the original value it’s a similar process to what we've done before, but since pandas doesn't provide a log-transform operator (i.e. you can’t call `df.Owned.log()` ) we need to use the `numpy` package:\n",
    "```python\n",
    "series = pd.Series(np.log(df.RoadsArea))\n",
    "```\n",
    "Try performing the transformation and then `describe()` the results in the coding area below. Is it more clear to you now why a log-transform is a non-linear transformation?\n",
    "\n",
    "**_Warning_**: What is the natural log of 0? And of 1? What do you need to change in the code above to make this transformation work? Output below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Why do I do this here???\n",
    "pd.set_option('display.float_format', lambda x: '{:,.4f}'.format(x))\n",
    "\n",
    "series = pd.Series(???)\n",
    "print(series.describe())\n",
    "sns.distplot(series, color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "count    4834.000000\n",
    "mean        8.689413\n",
    "std         4.910561\n",
    "min         0.000000\n",
    "25%         7.411466\n",
    "50%        11.244498\n",
    "75%        11.920692\n",
    "max        14.769750\n",
    "Name: RoadsArea, dtype: float64\n",
    "```\n",
    "Now this _is_ interesting: the output of the graph shows what seems to be two quite different things going on in our data! We've obviously got the LSOAs that contain _no_ major roads, but then we've got something else that is _much_ closer to 'normal' (though obviously not properly normal as there is clear evidence of negative skew). Technically, this is closer to _log-normal_ and you **should not** think of the mean or standard deviation as being numbers that you would report. What they _can_ do is help you to set a cut-off for the LSOAs that are most impacted by major roads..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hrds = df.loc[df.RoadsArea > 0] # hrds == has (major) roads\n",
    "\n",
    "g = sns.jointplot(x=np.log(hrds.RoadsArea), y=np.log(hrds.NOxmax), size=6)\n",
    "g.fig.suptitle('Log-Transformed NOx (Max) against Roads Area\"', fontsize=18,color=\"r\",alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There still isn't a clear answer here, but at this point I would be thinking: there's _something_ going on, but looking at the correlation coefficient (Pearson's r == 0.26 [_stay tuned for Week 9!_]) I wouldn't feel very comfortable that NOx (Max) values were predicted solely by the total area of a LSOA within 250m of major roads... this is what we mean by building a model: we know we've got our hands on something useful, but we also know that we need to work on it some more to make the model _useful_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardisation\n",
    "\n",
    "### Proportional Standardisation\n",
    "\n",
    "Clearly, a proportion (e.g. a percentage) of the total count contained in one LSOA is one way of standardising data since, unless you're measuring change, it limits the range to between 0% and 100%. Programmers and statisticians almost always write a proportion in a decimal format so the range is between 0.0 and 1.0. \n",
    "\n",
    "Mathematically, however, the notation is a little more forbidding:\n",
    "$$\n",
    "p_{i} = \\frac{x}{\\sum_{i=1}^{n} x}\n",
    "$$\n",
    "However, it's important that you begin to familiarise yourself with the mathematical notation since many papers on computational geography will make use of this form. Let's break it down:\n",
    "\n",
    "1. It's a fraction: the observation _x_ divided by, errrr, something involving x\n",
    "2. The numerator is easy\n",
    "3. The denominator is hard\n",
    "4. The key is in the $i=1$ and $n$, which tells us that the sum is for 'all i' (i.e. summing for every _x_-value in the data set) \n",
    "\n",
    "In other words, we take the `sum()` of the column of `x` as the divisor for *each* observation of `x`. \n",
    "\n",
    "To put it in terms of our heights data:\n",
    "\n",
    "1. Take each person's height (which we call '_x_')\n",
    "2. Add up (sum: $\\sum$) every _x_ (which runs from 1.._n_) in the data set\n",
    "3. And divide \\#1 by \\#2\n",
    "\n",
    "But the point is that that notation applies for every data set, not just our heights data. That's where the mathematical formula is more useful – it's not linked to the specifics of this particular data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proportional Standardisation in Pandas\n",
    "\n",
    "We can calculate the proportion for each area using a similar approach to what we've seen before in the Transformation section:\n",
    "```python\n",
    "df['proportion'] = df.<column>/df.<column>.sum()\n",
    "```\n",
    "You can see the link here between the mathematical notation and the computational representation, right?\n",
    "\n",
    "Let's do this for the columns that we've been working with and try printing out various summary metrics for the new column and comparing them to the raw values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfs = df.copy() # Copy df to df-standardised\n",
    "\n",
    "for c in ['RoadsArea','NOxmax','Owned']:\n",
    "    dfs[c] = ??? # Note that we're overwriting the column in dfs _only_\n",
    "    print(dfs[c].describe())\n",
    "    \n",
    "    fig1 = plt.figure()\n",
    "    ax1 = fig1.add_subplot(111)\n",
    "    sns.distplot(dfs[c]) \n",
    "    sns.distplot(normal_from_dist(???), hist=False, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Think!\n",
    "\n",
    "In this case are the simple standardisations very useful? Can you think why we might not be _that_ interested in these results? So if that's _not_ what we want, then what _do_ we want? It's worth thinking about what these columns are actually measuring:\n",
    "\n",
    "1. Roads Area is a measure the area (in m^2) that are near to a major road. What other column contains areal data that might be relevant?\n",
    "2. Owned is the number of properties that are owned outright. What other column contains information about the total number of potential properties (allowing for some fudging for houses that are owned by one household but shared with others)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Areal Standardisation\n",
    "\n",
    "So the most basic and obvious standardisation at this point would be to work out what the 'Roads Area' is _really_ measuring -- we've got a total area near roads but still have no idea how big the actual LSOA _is_. So it's quite possible that an outer borough has a lot of area near major roads, but a low NOx reading overall because it also has a lot of areas that are a _long_ way from major roads and their pollutants.\n",
    "\n",
    "Try copying the code block above into the block below and adjusting it for the Roads Area data so that it gives you a more useful answer. *Hint*: the right answer in _this particular case_ will have the following output:\n",
    "```python\n",
    "count   4,834.0000\n",
    "mean        0.4224\n",
    "std         0.3659\n",
    "min         0.0000\n",
    "25%         0.0086\n",
    "50%         0.3937\n",
    "75%         0.7705\n",
    "max         1.0000\n",
    "dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfs = df.copy()\n",
    "\n",
    "for c in ['RoadsArea']:\n",
    "    series = ???\n",
    "    print(series.describe())\n",
    "    \n",
    "    fig1 = plt.figure()\n",
    "    ax1 = fig1.add_subplot(111)\n",
    "    sns.distplot(series)\n",
    "    dfs[c+'Std'] = series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Household Standardisation\n",
    "\n",
    "Let's do the same for the ownership data. Here we wouldn't standardise by area (we've already got a population density measure anyway) but by something else that would give us a sense of the _density of ownership_ in an area...\n",
    "\n",
    "Here's your hint:\n",
    "```python\n",
    "count   4,835.0000\n",
    "mean        0.4988\n",
    "std         0.2252\n",
    "min         0.0095\n",
    "25%         0.3181\n",
    "50%         0.4880\n",
    "75%         0.6689\n",
    "max         0.9740\n",
    "dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dfs = df.copy() # This would overwrite the copy created above\n",
    "\n",
    "for c in ['Owned']:\n",
    "    series = ???\n",
    "    print(series.describe())\n",
    "    \n",
    "    fig1 = plt.figure()\n",
    "    ax1 = fig1.add_subplot(111)\n",
    "    sns.distplot(???)\n",
    "    dfs[???+'Std'] = series # Std == Standardised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, so now we see a range of densities for ownership between 0.0 and something just south of 1.0. So ownership varies dramatically by LSOA, though the mean is about 50%. Let's compare how our standardised data compares with the original data... I get a plot that tells me Pearson's _R_ is 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.jointplot(x=dfs.OwnedStd, y=dfs.Owned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So yes, there is a very strong relationship between the raw count of home-owning households and our estimation of the density of home-ownership, but it's _also_ not perfect. Let's see if we can dig into this further to see what has been most affected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z-Score Standardisation\n",
    "\n",
    "The z-score is a common type of standardisation, but it's a little more complex than a simple proportion; however, both are designed to enable us to compare data across different groups of observations. We can easily compare two percentages to know which one is more, and which less (e.g. I got 80% on one exam and 70% on the other).\n",
    "\n",
    "But let's think about a slightly different question: 'which exam did I do _better_ on?' What if you got 80% on an exam where everyone else got 85%? Suddenly that doesn't look quite so good right? And what if your 70% was on an exam where the average mark was 50%? Suddenly that looks a lot better, right?\n",
    "\n",
    "The z-score is designed to help you perform this comparison directly.\n",
    "\n",
    "As a reminder, the z-score looks like this:\n",
    "$$\n",
    "z=(x-\\mu)/\\sigma\n",
    "$$\n",
    "\n",
    "That's: `(<data> - <mean>)/<standard deviation>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Z-Score\n",
    "\n",
    "Let's start to bring the idea of the 'data generating processes' to life. The first thing to do with the z-score is to look at what it implies:\n",
    "\n",
    "1. Subtracting the mean implies that the mean _is a useful measure of centrality_: in other words, the only reason to subtract the mean is if the mean is _meaningful_. If you are dealing with highly skewed data then the mean is not going to be very useful and, by implication, neither is the z-score.\n",
    "2. Dividing by the Standard Deviation implies that this _is a useful measure of distribution_: again, if the data is heavily skewed or follows some exponential distribution then the standard deviation is not going to very useful as an analytical tool.\n",
    "\n",
    "So the z-score is _most_ relevant when we are dealing with something that look vaguely like a standard normal distribution (which has mean=0 and standard deviation=1). In those cases, anything with a z-score more than 1.96 standard deviations from the mean is in the 5% significance zone. \n",
    "\n",
    "But remember: we can't really say _why_ one particular area has a high concentration of well-off individuals or why one individual is over 2m tall. All we are saying is that this standardised value is a pretty unlikely outcome _when compared to our expectation that people are randomly distributed across the region_ or _that people have randomly-distributed heights of mean 'x' and standard deviation 'y'_. \n",
    "\n",
    "Of course, we _know_ that people aren't randomly distributed around the country in the same way that we know that height isn't genuinely random becuase of the influence of genetics, nutrition, etc. But we need a way to pick out what counts as _**significant**_ over- or under-concentration (or height) from the wide range of 'a bit more' or 'a bit less' than 'normal'. **_If_** a normal distribution does a good job of representing the overall distribution of heights (_whatever_ the reason) then someone of 2m is highly unlikely but we can't say _how_ unlikely until we've placed them on the distribution.\n",
    "\n",
    "Let's put it another way:\n",
    "* Is 10% of wealthy individuals in a small area a high concentration?\n",
    "* How about 20%?\n",
    "* 30%?\n",
    "\n",
    "The only way to answer that question is to use something like the z-score since it standardises all of the values _against the average_. If wealthy people were distributed at random then we would _expect_ that most areas would have about the average concentration. Some areas will have a few more. Some areas a few less. But according to the way that the standard normal distribution works, _nowhere_ should have a z-score of 10. Or 20, since that is 20 standard deviations from the mean and just shouldn't exist in the lifetime of the universe. So if we see that kind of result then we know two things:\n",
    "\n",
    "1. That our assumption that normal distribution is a reasonable representation of reality breaks down at some point.\n",
    "2. That there are _some_ areas with _highly significant_ over- or under-representation by wealthy residents.\n",
    "\n",
    "But that's ok, because we're trying to set an expectation of what we think we'll see so that we can pick out the significant outliers.\n",
    "\n",
    "### Setting Expectations\n",
    "\n",
    "And here we get to the crux of the issue, most frequentist statistics boils down to this: subtracting **what you expected** from **what you got**, and then dividing by **some measure of spread** to control for the range of the data. We then look at what's _left_ to see if we think the gap between expectations and observations is _meaningful_ or whether it falls within the range of 'random noise'.\n",
    "\n",
    "It should be obvious that it's the _**expected**_ part of that equation that is absolutely crucial: we come up with a process that generates data that _represents_ the most important aspects of reality, but we shouldn't imagine that our process has explained them. It's the first step, not the last.\n",
    "\n",
    "### Illustrating Z-Score Selection\n",
    "\n",
    "So let's see how this works... let's take two of the NS-SeC groups: Group 1 and Group 4. And let's investigate their distributions and whether they contain obvious outliers (extremes that shouldn't exist if they were to follow a normal – which is to say: random – distribution). But we _can't_ do this with the raw counts because the raw counts are truncated (you can't have -1 Group 1 households, only positive counts). We'll use the z-score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-Score Standardisation in Pandas\n",
    "\n",
    "Let’s take our ownership column and calculate a new series called `OwnedZStd`!\n",
    "\n",
    "One way to do this is would be to do it in two stages: subtract the mean to create a new series, then divide by the standard deviation into another new series (_note:_ `<column>` is just a generic name; e.g. substitute 'Owned' for column to create actual code):\n",
    "```python\n",
    "dfs['<column>LessMean'] = dfs.<column> - dfs.<column>.mean()\n",
    "dfs['<column>ZStd']     = dfs.<column>LessMean / dfs.<column>.std()\n",
    "```\n",
    "Let's be clear: that solution is _perfectly acceptable_ and _perfectly accurate_. It is also, however, less elegant since it creates extra columns of data that are really only temporary variables that don't need to be added to the data frame.\n",
    "\n",
    "By now you've seen how we can 'chain' together method calls, then you should know that we could _also_ do it this way:\n",
    "```python\n",
    "df[<column>] = \n",
    "     (df.<column> - df.<column>.mean()) / df.<column>.std()\n",
    "```\n",
    "Do you see how we can begin to build increasingly complicated equations into the process of creating a new data series?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the z-score to select outliers\n",
    "\n",
    "Let's try finding outliers in the raw and standardised data series that we creates above using z-scores. We'll **use the 90% Confidence Interval equivalent** (have a look at the table above to find out the right z-score to use). Below that is some quite complex code that I'll talk you through once you've figured out how to do the z-score selection.\n",
    "\n",
    "You should get the following results:\n",
    "```python\n",
    "Using raw data we got: 10.0124%\n",
    "Using standardised data we got: 9.5987%\n",
    "Raw outliers: \n",
    "          LAD11NM  count\n",
    "4         Bromley     46\n",
    "27      Southwark     40\n",
    "15       Havering     37\n",
    "29  Tower Hamlets     35\n",
    "11        Hackney     34\n",
    "2          Bexley     27\n",
    "5          Camden     20\n",
    "7         Croydon     19\n",
    "10      Greenwich     16\n",
    "21        Lambeth     16\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Outliers according to raw scores\n",
    "raw_outliers = dfs[np.abs( (??? - ???)/??? ) > 1.645]\n",
    "\n",
    "# Outliers according to standardised scores\n",
    "std_outliers = dfs[???]\n",
    "\n",
    "# What percentage of observations did we select?\n",
    "print(\"Using raw data we got: {0:.4f}%\".format( 100 * (float(len(raw_outliers))/len(dfs)) ))\n",
    "print(\"Using standardised data we got: {0:.4f}%\".format( 100 * (float(len(std_outliers))/len(dfs)) ))\n",
    "\n",
    "# And how are they distributed geographically by borough?\n",
    "# Explanation of this code is below...\n",
    "print(\"Raw outliers: \")\n",
    "print(pd.DataFrame({'count': raw_outliers[['LAD11NM']].groupby('LAD11NM').size()}).reset_index().sort_values(by='count', ascending=False).head(10)) # ???\n",
    "\n",
    "print(\"Standardised outliers: \")\n",
    "print(pd.DataFrame(???) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "OK, so what's with the lines of code that report the results? Let's break it down:\n",
    "```python\n",
    "pd.DataFrame({'count': raw_outliers[['LAD11NM']].groupby('LAD11NM').size()}).reset_index().sort_values(by='count', ascending=False).head(10)\n",
    "```\n",
    "\n",
    "Let's start by reformatting this to make it easier to read:\n",
    "```python\n",
    "pd.DataFrame(\n",
    "    {'count': raw_outliers[['LAD11NM']].groupby('LAD11NM').size()}\n",
    ").reset_index().sort_values(by='count', ascending=False).head(10)\n",
    "```\n",
    "\n",
    "And now, working from the easy parts to the complex parts:\n",
    "1. You can see that `pd.DataFrame(...)` is creating a new data from something to do with the `raw_outliers`.\n",
    "2. If you carefully match the parentheses then you can see where the `(...)` ends:\n",
    "   * `pd.DataFrame(...).reset_index().sort_values(by='count', ascending=False).head(10)`\n",
    "   * Let's skip `reset_index` for a minute...\n",
    "   * `sort_values(by='count', ascending=False)` is something you've seen before, but you might be wondering where 'count' came from since that's not a column in `dfs`\n",
    "   * `head(10)` is obvious, and we've just tacked it on to the output of the `sort_values` method, so hopefully this makes a bit of sense as take the 10 largest values of `count`.\n",
    "3. If you look closely you'll see that there is a `count` also inside the `pd.DataFrame()` method call to create a new data frame\n",
    "   * `{'count': ...}` is the syntax for creating a column named 'count' in a new data frame using the dictionary syntax!\n",
    "   * `raw_outliers[['LAD11NM']].groupby('LAD11NM').size()` is the name syntax that we saw earlier for creating a count of LSOAs by borough that fell into the upper and lower outliers using standard deviations...\n",
    "4. The final bit of magic is the `reset_index()` which takes the results of the `groupby` function and turns the whole thing into a new data frame that no longer has the information about the original LSOAs.\n",
    "\n",
    "I couldn't remember how to do this myself, so I started with a Google search on `convert groupby to dataframe` and then built the code from there..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that, while there is overlap between the two reports there are also major differences. I would argue that the second output (from the standardised data) is more _meangingful_ because we've controled for the fact that LSOAs are of different sizes and have different populations. Here's a quick way to drive home the differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "v1 = sns.violinplot(x='LAD11NM', y='Owned', data=dfs, ax=ax)\n",
    "for item in v1.get_xticklabels():\n",
    "    item.set_rotation(90)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "v2 = sns.violinplot(x='LAD11NM', y='OwnedStd', data=dfs, ax=ax)\n",
    "for item in v2.get_xticklabels():\n",
    "    item.set_rotation(90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust Transformations (_e.g._ IQR)\n",
    "\n",
    "You will recall that we can use the IQR to standardise data where there are outliers. Since the IQR is _robust_, this qualifies as a robust transformation or, as the Python library sci-kit learn (a.k.a. _sklearn_) calls it: a [Robust Scaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html). We can use it together with a transformation to handle something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfs['NOxmaxT'] = ??? # Log-transform the NOxmax column\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "sns.distplot(dfs.NOxmax)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "sns.distplot(dfs.NOxmaxT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The transformed NOx-max is clearly still not normal, so the z-score would be a poor choice of standardisation. Let's try using sklearn instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Try changing the quantile ranges and seeing how this plot changes...\n",
    "rs = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True) \n",
    "dfs['NOxStd'] = rs.fit_transform(dfs[['NOxmaxT']])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "sns.distplot(???) # What are we plotting here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how this is still, fundamentally the _same_ data, we've just mucked about with its position and scale on the axis! If you _really_ care about normality, then there's a [Box-Cox transform](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html) in `scipy.stats` to get something that is almost _exactly_ normal.\n",
    "\n",
    "**Seeing how it works**: sklearn also has a really nice illustration of the different scalers and their effect on a data set. This is _well_ worth a read as it shows how massively the shape of the data can be impacted by transformation and standardisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(15,5), sharex=True, sharey=True)\n",
    "sns.regplot(ax=ax, x=dfs.RoadsAreaStd, y=dfs.NOxStd, scatter_kws={\"s\":10, \"color\":\"r\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4**. How does this last plot change your understanding of the relationship (if any) between roads and NOx pollution?\n",
    "\n",
    "**A4**. _Your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another exploration\n",
    "\n",
    "Right, so that was one pair of variables. I'd like you to tackle the same kind of work using _either_ (or _both_, if you have time) of the following pairs:\n",
    "\n",
    "1. Owned and White\n",
    "2. PrivateRented and Asian\n",
    "\n",
    "You're free to use others if you really want, but these two should have interesting relationships that you can explore and expand on using a combination of transformation and standardisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "This is quite a lot of content, so I'd strongly suggest that you take a little while to _think_ about what you've done and how this has helped you to tease out subtle relationships between variables with greater ease. We've 'tortured' the data (up to a point) and it has 'confessed', but it's now up to us to make sense of whether that confession was forced or has actually reveled something meaningful..."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [gsa2017]",
   "language": "python",
   "name": "Python [gsa2017]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
