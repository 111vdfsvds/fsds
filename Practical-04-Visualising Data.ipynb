{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "<h1>Visualising Data</h1>\n",
    "<h2>7SSG2059 Geocomputation 2017/18</h2>\n",
    "<h3>_Jon Reades_</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Week’s Overview\n",
    "\n",
    "This week we're going to explore how visualising data (using pandas and seaborn) helps us to make more sense of our data. We're then going to move on to automating this process because coding isn't _just_ about being able to load lots of data, it's also about being able to be constructively lazy with the data once it's loaded.\n",
    "\n",
    "## Learning Outcomes\n",
    "\n",
    "By the end of this practical you should:\n",
    "- Have created a set of different plots using Seaborn\n",
    "- Have automated the presentation of data for a number of columns\n",
    "- Have grasped how a mix of graphs and numbers can help you make sense of your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working through the Data\n",
    "\n",
    "### The LSOA Data\n",
    "\n",
    "You've already worked with the LSOA data in the previous week, so we won't revisit the details now; however, it is worth changing the way that we access the data each week -- rather than having to copy+paste several lines of code from Practical 3 to Practical 4, I'm giving you an example of how _automation_ can help. We know that we're going to need to download a copy of the data each week, so let's create a function that _encapsulates_ this process for us _and_ add a little 'intelligence' along the way...\n",
    "\n",
    "Try adding comments to the function below to make sure that you understand how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def download_gsa_data(src='http://bit.ly/2g4Iz3v', dst='lsoa.csv.gz', srczip='gzip'):\n",
    "    \"\"\"\n",
    "    A simple function to help simplify the process of downloading\n",
    "    and saving data to use in a practical. To save on bandwidth, \n",
    "    the function will first check whether a local copy of the file\n",
    "    exists (as specified by the dst parameter) before attempting to\n",
    "    read it from the remote source (as specified by the src parameter).\n",
    "    \n",
    "    \n",
    "    Keyword arguments:\n",
    "    src -- a remote URL to use as a source (default http://bit.ly/2g4Iz3v)\n",
    "    dst -- a local path to use as a destination (default lsoa.csv.gz)\n",
    "    \"\"\"\n",
    "    dst_compression='gzip'\n",
    "    \n",
    "    if os.path.exists(dst):\n",
    "        print(\"File already downloaded.\")\n",
    "        df = pd.read_csv(dst, compression=dst_compression, low_memory=False)\n",
    "        \n",
    "    else:\n",
    "        print(\"Downloading remote data...\")\n",
    "        \n",
    "        if srczip is not None:\n",
    "            df = pd.read_csv(src, compression=srczip, low_memory=False)\n",
    "        else:\n",
    "            df = pd.read_csv(src, low_memory=False)\n",
    "        \n",
    "        print(\"Writing to local file...\")\n",
    "        df.to_csv(dst, compression=dst_compression, index=False)\n",
    "    print(\"Done.\")\n",
    "    return df\n",
    "\n",
    "df = download_gsa_data()\n",
    "df.set_index(['LSOA11CD'], drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Data Exploration\n",
    "\n",
    "Let's start with some really obvious questions about the data. We looked at distributions and min/max values using built-in pandas methods (like `describe()`) the previous week, but that doesn't tell us _which_ places are very expensive/large/whatever. \n",
    "\n",
    "Let's start by examining the mean price for Airbnb properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[???].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's figure out where the highest means can be found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.sort_values(by='MeanPrice', ascending=False).head(5)[['MeanPrice','LSOA11NM']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That last command is actually quite complex, so let's take a second to unpack it:\n",
    "```python\n",
    "df.sort_values(by='Mean Price', ascending=False).head(5)[['Mean Price','LSOA11NM']]\n",
    "```\n",
    "\n",
    "1. Take the data frame `df`;\n",
    "2. Sort it using the `Mean Price` column by descending order (_i.e._ `ascending=False`);\n",
    "3. Take the first five values (`head(5)`);\n",
    "4. And print out the columns specified by the list (`['Mean Price','LSOA11NM']`).\n",
    "\n",
    "Let's pull it apart step-by-step at the code level:\n",
    "\n",
    "* The first step in this process is `df.sort_values` -- you can probably guess what this does: it sorts the data frame!\n",
    "* The parameters passed to the `sort_values(...)` function are `by`, which is the column on which to sort, and `ascending=False`, which gives us the data frame sorted in _descending_ order!\n",
    "* The output of `df.sort(...)` is _itself_ a new data frame, which means that we can simply add `.head(5)` to get the first five rows of the newly-sorted data frame.\n",
    "* And the output of `df.sort(...).head(...)` is yet _another_ data frame, which means that we can print out the values of selected columns using the 'dictionary-like' syntax: we use the outer set of square brackets (`[...]`) to tell pandas that we want to access a subset of the top-5 data frame, and we use the inner set of square brackets (`['Mean Price','LSOA11NM']`) to tell pandas which columns we want to see.\n",
    "\n",
    "I'd say 'simples, right?' but that's obviously _not_ simple. It _is_, however, very, very _elegant_ because it's quite clear (once you get past the way that lots of commands can be chained together) and it's very succinct (we did all of that in _one_ line of code!).\n",
    "\n",
    "**Q2.** Now, can you output the Mean and Median prices (as well as the LSOA11NM) for the 5 _cheapest (by median price)_ LSOAs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.sort_values(???).???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all has gone well (using the full data set) you _might_ have:\n",
    "\n",
    "| LSOACD | Mean Price | Median Price | LSOA11NM |\n",
    "|--------|------------|--------------|----------|\t\n",
    "| E01002469 | 0.0 | 0.0 | Hillingdon 010D |\n",
    "| E01002151\t| 0.0\t| 0.0\t| Harrow 006D |\n",
    "| E01000662\t| 0.0\t| 0.0\t| Bromley 026B |\n",
    "| E01000663\t| 0.0\t| 0.0\t| Bromley 026C |\n",
    "| E01000666\t| 0.0\t| 0.0\t| Bromley 033B |\n",
    "\n",
    "Not perhaps the _most_ useful thing to know -- it would probably be more useful to know the cheapest that _is not zero_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[???].sort_values(by='MedianPrice', ascending=True).head(5)[['MeanPrice','MedianPrice','LSOA11NM']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cheapest _non_-zero LSOA should be Newham 032B (£9/night)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with a Data Series\n",
    "\n",
    "Implicitly, we've already done quite a bit with the Series (i.e. column) class offered by pandas, but I want to revisit it so that you understand why getting to grips with how the series works (especially the 'index', which is a special type of series) is crucial to getting the most out of pandas.\n",
    "\n",
    "The easiest way to think about this: a `Series` is just another name for a pandas column.\n",
    "\n",
    "### Adding a New Series\n",
    "\n",
    "You may recall that you can add a new series to an existing data frame using the dictionary-like syntax:\n",
    "```python\n",
    "df['<new series name>'] = pd.Series(... <series definition> ...)\n",
    "``` \n",
    "But just to remind you: see how familiar that syntax is? `df['key'] = value` is _exactly_ like creating and assigning a new key/value pair to a dictionary called `df`! The only difference here is that the 'value' we store in the dictionary is a Series object, and not a simple variable (String, int, float).\n",
    "\n",
    "Let's do this for the price so that we get a sense of how expensive a place is _relative_ to all other listings... Perhaps something like standard deviations from the mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['score'] = pd.Series((df['MeanPrice'] - df['MeanPrice'].mean()) / df['MeanPrice'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.score.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's step through the code to make sense of what we just did:\n",
    "```python\n",
    "series = pd.Series((df.price - df.price.mean()) / df.price.std())\n",
    "```\n",
    "That doesn't look _exactly_ the same, but that's only because our column name has a space in it ('Mean Price'). If it were 'mean_price' or, more simply, 'price' then we could use the easier-to-read syntax above.\n",
    "\n",
    "Aaaaanyway:\n",
    "1. `pd.Series(...)` creates a new pandas Series from whatever data we pass it;\n",
    "2. `(df.price - df.price.mean())` does something really clever: it takes each _individual_ row value of the `price` column and _subtracts_ the average price of the entire column using `df.price.mean()`.\n",
    "3. We then divide that by the standard deviation, which we just calculated for the _entire_ column using `df.price.std()`!\n",
    "\n",
    "Doing this in Excel would be a bit more work and, more importantly, a _lot_ slower. This is especially true if what you want to do is get things working on a subset of the data _before_ analysing the entire data set in one go. Imagine doing this for 25,000,000 records and you begin to see how _scripability_ is incredibly useful here.\n",
    "\n",
    "**Q3.** What's the statistical term for what we've just calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's in a Score?\n",
    "\n",
    "Do you remember any GCSE statistics? What this pandas command does:\n",
    "```python\n",
    "(df.price - df.price.mean()) / df.price.std()\n",
    "```\n",
    "\n",
    "...is standardisation (more on this near the end of term):\n",
    "\n",
    "$$\n",
    "z = \\frac{x - \\bar x}{\\sigma}\n",
    "$$\n",
    "\n",
    "The number that we have now tells us how far from the _average_ price of properties in London _each_ individual property is as a multiple of the standard deviation. So something priced at the average would have a value of `0`, while the most expensive are many times that..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.sort_values(by='score', ascending=False).sample(5, random_state=123456789)[['MeanPrice','score','LSOA11NM']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.** Using the output of the `sample(5)` function what can you _guess_ about what the mean price of an Airbnb property in London might be? What is the potential problem with taking the mean of the mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising Data\n",
    "\n",
    "OK, we've added one useful new column (data series) to our data frame just to see how it's done, but now let's get down to some visualisation!\n",
    "\n",
    "### Start with a Chart\n",
    "\n",
    "If we weren't learning how to program at the same time as we learn to do data analysis then my recommendation would have been this: **start with a chart**. Of course, we mean start with a _good_ chart:\n",
    "\n",
    "[![Do maps lie?](http://img.youtube.com/vi/hYaoE4Kh9fk/0.jpg)](http://www.youtube.com/watch?v=hYaoE4Kh9fk)\n",
    "\n",
    "There is _no_ better tool for understanding what is going on in your data than to visualise it, but we couldn't show you how to make a plot without first teaching you how to load data and perform some basic operations on a data frame! \n",
    "\n",
    "Now that we've done *that*, we can get to grips with VDQI (the [Visual Display of Quantitative Information](https://www.edwardtufte.com/tufte/books_vdqi) and how this supports our understanding of the data.\n",
    "\n",
    "### Why Seaborn?\n",
    "\n",
    "We can do some straightforward plotting directly from pandas, but for the data visualisation part of the practical we are going to use the [Seaborn package](http://stanford.edu/~mwaskom/software/seaborn/) because it provides a lot of quite complex functionality (and very pretty graphs) at quite low 'cost' (_i.e._ effort). There are, however, other options out there that are worth checking out if you take things further; the two that you are most likely to hear mentioned are: [Bokeh](http://bokeh.pydata.org/en/latest/) and matplotlib. \n",
    "\n",
    "1. Bokeh is, like, Seaborn designed to make it easy for you to create good-looking plots with minimal effort. \n",
    "2. Matplotlib is a different beast: it is actually the _underlying_ package that supports the majority of plotting (drawing graphs) in Python. \n",
    "\n",
    "So Seaborn and Bokeh both make use of the matplotlib library to create their plots, and if you want to customise a figure from either of these two libraries then you will eventually need to get to grips with matplotlib. The reason we don't teach matplotlib directly is that it's much harder to make a good plot and the syntax is much more complex.\n",
    "\n",
    "A more recent entrant is yhat's (a data science 'joke') ggplot library, which deliberately mimics R’s ggplot2 (http://ggplot.yhathq.com) -- this library has become the dominant way of creating plots in the R programming language and it uses a 'visualisation' grammar that many people find incredibly powerful and highly customisable. Unfortunately, ggplot on Python does not currently support mapping (which R does in ggplot2).\n",
    "\n",
    "### Loading Seaborn \n",
    "\n",
    "As with other libraries that we’ve used, we’ll import Seaborn using an alias:\n",
    "```python\n",
    "import seaborn as sns\n",
    "```\n",
    "So to access Seaborn's functions we will now always just write `sns.<function name>()` (where `<function name>` would be something like `distplot`). \n",
    "\n",
    "### But First!\n",
    "\n",
    "<span style=\\\"color:red;\\\">Important Note for Mac Users</span>\n",
    "\n",
    "Recent changes in the way that the Mac OS handles the plotting of data means that you need to do certain things in a specific order at the start of any notebook in which you intend to show maps or graphs. Please make a copy of the following code for any notebook that you create and make it the _first_ code that you run in the notebook:\n",
    "\n",
    "```python\n",
    "# Needed on a Mac\n",
    "import matplotlib as mpl\n",
    "mpl.use('TkAgg')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "\n",
    "For non-Mac users the code above should run fine, but _should_ also note be entirely necessary:\n",
    "```python\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "This _should_ enable you to create plots, including in the practical that we're about to start! If you forget to run this code _before_ trying to use Seaborn then you will probably need to restart the Kernel (Kernel > Restart from the menu). If you do _that_ you will lose all of your 'live' work (_i.e._ variables, loaded modules, etc.).\n",
    "\n",
    "The `%matplotlib inline` command only need to be run _once_ in a jupyter notebook; it tells jupyter to show the plots as part of the web page, rather than trying to show them in a separate window. So the easiest thing to do is just stick whatever code you need at the top of your notebook so that you _always_ run it once when you start up a notebook and can then forget about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Needed on a Mac\n",
    "import matplotlib as mpl\n",
    "mpl.use('TkAgg')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a Distribution Plot\n",
    "\n",
    "One of the most useful ways to get a sense of a data series is simply to look at its overall distribution. Something like this:\n",
    "```python\n",
    "sns.distplot(<data series>)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note the x-axis range\n",
    "sns.distplot(df['Mean Price'], hist=False)\n",
    "sns.distplot(df['Median Price'], hist=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note the x-axis range!\n",
    "sns.distplot(df.score, kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all at went well you should see an 'ok' (kind of 'meh', really) distribution plot for the price! Let's try it *after* removing listings with z-scores greater than 8 instead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note the x-axis range!\n",
    "# Note the switch from a histogram to a KDE\n",
    "# What else can you do...\n",
    "#?sns.distplot\n",
    "sns.distplot(df['Mean Price'][df.score.abs() < 5], hist=False, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap\n",
    "\n",
    "OK, I want you to take a second here: although there was a lot of setup work that needed to be done, we just created a distribution plot in one line of code _after_ filtering out values that we felt were skewing our view of the data. _One line_. This is a more sophisticated plot than you could ever create in Excel and you just created it in one line of code!\n",
    "\n",
    "Let's review:\n",
    "```python\n",
    "sns.distplot(df.price[df.score.abs() < 5])\n",
    "```\n",
    "We are:\n",
    "1. Creating a seaborn distribution plot (`sns.distplot(...)`)\n",
    "2. Using the price column (`df.price`)\n",
    "3. But first selecting only those data where the absolute value of the standard deviation was less than 8 (`df.stdev.abs() < 5`).\n",
    "\n",
    "You'll notice too that list-type syntax: `df.price[...]` which tells us that we're selecting elements of the list in the same way that we selected column names above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automation\n",
    "\n",
    "Try producing similar plots for some of the other groups. You should be able to do this using two things:\n",
    "* A `for` loop to iterate over the column names\n",
    "* String interpolation to access the column in the data frame\n",
    "\n",
    "How about we try: `'Area that is Designated Greenspace', 'Owned', 'Asian/Asian British', 'Private room'`? (**Note:** you'll need to work out the simple names of these!)\n",
    "\n",
    "I'll get you started... this also demonstrates how we can use pandas' dictionary-style notation to create a new string that we can then use as a key for a new data series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in [???]:\n",
    "    plt.figure() # Create a new figure so they don't over-print\n",
    "    sns.distplot(df[???], hist=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a Plot\n",
    "\n",
    "But... wouldn’t be it a lot easier if we could save our plot automatically and not have to even touch a button to do so? This is where we need to use matplotlib syntax (and where you'll see why we opted not to spend too much time on it):\n",
    "\n",
    "```python\n",
    "series = df.price \n",
    "fig = plt.figure(series.name)\n",
    "sb.distplot(series)\n",
    "fig = plt.gcf()\n",
    "plt.savefig(\"{0}-Partially Automated.pdf\".format(series.name), bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "```\n",
    "\n",
    "To explain what's happening here: \n",
    "\n",
    "1. We import a sub-package of `matplotlib` that gives us access to all kind of useful functions (We've actually done this already, which is why this line is commented out).\n",
    "2. We copy the price data series to a new variable (this would mean that, to print out weekly_price or monthly_price we'd only have to change this one line... see: we're preparing to use a `for` loop!)\n",
    "3. We create a `figure` object into which Seaborn can 'print' its outputs.\n",
    "4. We call Seaborn and ask it to print (it doesn't really need to know that it's printing to something, it just nees to know what 'device' is should use for output).\n",
    "5. We save the plot, using the `format` command to replace the `{0}` with the name of the data series (so in this example we'd be saving our figure to `price-Test.pdf`).\n",
    "6. We then close the figure output so that we don't print other plots over top of it -- that's what happened in the earlier code when we used the `for` loop!\n",
    " \n",
    "The plot should have been saved to your working directory (where the Jupyter notebook is running) using the name of the data series that you were working with. We want to use string replacement (the `{0}`) so that when we save the plot for weekly_price or monthly_price we don't overwrite the one for price!\n",
    "\n",
    "Let's try saving the square footage data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "series = ???                  # Copy the home ownership data series to a new variable\n",
    "fig = plt.figure(series.name) # Create a figure using a title based on the series name\n",
    "sns.distplot(series)          # Plot the series to the open 'figure' device\n",
    "fig = plt.gcf()               # *G*et the *C*urrent *F*igure environment so that the next command works\n",
    "plt.savefig(\"{0}-Partially Automated.pdf\".format(series.name), bbox_inches=\"tight\")\n",
    "plt.close()                   # Close the plot so that nothing else overwrites our work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automation\n",
    "\n",
    "When we're undertaking an analysis of a data set, we often have to perform the same (or at least similar) tasks for many different columns. We *could* copy and paste the code, and then just change the variable names to update the analysis... but that would be a definite instance of what Larry Wall would have called 'false laziness': it seems like a time-saving device in the short run, but in the long run you've made your code less readily maintainable (what if you want to _add_ to your analysis or find a bug?) and less easy to understand.\n",
    "\n",
    "There are nearly always two things that you should look at if you find yourself repeating the same code: \n",
    "1. write a for loop; \n",
    "2. consider writing a function.\n",
    "\n",
    "**Q5.** Why do you think that we use these two strategies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a For Loop\n",
    "\n",
    "Let’s start with a for loop to generate summary statistics for several of our data columns -- you’ve already typed all of this code more than once so all we need to do is take the right bits of it and put them in a for loop like this:\n",
    "```python\n",
    "for c in ...:\n",
    "    series = df[c] # Get the series\n",
    "    print('Summarising ' + series.name)\n",
    "    print(\"\\tMean:    {0:> 9.2f}\".format(series.mean()))\n",
    "    print(\"\\tMedian:  {0:> 9.2f}\".format(series.median()))\n",
    "```\n",
    " \n",
    "All you need to do is figure out which data series go into the for loop in the first place and then your code will run for each one, producing a nicely formatted summary for each... we hope!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in df.columns:\n",
    "    series = df[c]\n",
    "    print('Summarising ' + series.name)\n",
    "    print(\"\\tMean:    {0:> 9.2f}\".format(series.mean()))\n",
    "    print(\"\\tMedian:  {0:> 9.2f}\".format(series.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as you've just now discovered you can't do this for every column because not every one is numeric, so it's time to introduce a useful little 'emergency' handler for what Python calls '[exceptions](https://docs.python.org/2/tutorial/errors.html)':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in df.columns[2:5]: # Notice that we start with some testing!!!\n",
    "    series = df[c]\n",
    "    try:              # Try to do something\n",
    "        print('Summarising ' + series.name)\n",
    "        print(\"\\tMean:    {0:> 9.2f}\".format(series.mean()))\n",
    "        print(\"\\tMedian:  {0:> 9.2f}\".format(series.median()))\n",
    "    except TypeError: # If you see this problem don't blow up please!\n",
    "        print(\"\\tData cannot be summarised numerically.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare that output with the output below and then answer this question:\n",
    "\n",
    "**Q6.** Why do we see the output for columns like `MSOA11CD` 'twice' even though we don't expect it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in df.columns[2:5]:\n",
    "    series = df[c]\n",
    "    try:              # Try to do something\n",
    "        print('Summarising ' + series.name)\n",
    "        print(\"\\tMean:    {0:> 9.2f}\".format(series.mean()))\n",
    "        print(\"\\tMedian:  {0:> 9.2f}\".format(series.median()))\n",
    "    except TypeError: # If you see this problem don't blow up please!\n",
    "        print(\"Series \" + series.name + \" cannot be summarised numerically.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what the `except` does is allow us to intercept the error _before_ Python simply gives up and throws an error at you. For automation this is a really useful feature since it allows us to figure out if there are problems and do something about them before the user is left trying to figure out what went wrong! You should read up on these as they are very useful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Graphs\n",
    "\n",
    "OK, so now we've got the `for` loop working and can handle issues relating to whether or not the column is actually numeric. So we've solved _one_ part of the problem. We can now move on to the _next_ part of the problem: creating a chart from each of the numeric columns.\n",
    "\n",
    "To help us make sense of the data it will be useful to add some additional information to our distribution plots: lines to show the location of the mean, median, and outlier thresholds.\n",
    "\n",
    "To do this, we need to get at the library that Seaborn itself uses: `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup work -- enables parameterisation\n",
    "series = df['MeanPrice'][df.score.abs() <= 6]\n",
    "fig = plt.figure(series.name)\n",
    "\n",
    "# Create the plot\n",
    "d    = sns.distplot(series)\n",
    "# Find the limits\n",
    "ymin = d.get_ylim()[0]\n",
    "ymax = d.get_ylim()[1]\n",
    "\n",
    "# Now add mean and median\n",
    "plt.vlines(series.mean(), ymin, ymax, colors='red', linestyles='solid', label='Mean')\n",
    "plt.vlines(series.median(), ymin, ymax, colors='green', linestyles='dashed', label='Median')\n",
    "\n",
    "# Add outlier marks (more than 1.5 times the IQR above or below the 1st and 3rd quartiles)\n",
    "iqr = series.quantile(0.75)-series.quantile(0.25)\n",
    "if series.quantile(0.25)-1.5*iqr > 0:\n",
    "    plt.vlines(series.quantile(0.25)-1.5*iqr, ymin, ymax, colors='blue', linestyles='dotted', label='Lower Outlier')\n",
    "if series.quantile(0.75)+1.5*iqr > 0:\n",
    "    plt.vlines(series.quantile(0.75)+1.5*iqr, ymin, ymax, colors='blue', linestyles='dotted', label='Upper Outlier')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What just happened???\n",
    "?plt.vlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that you can also type `help(plt.vlines)` to discover what parameters a function takes. \n",
    "\n",
    "**Q7.** Why do you think I put an `if` condition on the outlier lines? Would this always be appropriate? Why? Why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Function\n",
    "\n",
    "Now that we've got the figure looking right for _one_ column, why don't we try to create a useful _function_ -- we _can_ always do this directly within the `for` loop, but a function is more elegant since it makes it simpler to see what is going on. I've created the basics of the function for you to illustrate this, but you'll need to do some extra legwork to figure out how to print out all of your graphs and summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_plot(s):\n",
    "\n",
    "    fig = plt.figure(s.name)\n",
    "\n",
    "    # Create the plot\n",
    "    d    = sns.distplot(s.dropna())\n",
    "    # Find the limits\n",
    "    ymin = d.get_ylim()[0]\n",
    "    ymax = d.get_ylim()[1]\n",
    "\n",
    "    # Now add mean and median\n",
    "    plt.vlines(s.mean(), ymin, ymax, colors='red', linestyles='solid', label='Mean')\n",
    "    plt.vlines(s.median(), ymin, ymax, colors='green', linestyles='dashed', label='Median')\n",
    "\n",
    "    # Add outlier marks (more than 1.5 times the IQR above or below the 1st and 3rd quartiles)\n",
    "    iqr = s.quantile(0.75)-series.quantile(0.25)\n",
    "    if s.quantile(0.25)-1.5*iqr > 0:\n",
    "        plt.vlines(s.quantile(0.25)-1.5*iqr, ymin, ymax, colors='blue', linestyles='dotted', label='Lower Outlier')\n",
    "    if s.quantile(0.75)+1.5*iqr > 0:\n",
    "        plt.vlines(s.quantile(0.75)+1.5*iqr, ymin, ymax, colors='blue', linestyles='dotted', label='Upper Outlier')\n",
    "\n",
    "    fig = plt.gcf() # *G*et the *C*urrent *F*igure environment so that the next command works\n",
    "    plt.savefig(\"{0}-Fully Automated.pdf\".format(s.name), bbox_inches=\"tight\")\n",
    "    plt.close() \n",
    "\n",
    "for c in df.columns[1:20]:\n",
    "    series = df[c]\n",
    "    print('Summarising ' + series.name)\n",
    "    try:              # Try to do something\n",
    "        \n",
    "        # Print numerical summaries\n",
    "        print(\"\\tMin:     {0:> 9.2f}\".format(series.min()))\n",
    "        print(\"\\tMax:     {0:> 9.2f}\".format(series.max()))\n",
    "        print(\"\\tMean:    {0:> 9.2f}\".format(series.mean()))\n",
    "        print(\"\\tMedian:  {0:> 9.2f}\".format(series.median()))\n",
    "        print(\"\\tIQR:     {0:> 9.2f}\".format(series.quantile(0.75)-series.quantile(0.25)))\n",
    "        \n",
    "        # Create graphical summary\n",
    "        print(\"\\tCreating graph...\")\n",
    "        create_plot(series)\n",
    "        \n",
    "    except TypeError: # If you see this problem don't blow up please!\n",
    "        print(\"\\tData cannot be summarised numerically.\")\n",
    "    #???: # Fix the error from MSOA11CD, MSOA11NM, etc.\n",
    "    #    print(\"\\tProblem formatting string.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Types of Plots\n",
    "\n",
    "Using our data frame, let’s have a look at some other types of plots... We're also going to be playing with [aesthetics](https://seaborn.pydata.org/tutorial/aesthetics.html) that change how your plot looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "fig, ax = pyplot.subplots(figsize=(10,10))\n",
    "b = sns.boxplot(ax=ax, \n",
    "        x='LAD11NM', \n",
    "        y='MeanPrice', \n",
    "        data=df.sort_values(by='MeanPrice'), \n",
    "        palette='PRGn', \n",
    "        fliersize=4, \n",
    "        linewidth=1)\n",
    "plt.ylim(0, 800)\n",
    "sns.despine(offset=10, trim=True)\n",
    "for item in b.get_xticklabels():\n",
    "    item.set_rotation('vertical')\n",
    "plt.title(\"Price Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "fig, ax = pyplot.subplots(figsize=(10,10))\n",
    "b = sns.violinplot(ax=ax, \n",
    "        x='LAD11NM', \n",
    "        y='MedianPrice', \n",
    "        data=df.sort_values(by='MedianPrice'), \n",
    "        palette='PRGn', \n",
    "        fliersize=4, \n",
    "        linewidth=1)\n",
    "plt.ylim(0, 800)\n",
    "sns.despine(offset=10, trim=True)\n",
    "for item in b.get_xticklabels():\n",
    "    item.set_rotation('vertical')\n",
    "plt.title(\"Price Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.regplot(x='MeanPrice', y='PropertyCount', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.lmplot(x='MedianIncome', y='ORbedsM1', col='LAD11NM', data=df,\n",
    "          col_wrap=3, ci=None, palette='muted', size=4, \n",
    "          scatter_kws={'s':50, 'alpha': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.jointplot(df['MedianIncome'], df['ORbedsM1'], color='#4CB391')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Plots\n",
    "Finally, I wanted to show you how to create a 3D scatter plot – in this case the plot doesn’t add a lot to our understanding of the data, but there are cases where it might and it does illustrate how pandas, seaborn, and matplotlib work together to produce some pretty incredible outputs.\n",
    "  \n",
    "I would encourage you to look into the options in more detail:\n",
    "* Can you change the colour map used to indicate which neighbourhood each listing is drawn from?\n",
    "* Can you change the icons used to mark each neighbourhood so that they are different?\n",
    "* Can you add a legend to indicate which marker is for which area?\n",
    "\n",
    "On with the show!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.colors as colours \n",
    "import matplotlib.cm as cmx\n",
    "\n",
    "# Set up the figure\n",
    "w, h = 12, 8\n",
    "fig = plt.figure(figsize=(w, h))\n",
    "ax  = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Set up the 3D axes\n",
    "x = df['BedsHH']\n",
    "y = df['MedianIncome']\n",
    "z = df['PrivateRented']\n",
    "\n",
    "# Set up the colourmap so that we see\n",
    "# different colours for each borough's\n",
    "# data.\n",
    "# From: http://stackoverflow.com/questions/28033046/matplotlib-scatter-color-by-categorical-factors\n",
    "boroughs  = list(set(df.LAD11NM)) \n",
    "hot       = plt.get_cmap('hot')\n",
    "cNorm     = colours.Normalize(vmin=0, vmax=len(boroughs))\n",
    "scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=hot)\n",
    "\n",
    "for i in xrange(len(boroughs)):\n",
    "    indx = df.LAD11NM==boroughs[i]\n",
    "    ax.scatter(x[indx], y[indx], z[indx], c=scalarMap.to_rgba(i), marker='.')\n",
    "\n",
    "ax.set_xlabel(x.name)\n",
    "ax.set_ylabel(y.name)\n",
    "ax.set_zlabel(z.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally!\n",
    "\n",
    "**Q7.** What is the _fundamental_ weakness of what we've done here? We have produced quite a few plots, and we've got the capacity to produce more, but we're overlooking something important in all this... It's a big picture question, not a code question.\n",
    "\n",
    "## Last task (time permitting)\n",
    "\n",
    "Now redo all of this with the _full_ data set! This is like building a _script_: you want to be able to click 'run' once and have a full analysis complete for the entire data set. So you'll need to select the bits of code that you want and then reassemble them so that they run in a nice, automated way without any user intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copy+paste code to here that you need to run the analysis\n",
    "# using the full data set (including outputing of charts and\n",
    "# summary stats) in one go. This will test if you reall 'got'\n",
    "# the hang of what we've done. You should also restart the \n",
    "# kernel to clear the existing variables from memory:\n",
    "#    Kernel > Restart [Note: not Restart & Clear Output]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More About the AirBnB Data\n",
    "\n",
    "You'll have noticed that there is some data about AirBnB properties in London. [Inside Airbnb](http://insideairbnb.com/) provides two data sets relating to this:\n",
    "\n",
    "1. The uncompressed `listings.csv` file contains 'just' 16 columns and so is quite a bit smaller and easier to work with. \n",
    "2. The compressed `listings.csv.gz` file which contains 95 columns.\n",
    "\n",
    "_Note:_ notice that there is a 'show archived data' link containing older data for each city on their [Get the Data](http://insideairbnb.com/get-the-data.html) web page... think that could be useful for something???\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:gsa2017]",
   "language": "python",
   "name": "conda-env-gsa2017-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
