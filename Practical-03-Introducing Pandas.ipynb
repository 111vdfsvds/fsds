{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Data\n",
    "\n",
    "## The importance of exploration\n",
    "\n",
    "One of the first things that we do when working with any new data set is to familiarise ourselves with it. There are a _huge_ number of ways to do this, but there are no shortcuts to:\n",
    "* Reading about the data (how it was collected, what the sample size was, etc.)\n",
    "* Reviewing any accompanying metadata (data about the data, column specs, etc.)\n",
    "* Looking at the data itself at the row- and column-levels\n",
    "* Producing descriptive statistics \n",
    "* Visualising the data using plots \n",
    "In fact, you should use _all_ of these together to really understand where the data came from, how it was handled, and whether there are gaps or other problems. If you're wondering which comes first, I've always liked this approach: _start with a chart_. We're _not_ going to do that here because, first, I want you to get a handle on pandas itself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Revisiting Last Week's Data with Pandas\n",
    "\n",
    "Pandas stands for 'Python Data Analysis Library'; it is designed to provide data scientists working in Python with a set of powerful tools to load, transform, and process large-ish data sets. As a result, it has become something of a *de facto* standard for online tutorials and many of the lessons that you can find online will make use of pandas at some point.\n",
    "\n",
    "You will want to bookmark [the documentation](http://pandas.pydata.org/pandas-docs/stable/) since you will undoubtedly need to refer to it fairly regularly. _Note_: this link is to the most recent release. Over time there will be updates published and you _may_ find that you no longer have the most up-to-date version. If you find that you are now using an older version of pandas then you'll need to track down the _specific_ version of the documentation that you need from the [home page](http://pandas.pydata.org).\n",
    "\n",
    "You can always check what version you have installed like this:\n",
    "```python\n",
    "import pandas as pd\n",
    "print pd.__version__\n",
    "```\n",
    "*Note*: this approach isn't guaranteed to work with _every_ package, but it will work with most of them. Remember that variables and methods starting and ending with '`__`' are **private** and any interaction with them should be approached very, very carefully.\n",
    "\n",
    "Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "help(pd.DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On second thought, let's never do that again. Well, at least not _that_ way! You'll have noticed that the help documentation for the DataFrame is not just a bit longer than anything we've seen before, it's massively longer. There's probably quite a lot of intimidating terminology in there too... Right from the start we get things like \"Two-dimensional size-mutable, potentially heterogeneous tabular data structure with labeled axes (rows and columns).\" \n",
    "\n",
    "## You've already invented pandas!\n",
    "\n",
    "Here's the thing: in the [last notebook](https://raw.githubusercontent.com/kingsgeocomp/geocomputation/master/Practical-4-Functions%2C%20Packages%20and%20Methods.ipynb) we came close to writing something like pandas from scratch. That's because pandas takes a column-view of data in the same way that our Dictionary-of-Lists did, it's just that it's got a lot more features than our 'simple' tool does. That's why the documentation is so much more forbidding and why pandas is so much more powerful.\n",
    "\n",
    "But at its heart, a pandas data frame ('df' for short) is just a collection of data series (i.e. columns) with an index. Each Series is like one of our column-lists from the last notebook. And the df is like the dictionary-of-lists that held the data together. You've seen this before, so you already _know_ what's going on... or at least you now have an _analogy_ that you can use to make sense of pandas:\n",
    "```python\n",
    "myDataFrame = {\n",
    "    '<column name 1>': <Series 1>,\n",
    "    '<column name 2>': <Series 2>,\n",
    "    '<column name 3>': <Series 3>\n",
    "}\n",
    "```\n",
    "\n",
    "Let's try using pandas with last week's data!\n",
    "\n",
    "## Load a Remote CSV file in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('http://www.reades.com/CitiesWithWikipediaData.csv') # Read the remote CSV file\n",
    "print(type(df)) # What is 'df'?\n",
    "df.head() # What did we get?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check it out!\n",
    "\n",
    "Instead of having to write a 'readRemoteCSV' function and then manually create a Dictionary-of-Lists from that remote file, we just told pandas to read it for us and it automagically converted it to a data structure that we could view. You'll notice that it even figured out where the column names were. \n",
    "\n",
    "All we did with `df.head()` was to ask it to print out the first 5 rows of data. If we wanted to only see the first two rows it would be `df.head(2)`. This is pretty handy, right? \n",
    "\n",
    "Also, it deliberately mimics the Unix command-line tool `head` (i.e. `head -5 CitiesWithWikipediaData.csv`). So you've learned two tools for the price of one!\n",
    "\n",
    "## Describing Numerical Data in Pandas\n",
    "\n",
    "Let's try a few more things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll probably have seen a fairly prominent warning (\"Invalid value encountered in percentile\"), and if you look closely you'll see that there are some fields that report '`NaN`' in some of the rows. NaN is 'Not A Number', and if you were to look at the original data you'd see that we're missing the Area and Density for some of the cities in the data set -- pandas uses NaN because it wants us to know that there was _no_ data there. \n",
    "\n",
    "**_Why wouldn't we want to pandas to default to `0` for 'no data'?_**\n",
    "\n",
    "### Recap\n",
    "\n",
    "Let's take a step back for a second to appreciate where we're at: in one function call we loaded a remote data set, parsed it to turn it into data, and then produced a 7-figure summary for _most_ of the columns in the data! That's a bit faster than trying to do it all in Excel, right?\n",
    "\n",
    "So, just by calling `describe`...\n",
    "1. We've asked Python to describe the data frame and it has returned a set of columns with descriptive metrics for each.\n",
    "2. Note what is _missing_ from this list: where are 'Name', 'MetroArea', and a couple of the other columns? Can you think why they weren't reported in the descriptives?\n",
    "\n",
    "Of course, maybe you don't want the report for all columns, maybe you're just interested in one column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.Population.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have the same information, but only for the Population column. We have to do this a _little_ differently because describing the DataFrame does some clever formatting when you're using Jupyter notebooks, and describing a Series requires us to print out the result. Also notice that `dtype` at the end: that tells us the _data type_ is a 64-bit float. You can have strings, floats, integers, booleans, etc. in a DataFrame.\n",
    "\n",
    "But the really crucial thing is that this introduces _one_ of the two ways that we access a Series in pandas: `<data frame>.<series name>.method`. So we could get similar information on the Name column with:\n",
    "```python\n",
    "df.Name.describe()\n",
    "```\n",
    "And so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print df.Name.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that describing a text column gives us an 'object' data type because a String is a complex object, not a simple float or int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"The mean population of the cities in the data is: \" + str(df.Population.mean())\n",
    "print \"The median population of the cities in the data is: \" + str(df.Population.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Challenge for You!\n",
    "\n",
    "If all of this has made some kind of sense, why not spend a four of five minutes (at most) exploring the CSV data from last week using pandas. Try the following:\n",
    "\n",
    "1. What's the standard deviation of the population? _[Hint: look in the help for a Series, or Google it.]_\n",
    "2. What's the highest rank (i.e. smallest city) in the data set? _[Hint: you are looking for the maximum value in the id column.]_\n",
    "\n",
    "Use the coding block below for your exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And notice to that we can ask the data frame to quickly work out a derived variable (such as the mean) just by asking the Series to do the work for us: `<data frame>.<series>.method()`. You might want to have a [look at the documentation](http://pandas.pydata.org/pandas-docs/stable/api.html#series) to see what other methods are available for a data series. It's rather a long list, but most of your descriptive stats are here in [Cumulative / Descriptive Stats](http://pandas.pydata.org/pandas-docs/stable/api.html#computations-descriptive-stats) as are things about [strings](http://pandas.pydata.org/pandas-docs/stable/api.html#string-handling), [categorical data](http://pandas.pydata.org/pandas-docs/stable/api.html#categorical), and [plotting](http://pandas.pydata.org/pandas-docs/stable/api.html#plotting).\n",
    "\n",
    "## Quick and Dirty Plotting\n",
    "\n",
    "No, this isn't about this man: ![Niccolo Machiavelli](http://a5.files.biography.com/image/upload/c_fill,cs_srgb,dpr_1.0,g_face,h_300,q_80,w_300/MTE5NDg0MDU1MDQ5MzA3NjYz.jpg \"Niccolo Machiavelli\") (that's Niccolo Machiavelli) this is about making graphs of our data. This is going to be a long class, so I want you to recognise why it's worth getting the weather data below _into_ a pandas data frame before we go through the pain of working with the MetOffice API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You need to run this at _least_ once in each notebook to see your plots\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.Population.plot.hist() # Population histogram. What's the outlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.Population.plot.box() # Population histogram. More than one outlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.Latitude.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.Latitude.plot.density()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kind of handy, no? These aren't the _best_ looking plots, but they are all being generated on-the-fly for you by pandas with no more than a cheery `<data frame>.<data series>.plot.<plot type>`! Since those plots are all just method calls, many of them take optional parameters to change the colour, the notation (scientific or not), and other options. \n",
    "\n",
    "This is why we like pandas: it allows us to be _constructively lazy_. We don't need to know _how_ a draw a KDE plot (though it always helps if you don't see what you expected), we just need to know that pandas provides a method that will do it for you. And _that_ is why it's always worth having a [look at the documentation](http://pandas.pydata.org/pandas-docs/stable/api.html#plotting).\n",
    "\n",
    "### Looking Ahead\n",
    "\n",
    "This is a very basic introduction to pandas just so that you're familiar with the bare bones of examining a pandas data frame to work out what it contains, how to calculate a few descriptive variables, and how to plot. Now it gets real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas with _Real_ Data\n",
    "\n",
    "That's the last of the 'toy' data, for the remainder of this module we're going to be working with two types of data: data about people (Socio-economic Classifcation) and data about the environment (weather). \n",
    "\n",
    "We've selected these two very different types of data on purpose:\n",
    "\n",
    "1. Because we know that some of you have interests in the human environment, and others in the natural\n",
    "2. Because these are very different types of data with very different properties\n",
    "3. Because we'll see that _similar_ workflows can be used with each!\n",
    "\n",
    "What we want to highlight is that computational approaches are _highly transferrable_ between contexts. The mean or median is not _less_ relevant in one context than another, it's just more or less appropriate as a tool for understanding the data! \n",
    "\n",
    "We'll see the Socio-economic Classification data next week and focus on the weather API data this week. This week is harder conceptually because API data is harder to understand -- we've simplified it quite a bit (that's why there is an Appendix) but it's still got some parts that are going to be hard going.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Data in Pandas\n",
    "\n",
    "We've got ourselves a pandas data frame containing a set of locations, how do we go about finding one or more _specific_ rows in the data set rather than just summarising the data via `describe`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for a Number\n",
    "\n",
    "This is the easiest type of search to do in pandas because it looks _most_ like code you've already seen.\n",
    "\n",
    "### Find All Sites East of 1.1 Degrees Longitude\n",
    "\n",
    "To translate this into code, we just need to remember that: a) East would be _greater than_; and b) longitude is already a float. So in that case it's..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfEast = df2[df2.longitude > 1.1]\n",
    "dfEast.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break this down:\n",
    "\n",
    "* `df2.longitude` is obviously the longitude column of our data frame `df2`\n",
    "* `df2.longitude > 1.1` is therefore a kind of _query_ (or _selection_) of rows where the longitude is greater than 1.1. What it _actually_ does is compare each row's longitude value to 1.1 and remember if the result is `True` or `False`.\n",
    "* `df2[ ... ]` is _like_ what we do with a list when we write: `myList[3:5]` to select the fourth through sixth elements of a list, but in pandas we can _select_ non-sequential rows because we are using a `boolean` array (a.k.a. list) that looks like this: `[False, False, False, True, True, True, ...]`.\n",
    "* `dfEast = ...` saves the _result_ of the selection into a new data frame called `dfEast` (data frame East).\n",
    "\n",
    "You can check what I'm saying about the boolean result using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2.longitude > 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can check that `dfEast` and `df2` are not the same using `shape`, which gives us the dimensions of the data frame as `(<rows>, <columns>)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(df2.shape)\n",
    "print(dfEast.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this first example means is that _anything_ that can be evaluated to `True` or `False` can be used to select rows from a data frame... Let's try some more selections based on numbers..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Minimum & Maximum Elevation\n",
    "\n",
    "The lowest point is in the Fens, and the highest point is, of course, Ben Nevis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2[df2.elevation == df2.elevation.min()] # Somewhere in Cambridgeshire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2[df2.elevation == df2.elevation.???] # Find Ben Nevis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding a Range Between Known Values\n",
    "\n",
    "Perhaps we aren't just looking for extremes... how about all of the areas between 55 and 55.2 degrees latitude? *[There were 91 the last time I checked.]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfRange = df2.loc[ (df2.latitude > 55.0) & (df2.latitude < 55.2) ]\n",
    "dfRange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That example contains a few new things to which you need to pay attention:\n",
    "1. You'll see that, with mutiple selections, we had to put parentheses around each one -- this is to avoid confusing pandas as to what it should do _first_.\n",
    "2. We see an '&' (ampersand) which is completely new: it's a logical `AND` that asks pandas to \"Find all the rows where condition 1 _and_ condition 2 are both `True`\". So it calculates the `True`/`False` for the left side and the `True`/`False` for the right side of the `&`, and then combines them. Look at the appendix to this notebook for more examples and options.\n",
    "3. We had to a `.loc` on the end of the `df2` -- the best way to think of this is that it 'freezes' things so as to prepare the data frame to do a search based on the _location_ of some complex selection criteria. We'll see more of this next week.\n",
    "\n",
    "### Finding a Range Based on the Distribution\n",
    "\n",
    "Finally, let's try finding the stations whose elevation is _greater_ than the mean. *[There are 1,330 the last time I checked.]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfMean = df2.loc[ df2.elevation > df2.???.mean() ]\n",
    "print(\"There are \" + str(dfMean.shape[0]) + \" sites above the mean elevation of \" + str(df2.elevation.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching on Text & Categories\n",
    "\n",
    "Numeric searching is all well and good, but what if I'm interested in finding stations in a particular area?\n",
    "\n",
    "### Searching for a Category\n",
    "\n",
    "Let's find the names of every station inside the Cairngorms National Park! *[There were 71 the last time I checked.]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2[ df2.??? == ??? ].name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for Part of a String\n",
    "\n",
    "If you want to find a full match for a string then it's fairly easy and works like everything you've seen before with string matching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2[df2.name=='Cairn Gorm Summit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And pandas also provides a lot of useful tools for searching _inside_ a string, as long as you remember to _tell_ pandas to use the string-methods (notice the format: `<data frame>.<data series>.str.<string method>()`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2[df2.name.str.startswith('Beinn A\\'')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2[df2.name.str.endswith('Summit')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching _inside_ a string is no harder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2[df2.name.str.contains('Charn')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you ensure that you found _only_ the Geal Charn stations? \n",
    "\n",
    "Combine what you've learned above to create a complex query (two conditions on a single line) using a mix of search critera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2.loc[ (???) & (???'Highland') ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I want you to find and print out **_only_ the ID of Heathrow the town _not_ the Airport** using a single line of code. There are _at least_ two ways to retrieve this...\n",
    "\n",
    "We are going to want that ID for the next step in working with the MetOffice API, but there is a last trick to learn here and that's how to extract an actual value as a string, int, or float from a data series. The thing to remember is that a Series is basically a list with a lot of value-added features. The contents of the list can be found in `<data series>.values`. So to get the 2nd through 5th values of the elevation column it would be:\n",
    "```python\n",
    "df2.elevation.values[1:5]\n",
    "```\n",
    "\n",
    "If you do the selection criteria for Heathrow Airport properly there should only be one item in the list of values that you retrieve, so the right code will include a `[0]` at the end:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's yet another bunch of 'data' that's difficult for us to read, but by now this should be looking rather familiar to you... perhaps? Hang on a moment! It's a dictionary-of-lists-of-dictionaries-of..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Creating a DataFrame from a Dictionary\n",
    "\n",
    "And that, of course is exactly the type of data structure that we can work with in pandas! \n",
    "\n",
    "So the _last_ step here is to figure out how to create a new data frame from this dictionary. Here, the MetOffice has _not_ made our lives very easy because the data is packaged in a way that doesn't allow us to easily load it into pandas. If you search online, you'll find plenty of people complaining about how the MetOffice API works. Or doesn't work, if you prefer.\n",
    "\n",
    "So we're not going to ask you to sort this out for yourselves. Instead, we're going to provide you with a function (!) to take the observation data and convert it into a data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidying Up\n",
    "\n",
    "Before we can get back to plotting (again) we have a few more steps to work through:\n",
    "\n",
    "1. To rename the columns to something a little more useful.\n",
    "2. To turn the 'ts' field into an _actual_ timeseries so that pandas understands what it is.\n",
    "3. To convert all of the other series to the right numerical/categorical format.\n",
    "\n",
    "Let's do this in several stages... \n",
    "\n",
    "### Changing Column Names\n",
    "\n",
    "You may remember that I indicated what the observations returned by each weather station might include:\n",
    "\n",
    "* D  = Wind Direction\n",
    "* Dp = Dew Point\n",
    "* G  = Wind Gust\n",
    "* H  = Humidity\n",
    "* Pt = Pressure Tendency\n",
    "* S  = Wind Speed\n",
    "* T  = Temperature\n",
    "* V  = Visibility\n",
    "* W  = Weather Type\n",
    "* ts = Time of Day\n",
    "\n",
    "Given this, and the fact that I've listed these in order, what needs to replace the '???' in the code below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3.??? = ['WindDirection','DewPoint','WindGust','Humidity','PressureTendency','WindSpeed','Temperature','Visibility','WeatherType','ts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the 'full' column names now.\n",
    "\n",
    "### Changing column types\n",
    "\n",
    "If you were exploring the data frame along the way, you might have already noticed that the description of numeric columns (like Temperature) doesn't seem much like what we had before -- shouldn't we get the 7-figure summary for numeric columns? The problem is that pandas didn't know what we expected the columns to be, so it's treated them all as 'objects' (basically: strings) and not as numeric data types.\n",
    "\n",
    "So we need to fix that now... as you saw before, there's a function called `'astype'` that allows us to convert between data types where it's fairly easy for pandas to figure out what we want to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3.Temperature.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in ['WindDirection','WeatherType','PressureTendency']:\n",
    "    df3[c] = df3[c].astype('category')\n",
    "for c in ['DewPoint','Humidity','Temperature']:\n",
    "    df3[c] = df3[c].astype('float')\n",
    "for c in ['WindGust','Visibility']:\n",
    "    df3[c] = df3[c].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3.Temperature.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's more like it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Timeseries Data\n",
    "\n",
    "So that's looking a lot more useful, but as a final step we need to make sure that the temporal data is actually treated as a time series... again, Google is your friend here: `\"pandas convert datetime to time series\"`. \n",
    "\n",
    "Given that we are creating a _new_ column called 'Time' from an _existing_ column called 'ts', what do you think needs to replace the '???'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3['Time'] = pd.to_datetime(df3.???.values, infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3.ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3.Time.head() # A quick check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Time Series in an Index\n",
    "\n",
    "We can tell that that type conversion succeeded because we've got a new `dtype`: `datetime64[ns]`. \n",
    "\n",
    "We can also now do some really neat things to 'resample' the data based on the fact that we have temporal data; however, to take advantage of this we have to let pandas know that the entire data set is organised by time. We do this by replacing the existing integer index with a datetime one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3.index = pd.to_datetime(df3.ts.values, infer_datetime_format=True)\n",
    "df3.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output above, you'll notice that the left-most column (the one without a name, because it's an _index_, not a column) is now a datetime object. Why is that useful? Well check _this_ out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3.Temperature.resample('D').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, this is another geeky moment but how cool is that? By telling pandas that our data is temporal, we're now in a position to ask pandas to answer questions like \"What was the average daily ('D') temperature at Heathrow?\" And we can do this in _one_ line of code.\n",
    "\n",
    "If you had data at minutes-level resolution, then you could aggregate to Hourly or Daily. In principle, you can also do all sorts of datetime queries around things like \"What was the weekly average in the 3rd week of 2016?\" or \"What was last Friday's weather?\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting!\n",
    "\n",
    "This has been a long, slow build towards something more exciting: plotting! Well, plotting _again_. In a way, this has been a lot of effort just to make a graph, but let's recognise where we're at:\n",
    "\n",
    "* We can request data for _any_ lcoation in Britain by changing the location id.\n",
    "* We can get new data _any_ time we feel like it.\n",
    "* We can (in a minute) create a plot of that data.\n",
    "* We can update it continuously in the future!\n",
    "\n",
    "That's pretty awesome, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This command tells Jupyter that we want \n",
    "# the plots to be shown inline (on this \n",
    "# web page). You'll always need to do this\n",
    "# *once* on a notebook.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas can do a _lot_ of different plots, [see for yourself](http://pandas.pydata.org/pandas-docs/stable/visualization.html#visualization-hexbin). Here's a sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3.Humidity.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3.Temperature.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3.Temperature.plot.box() # Handy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3.plot.scatter(x='Temperature', y='Humidity') # Spot the problem data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3.plot.scatter(x='Temperature', y='Humidity', s=(df3.DewPoint+1)*25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3.plot.hexbin(x='Temperature', y='Humidity', gridsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3.WindDirection.plot() # Ooops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tend to think that that's quite enough to be coping with for one session... over to the script!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "The material below here is very helpful if you _really_ want to get to grips with both some powerful computing concepts (hello recursion!) and to understand how I pulled together the data frame through working with the data iteratively to get to grips with the MetOffice reply. However, it is not _necessary_ that you undertand these ideas now since they are relatively advanced and will be more directly useful in the _Spatial Analysis_ and _Applied Geocomputation_ modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logical Comparisons\n",
    "\n",
    "You have already seen a bit of Boolean logic using 'and', 'or', and 'not'. For reasons that aren't really worth getting into here, when you're dealing with the simple binary True/False data (a.k.a. [bit-wise](https://wiki.python.org/moin/BitwiseOperators)) comparisons then the same operations are written using a slightly different syntax:\n",
    "\n",
    "1. 'and' becomes '&'\n",
    "2. 'or' becomes '|'\n",
    "3. 'not' becomes '~'\n",
    "\n",
    "These are rather topical for complex queries in pandas."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:gds_test]",
   "language": "python",
   "name": "conda-env-gds_test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
