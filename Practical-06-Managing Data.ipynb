{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>7SSG2059 Geocomputation 2018/19</h1></center>\n",
    "\n",
    "<h1><center>Practical 7: Data Manipulation</h1></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Manipulating Data & `DataFrames`\n",
    "\n",
    "As we've discussed in lectures, manipulating data can be a major component of data analysis. This week will look at some further ways to manipulate data that might be useful for you when analysing the data for your final report. \n",
    "\n",
    "Specifically we will:\n",
    "1. recap on some useful sorting and selecting methods\n",
    "2. see how we can combine two `DataFrames` together using common properties for further analysis\n",
    "3. look at how we can group data for analysis (e.g. grouping LSOAs for borough-level analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "As usual we will be using pandas and doing some data analysis plotting, so we need to import the relevant packages (note the aliases used for reference below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start working with the initial LSOA data that we have been using previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df = pd.read_csv(\n",
    "    'https://github.com/kingsgeocomp/geocomputation/blob/master/data/LSOA%20Data.csv.gz?raw=true',\n",
    "    compression='gzip', low_memory=False) # The 'low memory' option means pandas doesn't guess data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later we'll look at how we can add more data to this `DataFrame` later, but first let's just check what columns of data we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we have our data loaded and we've reminded ourselves of what the data set contains (maybe by consulting the [metadata](https://github.com/kingsgeocomp/geocomputation/raw/master/Data/LSOA_metadata.xlsx)) we can move on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap: Sorting and Selecting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Rows in the Data\n",
    "\n",
    "You should remember from Week 3 that we can find out _where_ (i.e. which LSOA) the maximum value in the data occurs using code like this:\n",
    "```python\n",
    "my_df[my_df.POPDEN == my_df.POPDEN.max()]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task:\n",
    "Write some code to list all of the LSOAs where the population density is more than two standard deviations greater than the mean population density of all London LSOAs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, your code returns you 198 LSOAs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, now what about if we want to find the top 10 LSOAs in terms of population density, and examine how many households there are in those LSOAs? Recall that we did this last week in one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_df = my_df.sort_values(by='POPDEN', ascending=False).head(10)[['LSOA11NM','POPDEN','HHOLDS']]\n",
    "\n",
    "print(sort_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at that line of code and check you can see how the different lines previously have been combined. \n",
    "\n",
    "Let's pull it apart step-by-step at the code level:\n",
    "\n",
    "* The first step in this process is `my_df.sort_values` -- you can probably guess what this does: it sorts the data frame!\n",
    "* The parameters passed to the `sort_values` function are `by`, which is the column on which to sort, and `ascending=False`, which gives us the data frame sorted in _descending_ order!\n",
    "* The output of `my_df.sort(...)` is a _new_ data frame, which means that we can simply add `.head(10)` to get the first ten rows of the newly-sorted data frame.\n",
    "* And the output of `my_df.sort(...).head(...)` is yet _another_ data frame, which means that we can print out the values of selected columns using the 'dictionary-like' syntax: we use the outer set of square brackets (`[...]`) to tell pandas that we want to access a subset of the top-10 data frame, and we use the inner set of square brackets (`['LSOA11NM','POPDEN','HHOLDS']`) to tell pandas which columns we want to see.\n",
    "\n",
    "I'd say 'simples, right?' but that's obviously _not_ simple. It _is_, however, very, very _elegant_ because it's quite clear (once you get past the way that lots of methods can be chained together) and it's very succinct (we did all of that in _one_ line of code!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task:\n",
    "In a single line of code create a new `df` containing information about the name, population density and number of usual residents for the seven least populated LSOAs (in terms of usual residents). Then use another line of code to print the new `df`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a Random Sample of Data\n",
    "\n",
    "Of course, sometimes you don't want a particular range of data, you want a _random sample_ so that you can either \n",
    "a. get a better sense of the data, or \n",
    "b. perform some kind of test with a subsample before replicating on the full data set. \n",
    "\n",
    "Pandas has [got you covered](http://pandas.pydata.org/pandas-docs/version/0.18.1/indexing.html#selecting-random-samples) with a huge range of options, including sampling with replacement, sample weights, row numbers and a fraction of the data set. \n",
    "\n",
    "Let's look at some simple examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.sample(n=5)[ ['LSOA11NM','POPDEN','USUALRES'] ] # Sample of size 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.sample(n=5)[ ['LSOA11NM','POPDEN','USUALRES'] ] # This will not give you the same sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that even though the two lines of code above are identical we return a different (random) sample of rows. This is useful but what if we want to give our code to someone else to that they would get the same (random) sample of rows?  To do this we can specify the `random_state` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.sample(n=5, random_state=2)[ ['LSOA11NM','POPDEN','HHOLDS'] ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By specifying the same value for `random_state` we will get the same sample: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.sample(n=5, random_state=2)[ ['LSOA11NM','POPDEN','HHOLDS'] ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And using a different value for `random_state` gives us a different sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.sample(n=5, random_state=3)[ ['LSOA11NM','POPDEN','HHOLDS'] ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify the fraction of the `DataFrame` we want to sample, rather than an absolute number of observations (think about why this is useful for when we don't know what size `DataFrame` our code might be used with):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.sample(frac=0.002)[ ['LSOA11NM','POPDEN','USUALRES'] ] # Sample a fraction of the rows (here 0.2%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the code above has automatically been sampling rows of data, but we can also sample columns by specifying the `axis` of the `DataFrame` we want to sample: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.sample(n=5, axis=1).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Data\n",
    "\n",
    "Up until this point we have been working with a dataset of ~48 variables (columns) for the LSOAs. But what if we have additional data for LSOAs that we want to work with together with our original data, for example to look for correlations between variables. Here we will look at how to combine two datasets that have data for individual LSOAs:\n",
    "1. our original data\n",
    "2. data for air quality in each LSOA\n",
    "\n",
    "Combining these data would be useful, for example, to examine relationships between air quality and socio-economic and other variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical we will look at how to `merge` Pandas dataframes. There's also `join` and `concatenate` function. Each of these functions are slightly different:\n",
    "- `merge` enables us to [combine](http://pandas.pydata.org/pandas-docs/stable/merging.html#database-style-dataframe-joining-merging) two dataframes based on a column that is common between them\n",
    "- `join` is used to [combine](http://pandas.pydata.org/pandas-docs/stable/merging.html#joining-on-index) two dataframes when they share a common `index` (e.g. a `DateTimeIndex` in timeseries data)\n",
    "- `concatenate` [combines](http://pandas.pydata.org/pandas-docs/stable/merging.html#concatenating-objects) dataframes regardless of common attributes. \n",
    "\n",
    "We'll then look at how to analyse variables in the combined `DataFrame` we produce using correlation later in the module. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Air Quality Data\n",
    "\n",
    "Metadata about the air quality data are included in the [metadata](https://github.com/kingsgeocomp/geocomputation/blob/master/data/LSOA_metadata.xlsx?raw=true) file. The data themselves are hosted online and can be read using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_df = pd.read_csv(\n",
    "    'https://github.com/kingsgeocomp/geocomputation/blob/master/data/LSOA_AirQuality.csv.gz?raw=true',\n",
    "    compression='gzip', low_memory=False) # The 'low memory' option means pandas doesn't guess data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Familiarise yourself with the data you have just loaded in and compare it to the data we have worked with previously. To do this you might:\n",
    "1. check the column names and data types of the air quality data file and compare to the metadate file (hint: use `info()` method for `DataFrame`s - you might need to google this)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. calculate descriptive statistics for the air quality data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. compare the column names of the air quality dataset with the original LSOA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. compare the shapes of the two `DataFrames`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From your exploration of the new data and comparison with the original LSOA data you might notice a few things:\n",
    "1. They have the same number of rows\n",
    "2. They have different numbers of columns\n",
    "3. They share one column name in common (`LSOA11CD`)\n",
    "\n",
    "Check you can see these observations for yourself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge\n",
    "\n",
    "If we have a column in each of two `DataFrames` that contains the same identifier (column) for the other variables (columns) in the data, we can use the common identifier to define how the two `DataFrames` are joined together. For example, the data we are working with are for LSOAs (distinct geographical regions) - if any additional data we have is also for LSOAs, as long as we we have a common way of identifying the LSOAs in both `DataFrames` we can `merge` the `Data Frames`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully from the task above that we have a common identifier in both the orginal data `my_df` and the new air qualiy data `aq_df`: `LSOA11CD`. The `LSOA11CD` is a unique identifier code for each LSOA. We can use this to match rows of data in `my_df` (each of which is for a particular LSOA) with the corresponding rows in `aq_df` (which are also for individual LSOAs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the common identifier identified, we now need to decide what type of join we want to do. Recall from this week's lecture that there are four main types of 'join':\n",
    "1. left\n",
    "2. right\n",
    "3. outer\n",
    "4. inner\n",
    "\n",
    "We could use any of the above depending on our objectives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll do a **left join**, where the left `df` will be the original data and the right `df` will be our new air quality data. This seems appropriate so that we don't modify the original data too much (thereby potentially messing up some of our previous code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the two data frames \n",
    "merge_df = pd.merge(my_df, aq_df, how = 'left', on = 'LSOA11CD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let's check what the columns are in the new `DataFrame` we just created: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merge_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's check from a sample of the data how the rows look: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.sample(n=5, random_state=3)[ ['LSOA11CD','POPDEN','HHOLDS', 'PM25mean', 'PM25min', 'PM25max', 'PM25sd'] ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully it looks good so far. Let's review what we did the with `pd.merge` function above:\n",
    "```python\n",
    "pd.merge(my_df, aq_df, how = 'left', on = 'LSOA11CD')\n",
    "```\n",
    "\n",
    "1. `my_df` is the left `df`\n",
    "2. `aq_df` is the right `df` \n",
    "3. `how` defines what type of join is to be done\n",
    "4. `on` is the column we want to use as the common identifier to 'join on' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So above, each value in the `LSOA11CD` column in `aq_df` is matched with the same value in the `LSOA11CD` column in `my_df` and the rows those values are found in are combined. The figure below illustrates the process (and look back to the lecture slides and see this [nice tutoral](https://www.shanelynn.ie/merge-join-dataframes-python-pandas-index-1/)). \n",
    "\n",
    "![Illustration of the Pandas merge function](http://pandas.pydata.org/pandas-docs/stable/_images/merging_merge_on_key_left.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check you understand how something similar to the image above has been done for our LSOA data. Remember you can read [the documentation](http://pandas.pydata.org/pandas-docs/stable/merging.html#database-style-dataframe-joining-merging) for more detailed explanation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we used the pandas `merge` function here, we are doing what we called a _join_ in the lecture; the only difference between pandas `merge` and `join` is that the former uses a common column whereas the latter uses a common index. The 'merge' column can do all four of the joins we have considered (left, right, outer and inner).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if there were any missing values introduced into our new `DataFrame` we can do [a quick check](https://stackoverflow.com/a/29530601):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you received a `False` response! If so, this is more evidence the join worked (if not you might want to check what you did above and ask for help). \n",
    "\n",
    "Let's save these data for later - they may be useful for your final report!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.to_csv(\"LondonLSOAData.csv\", index=False)\n",
    "#or\n",
    "#merge_df.to_csv(\"LondonLSOAData.csv.gz\", compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(If you really want to check what the join has done, you might open the file you just saved in Excel to have a look)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises:\n",
    "\n",
    "Explore the air quality data to get an understanding of what they might show you in relation to other variables in the data set. For example:\n",
    "\n",
    "1. Find the population densities of the LSOAs with highest maximum values for each of the four pollutants.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a single boxplot to compare the distributions of the mean values of each of the four pollutants  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create four scatter plots within a single figure (use a loop) to visualise the relationship between area within 250m of a major road and the minimum values of each pollutant _[If you find it difficult you may want to skip this exercise in practical to ensure you can work through the Grouping section while help is available]_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping Data\n",
    "\n",
    "Often in geographical data, we have data specified for different aerial units; counties, State parks, constituencies, etc. And frequently, these are units are hierarchical; counties are sub-units within states (e.g. Yorkshire is within England), postcode units within postcode areas (e.g. WC2R 2LS is within WC2R). This is particularly true for census data and so for the data we are working with; Lower Super Output Areas (LSOAs) are within London Boroughs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to investigate differences or similarities at the borough-level, we are going to have to group the data for individual LSOAs into their respective boroughs. Then we can summarise the boroughs as a whole. \n",
    "\n",
    "As you can see from the [metadata](https://github.com/kingsgeocomp/geocomputation/blob/master/data/LSOA_metadata.xlsx?raw=true), the _LAD11CD_ and _LAD11NM_ columns contains Local Authority District (i.e. Borough) IDs and names. These columns are useful as they specify for every LSOA (which are on different rows) and ID and the name of the borough each LSOA lies within. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could check the contents of these columns using the `unique()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merge_df.LAD11CD.unique())\n",
    "print(merge_df.LAD11NM.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways we could use these columns to analyse our data at the borough level:\n",
    "1. create new `Dataframes` for each individual borough\n",
    "2. tell pandas to group the data using the values in the borough column \n",
    "\n",
    "The first approach might be useful if we want to examine one or few particular boroughs in detail. We could create these new `DataFrames` using the selection methods we have seen previously (but also [others you could learn about](http://pandas.pydata.org/pandas-docs/stable/indexing.html)). However, if we want to work with data for all London boroughs, this method would not be particularly easy to work with. \n",
    "\n",
    "So for the second approach, the pandas library has another data structure known as `DataFrameGroupBy` which is useful in this situation (see [documentation here](http://pandas.pydata.org/pandas-docs/stable/groupby.html)). We'll examine this approach in more detail now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataFrameGroupBy`\n",
    "\n",
    "The `DataFrameGroupBy` data structures is created using the `groupby` method. To do this, grouping by borough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boroughs = merge_df.groupby('LAD11NM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boroughs `DataFrameGroupBy` object is a special type of `DataFrame` that has additional methods available based on what groups we have specified (in this case the borough ID). For example, when we use the `head()` method it looks similar to a normal `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boroughs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But when we try to get the shape of the object we find it's slightly different from a normal `DataFrame` (you should get an error): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boroughs.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can’t use shape to find out how many elements in the boroughs `DataFrameGroupBy` object, but we can use our old favourite function `len()` (which works pretty much anywhere!). Compare the output for the next two lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(boroughs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merge_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of `boroughs` is the number of groups in the `DataFrameGroupBy` object, whereas the length of `merge_df` is the number of rows in the `DataFrame` object. Check you understand the difference! We can tell from this that there are 33 boroughs (groups) and 4835 LSOAs (rows).\n",
    "\n",
    "The difference between `merge_df` and `boroughs` also results in different output for other methods. Compare the output of the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boroughs[\"LAD11NM\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df[\"LAD11NM\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the `count` method for the `DataFrameGroupBy` object gives the count of LSOAs within in each borough (group) whereas the count method for the `DataFrame` object simply gives the count of the total number of LSOAs (rows). \n",
    "\n",
    "Note, the following two lines of input code would do exactly the same as the last two but using dot notation\n",
    "```python\n",
    "boroughs.LAD11NM.count()\n",
    "merge_df.LAD11NM.count()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other methods we can use on `DataFrameGroupBy`, for example `get_group()` gets the data (for LSOAs) for just one of the groups (boroughs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boroughs.get_group(\"City of London\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that to access the data for this group, we pass a value from the column we used to define the groups using `groupby()` previously. As we used _LAD11NM_ to specify our groups above, here we were able to type the name we wanted (_\"City of London\"_). But if we had used _LAD11CD_ to specify the groups, we would have had to pass _E09000001_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `DataFrameGroupBy` object also allows us to describe the data by group (rather than for all of the LSOAs as we did before). For example, to find the mean values for the columns by borough we can use the `aggregate()` method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bMeans = boroughs.aggregate(np.mean)\n",
    "print(bMeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the `aggregate()` method makes a call to the `numpy` function `mean`; this is why we needed to `import numpy as np` in the setup section at the start of the notebook. \n",
    "\n",
    "The `aggregate` method returns a `DataFrame`. Check this by:\n",
    "1. printing the type of object of `bMeans` \n",
    "2. printing the `POPDEN` and `HHOLDS` columns of the new `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(bMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bMeans[['POPDEN','HHOLDRES']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check you understand what has been produced here; `bMeans` contains the mean (average) of all columns in our original dataset but aggregated (grouped) by borough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asking questions about boroughs\n",
    "\n",
    "Let’s see how this all might be useful for answering a geographical question. Say we want to calculate what proportion of the population of the Borough of Harrow that identifies as 'White' ethnicity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boroughs = merge_df.groupby('LAD11NM')  #as above  \n",
    "bSums = boroughs.aggregate(np.sum)      #sum of columns grouped by borough\n",
    "\n",
    "harrow_sumW = bSums.White.loc[\"Harrow\"]          #note: equivalent using dot notation is harrow_sumW = bSums.White.Harrow   \n",
    "harrow_sumRes = bSums.USUALRES.loc[\"Harrow\"]     #note: equivalent using dot notation is harrow_sumRes = bSums.USUALRES.Harrow\n",
    "\n",
    "harrow_propW =  float(harrow_sumW) / float(harrow_sumRes)                 #convert to float when calculating proportion \n",
    "print(\"The proportion of Harrow that is White ethnicity is: {0:.3f}\".format(harrow_propW))   #print nicely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code above and check that you find that the proportion is 0.422. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating over `DataFrameGroupBy`\n",
    "\n",
    "Finally, a short note to highlight that [iterating over groups](http://pandas.pydata.org/pandas-docs/stable/groupby.html#iterating-through-groups) in a `DataFRameGroupBy` object is much the same as looping over many other objects in python. For example, to iterate over all boroughs (groups) printing out the total population of each: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boroughs = merge_df.groupby('LAD11NM')\n",
    "\n",
    "for key, value in boroughs:\n",
    "    popn = value.USUALRES.sum()\n",
    "    print(\"{0:8.0f} people in {1}\".format(popn,key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a think about what the code above does:\n",
    "1. iterating over the groups returns a tuple composed of `key` and `value`\n",
    "2.`value` allows us to get to the actual data in each group (borough)\n",
    "3. we can use `key` to get the label of each group (in this case the values of `LAD11NM` used to create the `GroupBy` object)\n",
    "\n",
    "Note, the above is just an example to show the structure of how to loop over `GroupBy` objects and we could have done much the same (although without the nice string formatting) using:\n",
    "\n",
    "```python\n",
    "print(boroughs.USUALRES.sum())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Similar to the exercises above for pollutants, but this time for borough-level data:\n",
    "\n",
    "1. Find the total number of usual residents of the boroughs with highest maximum values for each of the four pollutants.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a barplot to compare the borough-level means of mean values of each of the four pollutants  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create four scatter plots within a single figure (use a loop) to visualise the relationship between total area within 250m of a major road within a borough and the minimum values of each pollutant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary \n",
    "\n",
    "In this practical we have:\n",
    "1. had a recap of some sorting and selecting\n",
    "2. seen how to take a random sample of data\n",
    "3. introduced ourselves to the air quality data and combined it with our other data\n",
    "4. learned about the `DataFrameGroupBy` data structure. \n",
    "\n",
    "You now have the 'full' data set (combining the original data with the air quality data) that you can use for your final report. So start exploring!\n",
    "\n",
    "If you want to join your own LSOA data for analysis in your final report, please discuss with James before doing so. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits!\n",
    "\n",
    "#### Contributors:\n",
    "The following individuals have contributed to these teaching materials: Jon Reades (jonathan.reades@kcl.ac.uk), James Millington (james.millington@kcl.ac.uk)\n",
    "\n",
    "#### License\n",
    "These teaching materials are licensed under a mix of [The MIT License](https://opensource.org/licenses/mit-license.php) and the [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 license](https://creativecommons.org/licenses/by-nc-sa/4.0/).\n",
    "\n",
    "#### Acknowledgements:\n",
    "Supported by the [Royal Geographical Society](https://www.rgs.org/HomePage.htm) (with the Institute of British Geographers) with a Ray Y Gildea Jr Award.\n",
    "\n",
    "#### Potential Dependencies:\n",
    "This notebook may depend on the following libraries: pandas, matplotlib, seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "GSA2019",
   "language": "python",
   "name": "gsa2019"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
