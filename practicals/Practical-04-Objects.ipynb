{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left\">\n",
    "    <h1 style=\"width:450px\">Practical 4: Object-Oriented Programming</h1>\n",
    "    <h2 style=\"width:450px\">Getting to grips with Functions &amp; Packages</h2>\n",
    "</div>\n",
    "<div style=\"float:right\"><img width=\"100\" src=\"https://github.com/jreades/i2p/raw/master/img/casa_logo.jpg\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (Revisited). Why 'Obvious' is Not Always 'Right'\n",
    "\n",
    "Practical 3 is hard, so I want to provide _another_ chance for the concepts to bed in before we use them in an *object-oriented way through Pandas*. Yes, Week 5 will show how we combine concepts covered over the preceding two weeks in *practice* to do data science. \n",
    "\n",
    "So remember the finding from last week: if we don't really care about column order, then a dictionary of lists would be a nice way to handle data. And why should we care about column order? With our CSV file we saw what a pain it was to fix things when even a tiny thing like the layout of the columns changed. But if, instead, we could just reference the 'Description' column in the data set then it doesn't matter where that column actually is *and* we would know that all the descriptions would be *text*, while all the populations or prices would be *numbers*. Why is that? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1 The Way That Doesn't Work\n",
    "\n",
    "Here are four rows of 'data' for city sizes organised by _row_ as a list-of-lists. Try printing out *just* the cities contained in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myData = [\n",
    "    ['id', 'Name', 'Rank', 'Longitude', 'Latitude', 'Population'], \n",
    "    ['1', 'Greater London', '1', '-18162.92767', '6711153.709', '9787426'], \n",
    "    ['2', 'Greater Manchester', '2', '-251761.802', '7073067.458', '2553379'], \n",
    "    ['3', 'West Midlands', '3', '-210635.2396', '6878950.083', '2440986']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Print a List of Cities\n",
    "\n",
    "Print out a list of every city in the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col    = myData[0].index('Name')\n",
    "cities = []\n",
    "\n",
    "for i in range(1, len(myData)):\n",
    "    cities.append(??)\n",
    "    \n",
    "print(\"The cities in the data set are: \" + \", \".join(cities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Is Edinburgh in the List?\n",
    "\n",
    "Now write code to find out if `Edinburgh` is included in the list of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found = False\n",
    "for i in range(1, len(??)):\n",
    "    if myData[i][col] == ??:\n",
    "        print(\"Found Edinburgh in the data set!\")\n",
    "        found = True\n",
    "\n",
    "if found == ??:\n",
    "    print(\"Didn't find Edinburgh in the data set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 The Way That Does Work\n",
    "\n",
    "Compare that code to how it works for a dictionary-of-lists organised by _column_. Now try printing out the cities in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myData = {\n",
    "    'id'         : [0, 1, 2, 3, 4, 5],\n",
    "    'Name'       : ['Greater London', 'Greater Manchester', 'Birmingham','Edinburgh','Inverness','Lerwick'],\n",
    "    'Rank'       : [1, 2, 3, 4, 5, 6],\n",
    "    'Longitude'  : [-0.128, -2.245, -1.903, -3.189, -4.223, -1.145],\n",
    "    'Latitude'   : [51.507, 53.479, 52.480, 55.953, 57.478, 60.155],\n",
    "    'Population' : [9787426, 2705000, 1141816, 901455, 70000, 6958],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Print a List of Cities\n",
    "\n",
    "Print out a list of every city in the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What cities are in the data set?\n",
    "print(\", \".join(??))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Is Edinburgh in the List?\n",
    "\n",
    "Now write code to find out if `Edinburgh` is included in the list of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Edinburgh' in ??:\n",
    "    print(\"Found Edinburgh in the data set!\")\n",
    "else:\n",
    "    print(\"Didn't find Edinburgh in the data set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how even basic questions like \"Is Edinburgh in our list of data?\" are suddenly easy to answer? We no longer need to loop over the entire data set in order to find one data point. In addition, we know that everything in the 'Name' column will be a string, and that everything in the 'Longitude' column is a float, while the 'Population' column contains integers. So that's made life easier already. But let's test this out and see how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3 Standardising City Sizes\n",
    "\n",
    "To give you a sense of how scaleable this approach to data is, check out this neat little trick for working out $z$-scores for cities sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Use numpy functions to calculate mean and standard deviation\n",
    "mean = np.??(myData['Population'])\n",
    "std  = np.??(??)\n",
    "print(f\"City distribution has a mean {mean:,.0f} and standard deviation of {std:,.2f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`numpy` gives us a way to calculate the mean and standard deviation _quickly_ and without having to reinvent the wheel. The other potentially new thing here is `{std:,.2f}`. This is about [string formatting](https://www.w3schools.com/python/ref_string_format.asp) and the main thing to recognise is that this means 'format this float with commas separating the thousands/millions and 2 digits to the right'. The link I've provided uses the slightly older approach of `<str>.format()` but the formatting approach is the same.\n",
    "\n",
    "Does **that part** make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.4 Appending a Column\n",
    "\n",
    "Now we're going to see another useful benefit of this approach to the data: appending a new *column* to the data set!\n",
    "\n",
    "We'll also see how the code `[x for x in list]` gives us a way to apply an operation (converting to string, subtracting a value, etc.) to every item in a list without writing out a full for loop. This basically gives us a one-line way to avoid writing:\n",
    "```python\n",
    "rs = []\n",
    "for x in myData['Population']:\n",
    "    rs.append((x-mean)/std)\n",
    "```\n",
    "So here code in the `for` loop is applied and the result automatically added to the output list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We calculated mean and std above, so now we take\n",
    "# each population value and subtract the mean before\n",
    "# dividing by the standard deviation!\n",
    "rs = [(x - mean)/std for x in myData['Population']] # rs == result set\n",
    "print([f\"{x:.3f}\" for x in rs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's add it to the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myData['Std. Population'] = ??\n",
    "print(myData['Std. Population'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just to show how everything is in a single data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in myData['Name']:\n",
    "    idx = myData['Name'].index(c)\n",
    "    print(f\"{c} has a population of {myData['Population'][idx]:,} and standardised score of {myData['Std. Population'][idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. 'Functionalising'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start trying to pull what we've learned over the past two weeks together by creating a a set of functions that will help us to:\n",
    "\n",
    "1. Download a file from a URL (checking if it has already _been_ downloaded to save bandwidth).\n",
    "2. Parse it as a CSV file and...\n",
    "3. Convert it to a Dictionary-of-Lists\n",
    "4. Perform some simple calculations using the resulting data.\n",
    "\n",
    "To be honest, there's not going to be much about writing our _own_ objects here, but we will be making use of them and, conceptually, an understanding of objects and classes is going to be super-useful for understanding what we're doing in the remainder of the term!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Downloading from a URL\n",
    "\n",
    "Let's focus on the first part *first* because that's the precondition for everything else. If we can get the 'download a file from a URL' working then the rest will gradually fall into place through *iterative* improvments!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Finding an Existing Answer\n",
    "\n",
    "First, let's be sensibly lazy--we've already written code to read a file from the Internet and turn it into a list of lists. So I've copy+pasted that into the code block below since we're going to start from this point; however, just to help you check your own understanding, I've removed a few bits and replacement with `??`. Sorry. 😈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import csv\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/jreades/fsds/master/data/src/Wikipedia-Cities-simple.csv\"\n",
    "\n",
    "urlData = [] # Somewhere to store the data\n",
    "\n",
    "response = urlopen(url) # Get the data using the urlopen function\n",
    "csvfile  = csv.reader(response.read().decode('utf-8').splitlines()) # Pass it over to the reader\n",
    "\n",
    "for row in csvfile:\n",
    "    urlData.append(??)\n",
    "\n",
    "print(\"urlData has \" + str(len(urlData)) + \" rows and \" + str(len(urlData[0])) + \" columns.\")\n",
    "print(urlData[-1]) # Check it worked!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get `urlData has 11 rows and 4 columns.` and a row that looks like this: `['Bangor', '18808', '53.228', '-4.128']`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2: Getting Organised\n",
    "\n",
    "Let's take the code above and modify it so that it is:\n",
    "\n",
    "1. A function that takes two arguments: a URL; and a destination filename.\n",
    "2. Implemented as a function that checks if a file exists already before downloading it again.\n",
    "\n",
    "You will find that the `os` module helps here because of the `path` function. And you will [need to Google](https://lmgtfy.app/?q=check+if+file+exists+python) how to test if a file exists. I would normally select a StackOverflow link in the results list over anything else because there will normally be an _explanation_ included of why a particular answer is a 'good one'. I also look at which answers got the most votes (not always the same as the one that was the 'accepted answer'). In this particular case, I also found [this answer](https://careerkarma.com/blog/python-check-if-file-exists/) useful.\n",
    "\n",
    "--- \n",
    "\n",
    "I would start by setting my inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "url = \"https://raw.githubusercontent.com/jreades/fsds/master/data/src/Wikipedia-Cities-simple.csv\"\n",
    "out = os.path.join('data','Wikipedia-Cities.csv') # Print `out` if you aren't sure what this has done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3: Sketching the Function\n",
    "\n",
    "Then I would sketch out how my function will work using comments. And the simplest thing to start with is checking whether the file has already been downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "def get_url(src, dest):\n",
    "    \n",
    "    # Check if dest exists -- if it does\n",
    "    # then we can skip downloading the file,\n",
    "    # otherwise we have to download it!\n",
    "    if os.path.isfile(??):\n",
    "        print(f\"{dest} found!\")\n",
    "    else:\n",
    "        print(f\"{dest} *not* found!\")\n",
    "        \n",
    "get_url(url, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4: Fleshing Out the Function \n",
    "\n",
    "I would then flesh out the code so that it downloads the file if it isn't found and then, either way, returns the *local* file path for our CSV reader to extract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(src, dest):\n",
    "    \n",
    "    # Check if dest does *not* exist -- that\n",
    "    # would mean we had to download it!\n",
    "    if os.path.isfile(dest):\n",
    "        print(f\"{dest} found locally!\")\n",
    "    else:\n",
    "        print(f\"{dest} not found, downloading!\")\n",
    "        \n",
    "        # Get the data using the urlopen function\n",
    "        response = urlopen(src) \n",
    "        filedata = response.read().decode('utf-8')\n",
    "        \n",
    "        # Extract the part of the dest(ination) that is *not*\n",
    "        # the actual filename--have a look at how \n",
    "        # os.path.split works using `help(os.path.split)`\n",
    "        path = list(os.path.split(dest)[:-1])\n",
    "        \n",
    "        # Create any missing directories in dest(ination) path\n",
    "        # -- os.path.join is the reverse of split (as you saw above)\n",
    "        # but it doesn't work with lists... so I had to google how \n",
    "        # to use the 'splat' operator! os.makedirs creates missing \n",
    "        # directories in a path automatically.\n",
    "        if len(path) >= 1 and path[0] != '':\n",
    "            os.makedirs(os.path.join(*path), exist_ok=True)\n",
    "        \n",
    "        with open(dest, 'w') as f:\n",
    "            f.write(filedata)\n",
    "            \n",
    "        print(f\"Data written to {dest}!\")\n",
    "    \n",
    "    return dest\n",
    "        \n",
    "# Using the `return contents` line we make it easy to \n",
    "# see what our function is up to.\n",
    "src = get_url(url, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>&#9888; Stop!</b> Notice that we don't try to check if the data file contains any useful data! So if you download or create an empty file while testing, you won't necessarily get an error until you try to turn it into data afterwards!</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Parse the CSV File\n",
    "\n",
    "Now we turn to the next task: parsing the file if it's a CSV. This implies that it *might* not be so that's something we should also consider!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def read_csv(src):\n",
    "    \n",
    "    csvdata = []\n",
    "    with open(src, 'r') as f:\n",
    "        csvr = csv.??(f)\n",
    "        \n",
    "        for r in csvr:\n",
    "            csvdata.append(??)\n",
    "    \n",
    "    # Return list of lists\n",
    "    return ??\n",
    "\n",
    "read_csv(src)\n",
    "#read_csv('foo.bar') # <- Notice what happens if you try to run this code\n",
    "#read_csv('Practical-04-Objects-Answers.ipynb') # Or this code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:\n",
    "\n",
    "```\n",
    "[['City', 'Population', 'Latitude', 'Longitude'],\n",
    " ['Perth', '45770', '56.39583', '-3.43333'],\n",
    " ['Armagh', '14777', '54.3499', '-6.6546'],\n",
    " ['Dundee', '147268', '56.462', '-2.9707'],\n",
    " ['Colchester', '194706', '51.88861', '0.90361'],\n",
    " ['Salisbury', '40302', '51.07', '-1.79'],\n",
    " ['Portsmouth', '205056', '50.80583', '-1.08722'],\n",
    " ['Wakefield', '325837', '53.683', '-1.499'],\n",
    " ['Bradford', '522452', '53.792', '-1.754'],\n",
    " ['Lancaster', '138375', '54.047', '-2.801'],\n",
    " ['Bangor', '18808', '53.228', '-4.128']]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3: Convert the CSV into a DoL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can focus on converting the CSV data to a dictionary-of-lists! We're going to start with the *same* function name but expand what the function *does*. This kind of *iteration* is common in software development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def read_csv(src):\n",
    "    \n",
    "    csvdata = {} # An empty dictionary-of-lists\n",
    "    \n",
    "    with open(??, 'r') as f:\n",
    "        csvr = csv.reader(f)\n",
    "        \n",
    "        # Read in our column names and\n",
    "        # initialise the dictionary-of-lists\n",
    "        csvcols = next(csvr) \n",
    "        for c in csvcols:\n",
    "            csvdata[c] = []\n",
    "        \n",
    "        # Notice this code is still the same, \n",
    "        # we just used next(csvr) to get the \n",
    "        # header row first!\n",
    "        for r in ??: \n",
    "            # Although you can often assume that the order \n",
    "            # of the keys is the same, Python doesn't \n",
    "            # guarantee it; this way we will always make\n",
    "            # the correct assignment.\n",
    "            for idx, c in enumerate(csvcols):\n",
    "                csvdata[??].append(r[idx])\n",
    "    \n",
    "    # Return dictionary of lists\n",
    "    return csvdata\n",
    "\n",
    "read_csv(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get something that starts:\n",
    "```\n",
    "{'City': ['Perth',\n",
    "  'Armagh',\n",
    "  'Dundee',\n",
    "  'Colchester',\n",
    "  'Salisbury',\n",
    "  'Portsmouth',\n",
    "  'Wakefield',\n",
    "  'Bradford',\n",
    "  'Lancaster',\n",
    "  'Bangor'],\n",
    " 'Population': ['45770',\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4: Adding Docstring\n",
    "\n",
    "We've assumed that the first row of our data set is always a _header_ (i.e. list of column names). If it's not then this code is going to have problems. A _robust_ function would allow us to specify column names, skip rows, etc. when we create the data structure, but let's not get caught up in that level of detail. Notice that I've also, for the first time:\n",
    "\n",
    "1. Used the docstring support offered by Python. You'll be able to use `help(...)` and get back the docstring help!\n",
    "2. Provided hints to Python about the expected input and output data types. This can help to ensure consistency and is also critical in testing / continuous integration when working with others on a codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(src:str) -> dict:\n",
    "    \"\"\"\n",
    "    Converts a CSV file to a dictionary-of-lists (dol),\n",
    "    using the first row to create column names.\n",
    "    \n",
    "    :param src: a local CSV file\n",
    "    :returns: a dictionary-of-lists\n",
    "    \"\"\"\n",
    "    csvdata = {} # An empty dictionary-of-lists\n",
    "    \n",
    "    with open(src, 'r') as f:\n",
    "        csvr = csv.reader(f)\n",
    "        \n",
    "        # Read in our column names and\n",
    "        # initialise the dictionary-of-lists\n",
    "        csvcols = next(csvr) \n",
    "        for c in csvcols:\n",
    "            csvdata[c] = []\n",
    "        \n",
    "        # Notice this code is still the same, \n",
    "        # we just used next(csvr) to get the \n",
    "        # header row first!\n",
    "        for r in csvr: \n",
    "            # Although you can often assume that the order \n",
    "            # of the keys is the same, Python doesn't \n",
    "            # guarantee it; this way we will always make\n",
    "            # the correct assignment.\n",
    "            for idx, c in enumerate(csvcols):\n",
    "                csvdata[c].append(r[idx])\n",
    "    \n",
    "    # Return dictionary of lists\n",
    "    return csvdata\n",
    "\n",
    "ds = read_csv(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns are: \" + \", \".join(ds.keys()))\n",
    "print(f\"First two cities are: {ds['City'][:2]}\")\n",
    "print(f\"First two populations are: {ds['Population'][:2]}\")\n",
    "print(f\"First two latitudes are: {ds['Latitude'][:2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer should look like:\n",
    "```\n",
    "Columns are: City, Population, Latitude, Longitude\n",
    "First two cities are: ['Perth', 'Armagh']\n",
    "First two populations are: ['45770', '14777']\n",
    "First two latitudes are: ['56.39583', '54.3499']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.5: Fixing Data Types\n",
    "\n",
    "If you look closely at the above, you'll see that *everything* is a string, including the latitudes, longitudes, and populations, which are clearly numeric data types. Here's a 'simple' way to specify a `dtype` list to hold the _data type_ for each column. I'm also going to introduce you the `zip` function here as it has many uses with geographic data (especially converting lat/long to points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.1 Demonstrating Zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols  = ['City', 'Population', 'Latitude', 'Longitude'] # <- Column name\n",
    "dtype = [??, ??, ??, float]                        # <- Column data type\n",
    "\n",
    "# 'Zips up' these two lists into an iterator! So this will \n",
    "# take element 0 from *each* list and pass them to `col` as\n",
    "# a list-like 'tuple' (meaning there is a col[0] and a col[1]).\n",
    "for col in zip(cols, dtype):\n",
    "    colname = col[0]\n",
    "    coltype = col[1]\n",
    "    \n",
    "    # Notice the more advanced formatting here:\n",
    "    # - `>12` means right-align with up to 12 characters of whitespace; notice the last line!\n",
    "    # - `coltype.__name__` gives us the name of the data type, rather than a '<class...>' output.\n",
    "    print(f\"Column {colname:>12} should be type: {coltype.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.2 A Function to Convert Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the raw data to data of the appropriate\n",
    "# type: 'column data' (cdata) -> 'column type' (ctype)\n",
    "def to_type(cdata, ctype):\n",
    "    # If a string\n",
    "    if isinstance(cdata, str):\n",
    "        try:\n",
    "            if ctype==bool:\n",
    "                return cdata==True\n",
    "            else:\n",
    "                return ctype(cdata)\n",
    "        except TypeError:\n",
    "            return cdata\n",
    "    \n",
    "    # Not a string (assume list)\n",
    "    else: \n",
    "        fdata = []\n",
    "        for c in cdata:\n",
    "            try:\n",
    "                if ctype==bool:\n",
    "                    fdata.append( c=='True' )\n",
    "                else:\n",
    "                    fdata.append( ctype(c) )\n",
    "            except:\n",
    "                fdata.append( c )\n",
    "        return fdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now apply this! We'll copy the data to \n",
    "# new data structure only so that we know\n",
    "# we're not overwriting `ds` until we're sure\n",
    "# that the code works.\n",
    "ds2 = {}\n",
    "for col in zip(cols, dtype):\n",
    "    colname = col[0]\n",
    "    coltype = col[1]\n",
    "    ds2[ colname ] = to_type( ds[colname], coltype )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the output here to the output from `ds` up above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns are: \" + \", \".join(ds2.keys()))\n",
    "print(f\"First two cities are: {ds2['City'][:2]}\")\n",
    "print(f\"First two populations are: {ds2['Population'][:2]}\")\n",
    "print(f\"First two latitudes are: {ds2['Latitude'][:2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get the followg:\n",
    "```\n",
    "Columns are: City, Population, Latitude, Longitude\n",
    "First two cities are: ['Perth', 'Armagh']\n",
    "First two populations are: [45770, 14777]\n",
    "First two latitudes are: [56.39583, 54.3499]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.6: Checking Basic Functionality\n",
    "\n",
    "Now that we've got our data structure all set up correctly (appropriate names, data types, etc.) let's see if it works by testing out some of our previous operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # We'll need this to apply functions to lists easily\n",
    "\n",
    "print(f\"Average population is {np.mean(ds2['Population']):,.2f}\")\n",
    "print(f\"Westernmost city is {ds2['City'][np.where(ds2['Longitude']==np.min(ds2['Longitude']))[0][0]]}\")\n",
    "print(f\"Northernmost city is {ds2['City'][np.where(ds2['Latitude']==np.max(ds2['Latitude']))[0][0]]}\")\n",
    "print(f\"Southernmost city is {ds2['City'][np.where(ds2['Latitude']==np.min(ds2['Latitude']))[0][0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get the following:\n",
    "\n",
    "```\n",
    "Average population is 165,335.10\n",
    "Westernmost city is Armagh\n",
    "Northernmost city is Dundee\n",
    "Southernmost city is Portsmouth\n",
    "```\n",
    "\n",
    "There are a few things to understand here:\n",
    "\n",
    "1. `np.max`, `np.min`, `np.mean` and so on all calculate a *value* based a list or array of data. So the easiest case (and clearest output) here is `np.mean(sd2['Population'])` because we just want to know the mean of the list of populations. \n",
    "2. Where we want to find something else in the data *based on that value* then things get a little more complex: we use `np.min` to find the smallest value in the data, but we don't know *where* in the list that value actually *was* so we use `np.where` to find out which indexes match the minimum value. In this data set it's easy because there's one, and only one value that matches. But you'd need to do some clever thinking about how to handle ties.\n",
    "3. `np.where` returns a complex data structure (list-of-lists, essentially) so we need to pull the value we need out of that data in order to actually find the list index we need and look up the `City` name associated with, for example, the minium value in the data. That bit (`[0][0]`) is a bit clunky, but right now we're not too worried about it.\n",
    "\n",
    "Notice that we do *all* of this without recourse to a `for` loop or the need to keep track of different variables as we go... You *could* also achieve the same thing this way, but hopefully you see what the way we've used above is more *elegant* (it's also faster):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_long = 180\n",
    "min_idx  = -1\n",
    "\n",
    "for l in ds2['Longitude']:\n",
    "    if l < min_long: min_long = l\n",
    "#print(f\"Minimum longitude is {min_long}\")\n",
    "\n",
    "for idx, l2 in enumerate(ds2['Longitude']):\n",
    "    if l2==min_long:\n",
    "        min_idx = idx\n",
    "        break\n",
    "\n",
    "print(f\"Westernmost city is {ds2['City'][min_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Was it Worth It?\n",
    "\n",
    "At this point it's worth asking: was all this *worth* it? Let's see! \n",
    "\n",
    "The best way to test is to use a *different* data set and see if we've solved the 'hard-coding' problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/jreades/fsds/master/data/src/Wikipedia-Cities.csv\"\n",
    "out = os.path.join('data','Wikipedia-Cities-full.csv')\n",
    "\n",
    "cols  = ['City', 'Population', 'Latitude', 'Longitude']\n",
    "dtype = [str, int, float, float]\n",
    "\n",
    "untyped_dol = read_csv(get_url(??, ??))\n",
    "\n",
    "typed_dol = {}\n",
    "for col in zip(cols, dtype):\n",
    "    colname = col[0]\n",
    "    coltype = col[1]\n",
    "    typed_dol[ colname ] = to_type(untyped_dol[??], ??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average population is {np.mean(typed_dol['Population']):,.2f}\")\n",
    "print(f\"Westernmost city is {typed_dol['City'][np.where(typed_dol['Longitude']==np.min(typed_dol['Longitude']))[0][0]]}\")\n",
    "print(f\"Northernmost city is {typed_dol['City'][np.where(typed_dol['Latitude']==np.max(typed_dol['Latitude']))[0][0]]}\")\n",
    "print(f\"Southernmost city is {typed_dol['City'][np.where(typed_dol['Latitude']==np.min(typed_dol['Latitude']))[0][0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:\n",
    "\n",
    "```\n",
    "Average population is 202,283.36\n",
    "Westernmost city is Derry\n",
    "Northernmost city is Inverness, Inerness, Inbhir Nis\n",
    "Southernmost city is Truro\n",
    "```\n",
    "\n",
    "So we used all the same code as for the subset of the data but changed *nothing*. And this is even though the column order changed (print out the first row of each file if you don't believe me) *and* the number of columns changed *and* our city column now contains commas! So what this has given us is a much more flexible way not only to *access* the data, but also to *work* with it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. More Functions!\n",
    "\n",
    "Here is the skeleton of a function to replace the `np.where(column==np.[min|max|...](column))[0][0]` code. There are a few ways to do this: \n",
    "\n",
    "1. You could use if/else/elif and do different things based on testing against the specified string\n",
    "2. You could try to find a function in `numpy` that matches the specified string\n",
    "3. You could try to `eval` the code, but I really wouldn't recommend this for security reasons\n",
    "\n",
    "I have gone with a combination of 1 and 2, but you will need to really read the code to understand how it works. I've left the docstring for you to complete. This isn't the best function since it makes some assumptions about the types of data that it might be passed to the function: this is where *Object-Oriented Programming* could come to the rescue! If we had different column *types* (e.g. String, Float, Int) then we could have different *versions* of `find_val` that performed the same *function* (find a value) but did this completely differently depending on the data. This is what *methods* do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_val(col:list, val:str):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    if val in dir(np) and callable(getattr(np, val)):\n",
    "        func = getattr(np, val)\n",
    "        if val in ['min','max']:\n",
    "            return np.where(col==func(col))[0][0]\n",
    "        else:\n",
    "            return func(col)\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "print(find_val(typed_dol['Latitude'], 'min'))\n",
    "print(find_val(typed_dol['Latitude'], 'median'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average population is {find_val(typed_dol['Population'],'mean'):,.2f}\")\n",
    "print(f\"Westernmost city is {typed_dol['City'][find_val(typed_dol['Longitude'],'min')]}\")\n",
    "print(f\"Northernmost city is {typed_dol['City'][find_val(typed_dol['Latitude'],'max')]}\")\n",
    "print(f\"Southernmost city is {typed_dol['City'][find_val(typed_dol['Latitude'],'min')]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have streaminlined the code still further and implemented a fairly generic 'helper' function that uses `numpy` to perform calculations on a column of data (assuming it's numeric). We could, of course, extend this further, to handle strings and other data types, but we're going to see a *better* way to do all of this *next* week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Creating a Package from Functions\n",
    "\n",
    "Below is code to create a package called `dtools` (i.e. data tools) by converting the notebook into a Python script file called `__init__.py` that sits in the `dtools` directory. This is the first step to creating a package from code that is *already* working in a Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the directory using `mkdir` (the `!` means 'run this O/S command'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p 'dtools'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run this `jupyter` command to convert the notebook to `__init__.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented out so we don't automatically re-run this code every time\n",
    "#!jupyter nbconvert --ClearOutputPreprocessor.enabled=True \\\n",
    "#    --to python --output=dtools/__init__.py \\\n",
    "#    Practical-04-Objects-Answers.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will then need to tidy up the content of `dtools/__init__.py` directly in JupyterLab by double-clicking it:\n",
    "\n",
    "1. Keep all *final* versions of each function (i.e. `get_url`, `read_csv`, `to_type` and `find_val`)\n",
    "2. Keep all `import` and `from x import y` statements and gather them together at the start of the file.\n",
    "\n",
    "You should then be able to run the code below and can always compare it to my version [on GitHub](https://github.com/jreades/fsds/blob/master/practicals/dtools/__init__.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dtools\n",
    "help(dtools.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://github.com/jreades/fsds/raw/master/data/2019-sample-crime.csv'\n",
    "out = os.path.join('data','crime-sample.csv')\n",
    "\n",
    "ds = ??.read_csv(??.get_url(url, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ds has {len(ds.keys())} columns, these are: \" + \", \".join(ds.keys()))\n",
    "print(f\"There are {len(ds['ID'])} rows of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols  = ['Latitude', 'Longitude'] \n",
    "dtype = [float, float]\n",
    "\n",
    "typed_ds = {}\n",
    "for col in zip(cols, dtype): # <- This only copies these two columns to typed_ds\n",
    "    colname = col[0]\n",
    "    coltype = col[1]\n",
    "    typed_ds[ colname ] = dtools.to_type(ds[colname], coltype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = find_val(typed_ds['Latitude'], 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds['Case Number'][idx])\n",
    "print(ds['Description'][idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
