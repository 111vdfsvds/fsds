[
  {
    "objectID": "assessments.html",
    "href": "assessments.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "The overall assessment package is intended to test students’ comprehension of, and ability to integrate, technical skills with a broader understanding of, and reflection upon, computational approaches to urban research and spatial data science. The assessments are grounded in a mixture of critical reflection and group work that map on to real-world data science challenges, including:\n\nExamining a data set to determine its suitability for tackling a set ‘problem’ or ‘challenge’;\nCollaboratively writing a data-led policy briefing involving high-quality code, analysis, visualisation, and text suitable for use by a policy- or decision-maker;\nReflecting on the successes/failures of a completed project so as to improve future patterns and processes.\n\nCollectively, these assessments provide multiple opportunities to ‘shine’ both individually and as part of a group."
  },
  {
    "objectID": "assessments/briefing.html",
    "href": "assessments/briefing.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "The Policy Briefing will present an analysis of data from the Inside Airbnb web site for London. Students may use data from more than one time period if they wish, but this is not required. The briefing will be written as if to update the Mayor of the Greater London Authority on the challenges/opportunities relating to Airbnb’s operations in London. As such, the Policy Briefing will begin with a short Summary, including Key Findings and, if desired, Recommendations; this will be followed by a brief review of the evidence from both London and elsewhere, and then an analysis supported by the data and presented using appropriately generated and selected tables, charts, and maps.\nThe focus of this assessment is the student’s ability to make use of concepts and methods covered in class as part of an analytical process to support decision-making in a non-academic context. It is not necessary that you employ every technique covered in class. It is necessary that you justify your choice of approach with reference to relevant academic and ‘grey’ literature, as well as the computational, statistical, and analytical objectives of your briefing paper. It is perfectly possible to complete this assessment without the use of advanced analytical topics (e.g. clustering, NLP, or global/local/LISA autocorrelation methods); however, it is unlikely that you would be able to complete this assessment to a high standard without some graphs and some maps chosen for their ability to advance your argument.\nThe briefing may be written without substantially new modelling or coding by drawing on the code written in practicals to develop an analysis based on the judicious use of descriptive statistics (see, for instance, Housing and Inequality in London and The suburbanisation of poverty in British cities, 2004-16: extent, processes and nature), but it is likely that a better mark will be obtained by demonstrating the capacity to go beyond exactly what was taught by selectively deploying more advanced programming techniques.\nThis is not an essay, and students who submit a traditional essay format will be penalised in the overall mark. Examples of the kind of format, tone, and content expected are provided below and there will be opportunities to discuss the submission during Term.\n\n\n\n\n\n\nWarning\n\n\n\nAlways remember that you are writing this briefing for the Mayor of London! Are they going to know what k-Means Clustering is? Will they care? What makes writing a good briefing hard—and not just about writing good code—is finding the right balance of technical detail and high-level explanation: you can’t just say ‘here are the five types of accommodation we found…’, but you also can’t say ‘we tested clustering solutions in the range 3–50 and found the optimal result at k=12…’ You should have a look at the examples below."
  },
  {
    "objectID": "format.html",
    "href": "format.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "We have ‘flipped’ the classroom for this module so we expect you to come to ‘lecture’ (except in Week 1!) having already watched the assigned videos and completed the assigned readings. You may be called upon to present a short summary of the key points in, and relevance of, an assigned video or reading to the rest of the class.\nThis means that there is a mix of ‘asynchronous’ (work that you do in your own time) and ‘synchronous’ (work that we do during scheduled hours) interaction. Synchronous activities will normally be recorded for review afterwards, but you should bear in the mind the following: 1) we cannot be responsible for equipment failure; 2) small-group practicals cannot be recorded; and 3) a 2-hour video of a group discussion and live coding session will be rather less educational and informative than actually being there.\nIn short, recordings should not be used as a substitute for attendance save in exceptional circumstances.\n\n\n\nThe nature and amount of preparation will vary from week to week, but may include:\n\nReadings from both academic and non-academic sources.\nRecorded lectures from CASA staff.\nRecorded videos from non-CASA staff.\nShort Moodle quizzes to test your completion of readings and videos.\nPreparing contributions to set tasks (e.g. summaries of video lectures or readings, Q&A, etc.)\n\n\n\n\nThe ‘lecture’ in your timetable will be used for a mix of discussion and ‘live coding’ (eeek!) using the following framework:\n\nWe will review questions and issues arising from the previous week’s practical session and the weekly Padlet. We will use this to prioritise discussion around concepts and readings with which students are struggling or wish to engage further.\nWe will have a ‘live coding’ session following an ‘I do/We do’ format: we will employ concepts covered in the week’s activities, as well as approaches that will be explored further in the practical, to look a real-world data set together using code.\n\n\n\n\nIn order to make use of these materials you will need to download and install the Spatial Data Science computing environment. Details for this are provided in the Appendix.\nPracticals are run in small groups to maximise your ability to ask questions and interact with other students. You will be notified of your group by the Professional Services team; there may be limited opportunities to switch, but the best way would be to swap with another student and then notify us of the arrangment. You may wish to download the week’s Jupyter notebook before the start of class in order to familiarise yourself with the material.\n\n\n\nTo get the most value from the module you must do the readings even if we are not specifically referencing them in-class because of time-constraints. In previous years students who did not do the readings often failed the first assessment and struggled with the final assessment, leaving a lot of easy marks on the table. More importantly, we believe that the single most important skill that you can acquire from FSDS is not the ability to code, it’s the ability to critically interrogate data and recognise the strengths and limitations that are relevant to the problem at-hand. You will learn the technical aspects of this in the practicals. You will learn the theoretical dimension from doing the readings."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "The Bartlett Centre for Advanced Spatial Analysis↩︎"
  },
  {
    "objectID": "index.html#to-dos",
    "href": "index.html#to-dos",
    "title": "Foundations of Spatial Data Science",
    "section": "To Dos",
    "text": "To Dos\n\nShift away from .values\nSwap Weeks 8 (Visualising Data) and 9 (Dimensions in Data) + explain rationale to students.\nShift focus of (new) Week 9 & 10 to review/discussing the final assessment. Content still available to view/practice but not required/delivered.\nRequire more active use of GitHub/Git — a submission in which it’s easy to automate checks that they’ve created a repo and populated it with the completed notebooks?\nRefresh Code Camp and make it more self-test oriented. Use the self-test to set expectations about level of effort/preparation required (and whether or not taking the module will be useful).\nStandardise delivery of practical content by TAs: should have consistent approach across practicals.\nPoint to cross-module content/recaps.\nJoint reading list?\nInvestigate CoCalc as an alternative. What is situation with JupyterHub at UCL."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Foundations of Spatial Data Science",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWhile this module in indebted to both feedback from students and colleagues over the years, several people played a particularly outsize role in my thinking and deserve special acknowledgement:\n\nDani (for help with Docker, geopandas, and any number of other tools with which I’ve had to familiarise myself)\nAndy (for somehow knowing about all kinds of new web apps that I could use to create ‘books’ to support the module)\nThe Geocomp team at King’s College London, who supported my hare-brained scheme to teach Geography undergrads to code and offered all manner of useful feedback on what we could/could not feasibly cover."
  },
  {
    "objectID": "lectures/Presentation.html",
    "href": "lectures/Presentation.html",
    "title": "Foo",
    "section": "",
    "text": "Turn off alarm\nGet out of bed"
  },
  {
    "objectID": "lectures/Presentation.html#going-to-sleep",
    "href": "lectures/Presentation.html#going-to-sleep",
    "title": "Foo",
    "section": "Going to sleep",
    "text": "Going to sleep\n\nGet in bed\nCount sheep"
  },
  {
    "objectID": "week1.html",
    "href": "week1.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "Changes before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nMore time on Git/GitHub and pull/push/browsing version history; point to setting quantitative research questions (CASA0007 Week 1) but shift focus on to policy. Explain why there are two assessments and why one has group work with a peer component.\n\nRemove Docker installation practical from FSDS; keep in QM.\n\nJon to create short video on purpose of Docker that can be used in QM and discussed in FSDS (as part of a data science ‘workflow’)\n\nAdd discussion of .gitignore file to GitHub practical (and make sure they add .gz, .csv, and .zip!)\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis week’s Learning Outcomes are:\n\n\n\n\n\n\n\nThis week is focussed on getting you set up for the rest of the course in terms of having the requisite software installed and accounts configured so that you can keep track of your work, write code, and track changes. However, you should also see this as preparing the foundation not only for your remaining CASA modules (especially those in Term 2) but also for your post-MSc career: the ability to manage and version code (GitHub); collaborate around a shared codebase (GitHub+Markdown); and produce reproducible code (GitHub+Docker) is integral to modern software development and data science.\nYou should also see this session as connecting to Quantitative Methods Week 1 content on ‘setting quantitative research questions’ since the main assessment will require you to develop a data-led policy briefing. In other words, you’ll need to map current policy on to one or more research questions that can be quantitatively examined using the tools and techniques acquired over the course of the term! While you don’t need to start work on this yet, you should keep it in the back of your mind for when you come across readings/results that you’d like to explore in more detail.\n\n\n\nAlthough none of these activities are compulsory in advance of the first session, getting your computer set up to code does take time and most of these preparatory activites are fairly straightforward… with a few exceptions noted below. If you are able to get these tools installed in advance then you can focus on the taught content in the first two practicals rather than also wrestling with an installation process. This will also give us more time to help you if you discover that you’re one of the unlucky few for whom getting set up is a lot more work!\nComplete as many of these activities as you can:\n\nComplete the quick computer health check.\nHave a go at installing the Command Line Tools for your operating system.\nHave a go at installing the programming environment.\n\nThe last of these is the stage where you’re most likely to encounter problems that will need our assistance, so knowing that you need our help in Week 1 means that you can ask for it much sooner in the practical!\n\n\n\nIn this week’s workshop we will review the module aims, learning outcomes, and expectations with a general introduction to the course.\n\n\n\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nStandardise delivery by TAs where practicals are TA-led (e.g. FSDS): clearer guidance on how to take students through each week.\n\nClearer articulation of difficulty levels/targets for components within each week’s practicals.\n\nMake links between lecture and practical content explicit; ideally trace a question through the whole process (e.g. “If I wanted to know if and where blue plaques are clustered in London how would I find out?”)\n\n\n\nIf you have already installed the programming environment then the practical can be downloaded from GitHub (Click on Raw and then Save File As...) and viewed by saving it to your Documents/CASA/... folder, or you can just view it online in your web browser since there is no actual coding this week."
  },
  {
    "objectID": "week10.html",
    "href": "week10.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "Changes before 22/23\n\n\n\n\n\nAdd log-scale option, data joins (link to CASA0005)\n\nAdd Learning Outcomes for each week\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis week’s Learning Outcomes are:\n\n\n\n\n\n\n\n\n\n\n\n\nCome to class prepared to present:\n\n\n\nVideo\nMarkdown for Note-taking\n\n\n\n\nLinking Data\nNotes\n\n\nLinking Spatial Data\nNotes\n\n\nGrouping Data\nNotes\n\n\nData Visualisation\nNotes\n\n\n\n\n\n\nCome to class prepared to present:\n\n(D’Ignazio and Klein 2020, chap. 3) On Rational, Scientific, Objective Viewpoints from Mythical, Imaginary, Impossible Standpoints in Data Feminism; Pre-review URL\n(Badger, Bui, and Gebeloff 2019) The Neighborhood Is Mostly Black. The Home Buyers Are Mostly White. URL\n\n\n\n\n\nPadlet: [Collaborative Agenda]\nComplete the short Moodle quiz associated with this week’s activities.\n\n\n\n\n\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nStandardise delivery by TAs where practicals are TA-led (e.g. FSDS): clearer guidance on how to take students through each week.\n\nClearer articulation of difficulty levels/targets for components within each week’s practicals.\n\nMake links between lecture and practical content explicit; ideally trace a question through the whole process (e.g. “If I wanted to know if and where blue plaques are clustered in London how would I find out?”)\n\n\n\nThe practical can be downloaded from GitHub.\n\n\n\n\n\n\n\nBadger, E., Q. Bui, and R. Gebeloff. 2019. “Neighborhood Is Mostly Black. The Home Buyers Are Mostly White. New York Times.” New York Times. https://www.nytimes.com/interactive/2019/04/27/upshot/diversity-housing-maps-raleigh-gentrification.html.\n\n\nD’Ignazio, Catherine, and Lauren F. Klein. 2020. Data Feminism. MIT Press. https://bookbook.pubpub.org/data-feminism."
  },
  {
    "objectID": "week10.html#class",
    "href": "week10.html#class",
    "title": "Foundations of Spatial Data Science",
    "section": "Class",
    "text": "Class\n\nReviewing the Collaborative Agenda\nDiscussion of Readings\nLive Coding"
  },
  {
    "objectID": "week10.html#practical",
    "href": "week10.html#practical",
    "title": "Foundations of Spatial Data Science",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nStandardise delivery by TAs where practicals are TA-led (e.g. FSDS): clearer guidance on how to take students through each week.\n\nClearer articulation of difficulty levels/targets for components within each week’s practicals.\n\nMake links between lecture and practical content explicit; ideally trace a question through the whole process (e.g. “If I wanted to know if and where blue plaques are clustered in London how would I find out?”)\n\n\n\nThe practical can then be downloaded from GitHub.\n\n\n\n\nBadger, E., Q. Bui, and R. Gebeloff. 2019. “Neighborhood Is Mostly Black. The Home Buyers Are Mostly White. New York Times.” New York Times. https://www.nytimes.com/interactive/2019/04/27/upshot/diversity-housing-maps-raleigh-gentrification.html.\n\n\nD’Ignazio, Catherine, and Lauren F. Klein. 2020. Data Feminism. MIT Press. https://bookbook.pubpub.org/data-feminism."
  },
  {
    "objectID": "week11.html",
    "href": "week11.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "Changes before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nPoint to clustering code and show DBSCAN, k-Means, etc. as applied to the Airbnb data.\n\nStress different view of clustering as part of a pipeline, not an absolute “just k-means it”; link to thinking about paradigms in CASA0001;\n\nthinking about proxies (why would you find ppsqm more/less useful than price? what are you really measuring?\n\nConnects to both CASA0001 and CASA0007 and CASA0005.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis week’s Learning Outcomes are:\n\n\n\n\n\n\n\n\n\n\n\n\nCome to class prepared to briefly present:\n\n\n\nVideo\nMarkdown for Note-taking\n\n\n\n\nClassification\nNotes\n\n\nClustering\nNotes\n\n\n\n\n\n\nCome to class prepared to briefly present:\n\n(Shapiro and Yavuz 2017) URL\n(Wolf et al. 2021) DOI\n(Singleton and Arribas-Bel 2021) DOI\n\n\n\n\n\nPadlet: [Collaborative Agenda]\nComplete the short Moodle quiz associated with this week’s activities.\n\n\n\n\n\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nStandardise delivery by TAs where practicals are TA-led (e.g. FSDS): clearer guidance on how to take students through each week.\n\nClearer articulation of difficulty levels/targets for components within each week’s practicals.\n\nMake links between lecture and practical content explicit; ideally trace a question through the whole process (e.g. “If I wanted to know if and where blue plaques are clustered in London how would I find out?”)\n\n\n\nThe practical can be downloaded from GitHub.\n\n\n\n\n\n\n\nShapiro, W., and M. Yavuz. 2017. “Rethinking ’distance’ in New York City.” Medium. https://medium.com/topos-ai/rethinking-distance-in-new-york-city-d17212d24919.\n\n\nSingleton, Alex, and Daniel Arribas-Bel. 2021. “Geographic Data Science.” Geographical Analysis 53 (1):61–75. https://doi.org/10.1111/gean.12194.\n\n\nWolf, Levi John, Sean Fox, Rich Harris, Ron Johnston, Kelvyn Jones, David Manley, Emmanouil Tranos, and Wenfei Winnie Wang. 2021. “Quantitative Geography III: Future Challenges and Challenging Futures.” Progress in Human Geography 45 (3). SAGE Publications Sage UK: London, England:596–608."
  },
  {
    "objectID": "week2.html",
    "href": "week2.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "Changes before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nMore focus on code re-use and simplicity.\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis week’s Learning Outcomes are:\n\n\n\n\n\n\n\nThis week we will be (briefly) reviewing the basics of Python with a view to recapping the more complex aspects of simple data structures. And yes, that’s a bit of an oxymoron, but we’re focussing here on how computers ‘think’ and how that differs from what you might be expecting as an intelligen human being!\nWe will be contextualising all of this within the longer history of the study of geography through computation. I hope to convince you that many of the problems we face today are not new and why that should encourage you to continue to do the readings!\n\n\n\nAs we’re working in a ‘flipped’ environment, you should watch these videos before class so that the ‘live’ workshop can focus on demonstration, discussion, and clarification. This week is very busy because we need to cover off the basics for those of you who were unable to engage with Code Camp, while recapping only the crucial bits for those of you who were able to do so.\nCome to class prepared to present:\n\n\n\nVideo\nMarkdown for Note-taking\n\n\n\n\nPython: the Basics\nNotes\n\n\nLists\nNotes\n\n\nIteration\nNotes\n\n\nThe Command Line\nNotes\n\n\nGetting Stuck Into Git\nNotes\n\n\n\n\n\nCome to class prepared to present:\n\n(Burton 1963) DOI\n(Arribas-Bel and Reades 2018) DOI\n\n\n\n\n\nPadlet: [Collaborative Agenda]\nComplete the short Moodle quiz associated with this week’s activities.\n\n\n\n\n\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nStandardise delivery by TAs where practicals are TA-led (e.g. FSDS): clearer guidance on how to take students through each week.\n\nClearer articulation of difficulty levels/targets for components within each week’s practicals.\n\nMake links between lecture and practical content explicit; ideally trace a question through the whole process (e.g. “If I wanted to know if and where blue plaques are clustered in London how would I find out?”)\n\n\n\nThis week’s practical requires you to have completed installation of the programming environment. The practical can be downloaded from GitHub.\n\n\n\n\n\n\n\nArribas-Bel, D., and J. Reades. 2018. “Geography and Computers: Past, Present, and Future.” Geography Compass 0 (0):e12403. https://doi.org/10.1111/gec3.12403.\n\n\nBurton, I. 1963. “The Quantitative Revolution and Theoretical Geography.” The Canadian Geographer/Le Géographe Canadien 7 (4):151–62. https://doi.org/10.1111/j.1541-0064.1963.tb00796.x."
  },
  {
    "objectID": "week3.html",
    "href": "week3.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "Changes before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nMore focus on code re-use and simplicity.\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis week’s Learning Outcomes are:\n\n\n\n\n\n\n\nThis week we start to move beyond Code Camp. So although you should recognise many of the parts that we discuss, you’ll see that we start to put them together in a new way.\n\n\n\n\n\nAll students should complete/revisit Code Camp Notebooks 8–11.\nNote: there is an issue with the GeoJSON tasks in Notebooks 8 and 9. We can discuss in the Class.\n\n\n\nCome to class prepared to present:\n\n\n\nVideo\nMarkdown for Note-taking\n\n\n\n\nDictionaries\nNotes\n\n\nLOLs\nNotes\n\n\nDOLs to Data\nNotes\n\n\nFunctions\nNotes\n\n\nPackages\nNotes\n\n\n\n\n\n\nCome to class prepared to present:\n\n(Etherington 2016) DOI\n(Donoho 2017) DOI\n(Unwin 1980) DOI\n\n\n\n\n\nPadlet: [Collaborative Agenda]\nComplete the short Moodle quiz associated with this week’s activities.\n\n\n\n\n\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nStandardise delivery by TAs where practicals are TA-led (e.g. FSDS): clearer guidance on how to take students through each week.\n\nClearer articulation of difficulty levels/targets for components within each week’s practicals.\n\nMake links between lecture and practical content explicit; ideally trace a question through the whole process (e.g. “If I wanted to know if and where blue plaques are clustered in London how would I find out?”)\n\n\n\nThe practical can be downloaded from GitHub.\n\n\n\n\n\n\n\nDonoho, D. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4):745–66. https://doi.org/10.1007/978-3-642-23430-9_71.\n\n\nEtherington, Thomas R. 2016. “Teaching Introductory GIS Programming to Geographers Using an Open Source Python Approach.” Journal of Geography in Higher Education 40 (1). Taylor & Francis:117–30.\n\n\nUnwin, David. 1980. “Make Your Practicals Open-Ended.” Journal of Geography in Higher Education 4 (2). Taylor & Francis:39–42. https://doi.org/10.1080/03098268008708772."
  },
  {
    "objectID": "week4.html",
    "href": "week4.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "Changes before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nMore focus on code re-use and simplicity.\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis week’s Learning Outcomes are:\n\n\n\n\n\n\n\nThis week deals with ‘objects’ and ‘classes’, which is fundamental to mastering the Python programming language: in Python, everything is an object, you just didn’t need to know it until now. Understanding how classes and objects work is essential to using Python effectively, but it will also make you a better programmer in any language because it will help you to think about how data and code work together to achieve your goals.\n\n\n\n\n\nCome to class prepared to present:\n\n\n\n\n\n\nFocus\n\n\n\n\n\n\n\n\n\nVideo\nMarkdown for Note-taking\n\n\n\n\nMethods\nNotes\n\n\nClasses\nNotes\n\n\nDesign\n(Notes\n\n\nExceptions\nNotes\n\n\n\n\n\n\nCome to class prepared to present:\n\n(Cox and Slee 2016) PDF\n(D’Ignazio and Klein 2020, chap. 5) Pre-review URL: Unicorns, Janitors, Ninjas, Wizards, and Rock Stars\n\n\n\n\n\nPadlet: [Collaborative Agenda]\nComplete the early feedback questionnaire please!\nComplete the short quiz\n\n\n\n\n\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nStandardise delivery by TAs where practicals are TA-led (e.g. FSDS): clearer guidance on how to take students through each week.\n\nClearer articulation of difficulty levels/targets for components within each week’s practicals.\n\nMake links between lecture and practical content explicit; ideally trace a question through the whole process (e.g. “If I wanted to know if and where blue plaques are clustered in London how would I find out?”“)\n\n\n\nThe practical can be downloaded from GitHub.\n\n\n\n\n\n\n\nCox, M., and T. Slee. 2016. “How Airbnb’s Data Hid the Facts in New York City.” Inside Airbnb. http://insideairbnb.com/reports/how-airbnbs-data-hid-the-facts-in-new-york-city.pdf.\n\n\nD’Ignazio, Catherine, and Lauren F. Klein. 2020. Data Feminism. MIT Press. https://bookbook.pubpub.org/data-feminism."
  },
  {
    "objectID": "week5.html",
    "href": "week5.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "Changes before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nHighlight differences from CASA0007; Airbnb data from CASA0005\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis week’s Learning Outcomes are:\n\n\n\n\n\n\n\n\n\n\n\n\nCome to class prepared to present:\n\n\n\nVideo\nMarkdown for Note-taking\n\n\n\n\nLogic\nNotes\n\n\nRandomness\nNotes\n\n\nData Files\nNotes\n\n\nPandas\nNotes\n\n\n\n\n\n\nCome to class prepared to present:\n\n(D’Ignazio and Klein 2020, chap. 4) Pre-review URL What Gets Counted Counts\n(Wachsmuth and Weisler 2018) DOI\n(Harris 2018) URL\n\n\n\n\n\nPadlet: [Collaborative Agenda]\nShort Moodle quiz\n\n\n\n\n\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nStandardise delivery by TAs where practicals are TA-led (e.g. FSDS): clearer guidance on how to take students through each week.\n\nClearer articulation of difficulty levels/targets for components within each week’s practicals.\n\nMake links between lecture and practical content explicit; ideally trace a question through the whole process (e.g. “If I wanted to know if and where blue plaques are clustered in London how would I find out?”)\n\n\n\nThe practical can be downloaded from GitHub.\n\n\n\n\n\n\n\nD’Ignazio, Catherine, and Lauren F. Klein. 2020. Data Feminism. MIT Press. https://bookbook.pubpub.org/data-feminism.\n\n\nHarris, J. 2018. “Profiteers Make a Killing on Airbnb - and Erode Communities.” The Guardian. https://www.theguardian.com/commentisfree/2018/feb/12/profiteers-killing-airbnb-erode-communities.\n\n\nWachsmuth, D., and A. Weisler. 2018. “Airbnb and the Rent Gap: Gentrification Through the Sharing Economy.” Environment and Planning A: Economy and Space 50 (6):1147–70. https://doi.org/10.1177/0308518X18778038."
  },
  {
    "objectID": "week6.html",
    "href": "week6.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "Changes before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nHighlight relevance of readings to Assessment 1 and overall Learning Outcomes.\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis week’s Learning Outcomes are:\n\n\n\n\n\n\n\nYou should be doing the readings that will support your answers to Assessment #2. And, looking ahead to the Final Assessment and where you might find ideas or literature to support your thinking, I’d suggest having a browse of the Full Bibliography. This is a working document and I will add more items as and when I come across them or new works are published!\n\n\n\nIn addition to looking for relevant content in (D’Ignazio and Klein 2020) (URL), you will also want to check consider:\n\n(Elwood and Wilson 2017) DOI\n(Elwood and Leszczynski 2018) DOI\n(Bemt et al. 2018) DOI\n(Amoore 2019) DOI\n(Crawford and Finn 2015) DOI\n\n\n\n\n\nAmoore, L. 2019. “Doubt and the Algorithm: On the Partial Accounts of Machine Learning.” Theory, Culture, Society 36 (6):147–69. https://doi.org/https://doi.org/10.1177%2F0263276419851846.\n\n\nBemt, V. van den, J. Doornbos, L. Meijering, M. Plegt, and N. Theunissen. 2018. “Teaching Ethics When Working with Geocoded Data: A Novel Experiential Learning Approach.” Journal of Geography in Higher Education 42 (2):293–310. https://doi.org/https://doi.org/10.1080/03098265.2018.1436534.\n\n\nCrawford, K., and M. Finn. 2015. “The Limits of Crisis Data: Analytical and Ethical Challenges of Using Social and Mobile Data to Understand Disasters.” GeoJournal 80 (4):491–502. https://doi.org/https://doi.org/10.1007/s10708-014-9597-z.\n\n\nD’Ignazio, Catherine, and Lauren F. Klein. 2020. Data Feminism. MIT Press. https://bookbook.pubpub.org/data-feminism.\n\n\nElwood, S., and A. Leszczynski. 2018. “Feminist Digital Geographies.” Gender, Place and Culture 25 (5):629–44. https://doi.org/https://doi.org/10.1080/0966369X.2018.1465396.\n\n\nElwood, S., and M. Wilson. 2017. “Critical GIS Pedagogies Beyond ‘Week 10: Ethics.” International Journal of Geographical Information Science 31 (10):2098–2116. https://doi.org/https://doi.org/10.1080/13658816.2017.1334892."
  },
  {
    "objectID": "week7.html",
    "href": "week7.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "Changes before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nLink back to Week 5 CASA0005 (Mapping) and Weeks 1–3 CASA0005 (Spatial Data)\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis week’s Learning Outcomes are:\n\n\n\n\n\n\n\n\n\n\n\n\nCome to class prepared to present:\n\n\n\nVideo\nMarkdown for Note-taking\n\n\n\n\nMapping\nNotes\n\n\nGeoPandas\nNotes\n\n\nEDA\nNotes\n\n\nESDA\nNotes\n\n\n\n\n\n\nCome to class prepared to present:\n\n(D’Ignazio and Klein 2020, chap. 6) The Numbers Don’t Speak for Themselves in Data Feminism; Pre-review URL\n(Elwood and Wilson 2017) DOI\n\n\n\n\n\nPadlet: [Collaborative Agenda]\nComplete the short Moodle quiz associated with this week’s activities.\n\n\n\n\n\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nStandardise delivery by TAs where practicals are TA-led (e.g. FSDS): clearer guidance on how to take students through each week.\n\nClearer articulation of difficulty levels/targets for components within each week’s practicals.\n\nMake links between lecture and practical content explicit; ideally trace a question through the whole process (e.g. “If I wanted to know if and where blue plaques are clustered in London how would I find out?”)\n\n\n\nThe practical can be downloaded from GitHub.\n\n\n\n\n\n\n\nD’Ignazio, Catherine, and Lauren F. Klein. 2020. Data Feminism. MIT Press. https://bookbook.pubpub.org/data-feminism.\n\n\nElwood, S., and M. Wilson. 2017. “Critical GIS Pedagogies Beyond ‘Week 10: Ethics.” International Journal of Geographical Information Science 31 (10):2098–2116. https://doi.org/https://doi.org/10.1080/13658816.2017.1334892."
  },
  {
    "objectID": "week8.html",
    "href": "week8.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "Changes before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nLink to regex/working with strings in Week 2 CASA0005\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis week’s Learning Outcomes are:\n\n\n\n\n\n\n\n\n\n\n\n\nCOme to class prepared to present:\n\n\n\nVideo\nMarkdown for Note-taking\n\n\n\n\nNotebooks as Documents\nNotes\n\n\nPatterns in Text\nNotes\n\n\nCleaning Text\nNotes\n\n\nAnalysing Text\nNotes\n\n\n\n\n\n\nCome to class prepared to present:\n\n(Ladd 2020) Web Page\n(Lavin 2019) Web Page\n\n\n\n\n\nPadlet: [Collaborative Agenda]\nComplete the short Moodle quiz associated with this week’s activities.\n\n\n\n\n\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nStandardise delivery by TAs where practicals are TA-led (e.g. FSDS): clearer guidance on how to take students through each week.\n\nClearer articulation of difficulty levels/targets for components within each week’s practicals.\n\nMake links between lecture and practical content explicit; ideally trace a question through the whole process (e.g. “If I wanted to know if and where blue plaques are clustered in London how would I find out?”)\n\n\n\nThe practical can be downloaded from GitHub.\n\n\n\n\n\n\n\nLadd, John R. 2020. “Understanding and Using Common Similarity Measures for Text Analysis.” The Programming Historian, no. 9. https://doi.org/10.46430/phen0089.\n\n\nLavin, Matthew J. 2019. “Analyzing Documents with TF-IDF.” The Programming Historian, no. 8. https://doi.org/10.46430/phen0082."
  },
  {
    "objectID": "week9.html",
    "href": "week9.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "Changes before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nLink to clustering (Week 6 GIS & QM)\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis week’s Learning Outcomes are:\n\n\n\n\n\n\n\n## Preparation\n\n\nCome to class prepared to present:\n\n\n\nVideo\nMarkdown for Note-taking\n\n\n\n\nThe Data Space\nNotes\n\n\nTransformation\nNotes\n\n\nDimensionality\nNotes\n\n\n\n\n\n\nCome to class prepared to present:\n\n(Bunday n.d.) A Final Tale or You Can Prove Anything with Figures URL\n(Harris, n.d.) The Certain Uncertainty of University Rankings URL\n(Cima, n.d.) The Most and Least Diverse Cities in America, Priceonomics URL (PDF with Figures)\n(Lu and Henning 2013) Are statisticians cold-blooded bosses? a new perspective on the ‘old’ concept of statistical population DOI\n\n\n\n\n\nPadlet: [Collaborative Agenda]\nComplete the short Moodle quiz associated with this week’s activities.\n\n\n\n\n\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nStandardise delivery by TAs where practicals are TA-led (e.g. FSDS): clearer guidance on how to take students through each week.\n\nClearer articulation of difficulty levels/targets for components within each week’s practicals.\n\nMake links between lecture and practical content explicit; ideally trace a question through the whole process (e.g. “If I wanted to know if and where blue plaques are clustered in London how would I find out?”)\n\n\n\nThe practical can be downloaded from GitHub.\n\n\n\n\n\n\n\nBunday, B. D. n.d. “A Final Tale or You Can Prove Anything with Figures.” https://www.ucl.ac.uk/~ucahhwi/AFinalTale.pdf.\n\n\nCima, R. n.d. “The Most and Least Diverse Cities in America.” Priceonomics. https://priceonomics.com/the-most-and-least-diverse-cities-in-america/.\n\n\nHarris, R. n.d. “The Certain Uncertainty of University Rankings.” RPubs. https://rpubs.com/profrichharris/uni-rankings.\n\n\nLu, Yonggang, and Kevin SS Henning. 2013. “Are Statisticians Cold-Blooded Bosses? A New Perspective on the ‘Old’concept of Statistical Population.” Teaching Statistics 35 (1). Wiley Online Library:66–71."
  },
  {
    "objectID": "week9.html#class",
    "href": "week9.html#class",
    "title": "Foundations of Spatial Data Science",
    "section": "Class",
    "text": "Class\n\nReviewing the Collaborative Agenda\nDiscussion of Readings\nLive Coding"
  },
  {
    "objectID": "week9.html#practical",
    "href": "week9.html#practical",
    "title": "Foundations of Spatial Data Science",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nStandardise delivery by TAs where practicals are TA-led (e.g. FSDS): clearer guidance on how to take students through each week.\n\nClearer articulation of difficulty levels/targets for components within each week’s practicals.\n\nMake links between lecture and practical content explicit; ideally trace a question through the whole process (e.g. “If I wanted to know if and where blue plaques are clustered in London how would I find out?”)\n\n\n\nThe practical can then be downloaded from GitHub.\n\n\n\n\nBunday, B. D. n.d. “A Final Tale or You Can Prove Anything with Figures.” https://www.ucl.ac.uk/~ucahhwi/AFinalTale.pdf.\n\n\nCima, R. n.d. “The Most and Least Diverse Cities in America.” Priceonomics. https://priceonomics.com/the-most-and-least-diverse-cities-in-america/.\n\n\nHarris, R. n.d. “The Certain Uncertainty of University Rankings.” RPubs. https://rpubs.com/profrichharris/uni-rankings.\n\n\nLu, Yonggang, and Kevin SS Henning. 2013. “Are Statisticians Cold-Blooded Bosses? A New Perspective on the ‘Old’concept of Statistical Population.” Teaching Statistics 35 (1). Wiley Online Library:66–71."
  },
  {
    "objectID": "assessments/code.html",
    "href": "assessments/code.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "The entire Reproducible Analysis must be written in Python as a Jupyter Notebook. You are free to draw on concepts and methods covered in both Quantitative Methods and GIS, but must still write the code in Python (e.g. adapting something from R in the GIS module to Python). Please ensure that your submission includes: the module name, your group’s student ids, and the title of your Policy Briefing.\n\n\nWe will assess reproducibility by selecting “Restart Kernel and Run All” using the sds:2022 Docker environment. If you have made use of another Docker image then you must clearly signpost this at the start of your notebook so that we know to select a different image. We will not install libraries ‘by-hand’ in an ad hoc manner order to test the reproducibility of your work.\n\n\n\n\n\n\nReproducibility\n\n\n\nTo ensure reproducibility, markers must be able to select Kernel > Restart Kernel and Run All Cells... and reproduce your entire analysis. This includes downloading and extracting data, cleaning, transformation, clustering… charts, tables, etc. If you need to provide supplementary or partially-processed data then you can provide this via Dropbox, OneDrive (public sharing link), or some other robust cloud solution that will be accessible from the marker’s system.\n\n\nIf you have made use of one or more libraries that are not part of the Docker image then you can install these as part of your Notebook code using ! pip install; however, if you take this approach then you should also ‘place nice’ by checking first to see if the library is already installed using try... except code that you can find on Stack Overflow and elsewhere (you will need to look this up).\n\n\n\nIt is also up to you to ensure that all relevant data are available via a valid URL: for downloading and running. You may host your data anywhere: a GitHub repo or a Dropbox link or some other resource but note that we may not be able to access resources placed on Chinese web servers so please bear this in mind.\n\n\n\n\n\n\nTime-Consuming Code\n\n\n\nIf your analysis has a particularly time-consuming stage (e.g. Named-Entity Recognition or Part-of-Speech tagging) then you can provide partially-processed data: comment out the code up to the point where you have generated the ‘expensive’ data set but leave it in the notebook. That way we can see how you generated the data without it being part of the Restart Kernel and Run All Cells... reproducibility stage."
  },
  {
    "objectID": "assessments/group.html",
    "href": "assessments/group.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "This is a collaborative project that you will undertake in a small group of no more than three students. The project is intended to resemble real-world data science ways of working: you will be part of a small team, you will need to figure out how to work effectively together, you will need to jointly produce an output in which you all have confidence.\n\n\n\n\n\n\nGroup Disputes\n\n\n\nIn the event that there is irreconcilable disagreement within a group, we will use GitHub to determine contributions and inform individual marks. You are therefore strongly advised to use GitHub to manage both the Reproducible Analysis and the Policy Briefing.\n\n\nPlease pay careful attention to the submission process since this is unusually complex thanks to the limitations of Moodle. This assessment consists of two separate submissions:\n\nA Reproducible Analysis (40% of this assessment; i.e. 20% of module grade) consisting of a Jupyter notebook uploaded as a .ipynb file. There is no limit to the amount of code in the Reproducible Analysis. See details.\nA Policy Briefing (60% of this assessment; i.e. 30% of module grade) consisting of a PDF generated from GitHub Markdown. The word limit for the Policy Briefing is 2,500 words. See details."
  },
  {
    "objectID": "assessments/individual.html",
    "href": "assessments/individual.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "This assessment asks you to reflect on the process of ‘doing data science’ as part of a team, since this format is typical of real-world projects. Many companies (e.g. Apple, Google, etc.) employ an Agile project format in which teams undertake a ‘Retrospective’ at the end of a project in order to identify ways to improve how they work in the future. We are not asking you to do this as a group (indeed, it’s an individual reflection), but we hope that this will help you to develop as a programmer, analyst, or data scientist:\n\nWhat do you think went well in the project?\nWhat did not go well?\nHow do you think the other students in your group would evaluate your contribution to each of these outcomes?\nDescribe one event or experience that gave you new insight into ‘doing’ data science; explain how and why this will be useful to you in the future.\n\nYou can write this using whatever tool you like, but you must address these four questions."
  },
  {
    "objectID": "assessments/restrictions.html",
    "href": "assessments/restrictions.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "Unless you are presenting (and citing) a figure from another source as part of your framing, all figures and tables used in the Policy Briefing must be generated by Python code cells included in the Reproducible Analysis notebook. You may not modify or create figures in another application since this undermines the reproducibility of the analysis.\n\n\n\nEach figure or table in the Policy Briefing counts for 250 words, and so students should give careful consideration to the trade-offs involved: more figures may serve to illustrate your points but leave you with much less space to synthesise and present and argument.\nFigures in the Reproducible Analysis do not count towards your word count, but you may not refer to them in the Policy Briefing, which must stand on its own. This is so that you don’t need to go through your reproducible code and delete any/all figures that you produced as part of your research process.\n\n\n\nA figure with A/B elements will count as one figure, but only where the two parts are conceptually related (e.g. before/after; non-spatial/spatial distribution; type 1 and type 2; etc.). Figures may not have more than two elements (i.e. A/B/C); the only exception to this will be the output from PySAL’s LISA analysis library since that is pre-formatted as 3 figures. Similarly, Seaborn’s jointplot will only be considered to be one plot even though it is technically three because the distribution plots in the margin are related to the scatter plot that is the focus of the plot.\nIn principle, a briefing with 10 figures would have no space for any text or interpretation; this choice is deliberate because its purpose is to focus your attention on which charts and tables best-communicate your findings. In practice, using A/B figure layouts then you are looking at up to 20 separate figures before hitting the limit, though you would at this point be producing an infographic and not a briefing."
  },
  {
    "objectID": "assessments/marking.html",
    "href": "assessments/marking.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "The marking scheme for this submission has two parts:\n\nThe Executive Briefing (60% of total mark for this submission) will be assessed as an essay incorporating analytical elements, with consideration given to the language, presentation, and content of the essay as befits a data-led briefing for a busy politician (the Mayor of London) and their policy-makers.\n\nResearch Question and Framing (20% of total mark): the briefing develops a set of research questions and/or research problem within a clearly-defined frame that is relevant to the specified audience.\nResults and Interpretation (30% of total mark): the briefing’s results and interpretation are relevant to the specified audience.\nStructure and Presentation (10% 0f total mark): the briefing is written and presented in a manner that is appropriate to the specified audience.\n\nThe Reproducible Analysis (40% of total mark for this submission) will be assessed on the following criteria:\n\nReproducibility (20% of total mark): the code is portable and robust.\nQuality (20% of total mark): the code and its outputs are efficient and legible.\n\n\nThe evaluation criteria span the skillset expected of a practicing analyst or data scientist: it is not just about writing code, but about selecting, developing, and communicating evidence with your ‘end user’ in mind, as well as demonstrating an understanding of how to give others within your organisation or team confidence in the results that you present.\n\n\n\n\nThis aspect is focussed on your ability to select and frame a research question or problem that is relevant to a specified audience.\n\n\n\n\n\n\n\nMark\nGuidance\n\n\n\n\nDistinction (70+%/A)\nThe research problem has been well-developed and -framed for the specified audience. There is a broad range of background research entailing the use of diverse sources to produce a convincing framing of the topic. The briefing is clearly of compelling interest to the audience.\n\n\nMerit (60-70%/B)\nThe research problem has been developed for the specified audience. There is a range of background research used to produce a relevant framing of the topic. The briefing is clearly of interest to the audience, but makes some assumptions that suggest more limited reasoning about the audience.\n\n\nPass (50-60%/C)\nThe research problem is of limited interest to the specified audience. There is a limited range of background research used to support the framing of the topic. Parts of the briefing are demonstrably of interest to the audience but this is only weakly-communicated or largely implicit.\n\n\nFail (<50%)\nThe research problem is not obviously of interest to the specified audience. There is a very limited range of background research used to support the framing of the topic. Few, if any, efforts are mode to make the briefing relevant to the audience, and the overall framing is weak and/or illogical.\n\n\n\n\n\n\nThis aspect is focussed on your ability to choose and employ the analytical tools covered in this and other modules (you are free to draw on knowledge developed in Quantitative Methods and GIS) to support your investigation of the research question.\n\n\n\n\n\n\n\nMark\nGuidance\n\n\n\n\nDistinction (70+%/A)\nThe analysis is entirely appropriate to the problem. It demonstrates a comprehensive technical understanding of how to select and deploy analytical approaches in support of an analysis. Interpretations of results are very well constructed, clear and focused in relation to the research problem/framing. Maps, figures, and tables are excellent additions to the text and all work together to communicate effectively with the specified audience.\n\n\nMerit (60-70%/B)\nThe analysis is appropriate to the problem. It demonstrates a sound technical understanding of how to select and deploy analytical approaches in support of an analysis, but with some opportunities for additional learning. Interpretations of results are logical and contain a well constructed discussion with linkage to the research problem. Maps, figures, and tables are appropriate, but could be improved upon or may contain minor errors.\n\n\nPass (50-60%/C)\nThe analysis is relevant to the problem but lacks focus and/or clarity. It demonstrates sound technical understanding of individual analytical approaches, but those selected may not be the most relevant to the research problem or could be better-specified. Interpretations of results are largely correct but need to be better-connected to the research problem. Maps, figures, and tables are adequate but more appropriate visualisations could have been implemented or may containt multiple errors that detract from their usefulness.\n\n\nFail (<50%)\nAn attempt at analysis is made but does not address the problem effectively and demonstrates that the material covered in class has not been fully understood. It demonstrates limited technical understanding of individual analytical approaches, and those selected may not address the research problem or may be incorrectly-specified. Interpretations of results may not be correct or may not have a bearing on the research problem. Maps, figures, and tables are of poor quality and do not aid understanding in any way.\n\n\n\n\n\n\nThis aspect focusses on the attention you’ve given to communicating your results in a manner that is appropriate to the target audience:\n\n\n\n\n\n\n\nMark\nGuidance\n\n\n\n\nDistinction (70+%/A)\nThe briefing demonstrates an excellent awareness of its audience, and excellently balances the elements of a model data-led briefing. The linkage between sections is obvious and there is a strong narrative throughout.\n\n\nMerit (60-70%/B)\nThe briefing demonstrates a good awareness of its audience, and there is a clear and logical structure to the data-led briefing. Attempts to link sections as part of an overall narrative are evident, but could be improved by further reflection about the audience and topic.\n\n\nPass (50-60%/C)\nThe report has an obvious structure but demonstrates a limited awareness of its audience. Connections between sections are weakly-articulated and the overall narrative is lost.\n\n\nFail (<50%)\nThe report is weakly structured and demonstrates a very limited awareness of its audience. There is no obvious linkage between sections and little narrative throughout.\n\n\n\nThe models provided should assist in determining whether you will do well in this area.\n\n\n\n\n\n\nThis aspect focusses on the attention you’ve given to ensuring that your analysis can be run on another system without changes being necessary to the code. So this requires thinking about both ‘portability’ and how to ensure that data and libraries are made available to end-users.\n\n\n\n\n\n\n\nMark\nGuidance\n\n\n\n\nDistinction (70%+/A)\nYour notebook runs without errors and you have clearly thought through the ways that your code might fail on another system as well as the processes that could mitigate this.\n\n\nMerit (60-70%/B)\nYour notebook runs without errors.\n\n\nPass (50-60%/C)\nYour notebook runs with minor errors, such as to a path variable or a missed library import from the Docker image.\n\n\nFail (50-%)\nYour notebook does not run without significant edits to the code, such as installing a missing library or a failed download.\n\n\n\nIf you have, for instance, a NLP analysis that would take hours to run then it is acceptable to provide a partially-processed data file (i.e. at the point where the time-consuming analysis is complete) and to ‘comment out’ the cells that generated this data, but you must include the code used to generate this data.\n\n\n\nThis aspect focusses on the attention you’ve given to creating an analysis that is intelligible and well-presented such that another user doesn’t just ‘run the code’ but actually understands and has confidence in the how and why of what you’ve done. This is about making maximal use of the tools at your disposal such that the code is efficient, even elegant, and the outputs are clear and legible.\n\n\n\n\n\n\n\nMark\nGuidance\n\n\n\n\nDistinction (70+%/A)\nThe code is well-commented and each stage of the analysis is organised in a logical manner as part of a coherent and well-structured whole. Another data scientist could understand and use your code with minimal effort demonstrating a comprehensive understanding of the practices taught on the course. The code is efficient, including, for example, the creation of functions where appropriate, the use of iteration to work efficiently with data (loops), and the demonstration of more advanced programming techniques, and clever and creative use of the packages taught on the course. Outputs from the analysis are appropriate, clear, and well-designed.\n\n\nMerit (60-70%/B)\nThe code is well-commented and each stage of the analysis is organised in a logical manner. Another data scientist could understand and use your code with some effort, demonstrating a good understanding of the practices taught on the course. The code is also efficient, including, for example, the creation of functions where appropriate, and good use of the packages taught on the course. Outputs are mostly appropriate even if they rely largely on sensible defaults.\n\n\nPass (50-60%/C)\nThe code is well-commented, but is unstructured in places. Another data scientist could understand and use your code, but it would require a lot of effort on their part, suggesting more limited engagement with the course material. While the code is efficient in places there are often more effective and ‘legible’ ways that could have been used to generate the results, including, for example, repeated code that could have been made into a function, or failing to take advantage of the packages taught in the course. Outputs are adequate for the analysis but more appropriate visualisations could have been implemented.\n\n\nFail (<50%)\nThe code is uncommented/poorly commented and largely unstructured. Another data scientist could not understand and use your code, suggesting almost no engagement with the fundamentals of the course. The code is generally of poor quality, highly inefficient, and riddled with errors. Attempts are made at outputs but they do not add to understanding in any way."
  },
  {
    "objectID": "assessments/audit.html",
    "href": "assessments/audit.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "A Markdown document submitted in Week 6 of teaching (after Reading Week). The word limit for this submission is 1,050 words (including the wording of the questions without the material in square brackets—see the template that I have provided). The data set for which the ‘biography’ will be written is the Inside Airbnb data for London.\nThis assessment will be quite straightforward if you have been doing the readings, and rather challenging if you have not.\nThis is a structured assessment that requires you to answer the following questions:\n\nWho collected the data? Consider the source of the data and its relation to the underlying data generating process.\nWhy did they collect it? Consider the purposes for which the data was collected and how this might shape its structure or content.\nHow was it collected? What was the method by which the data was collected and how might this shape its structure, content, or completeness.\nWhat useful information does it contain? Discuss how the data might support a range of analyses and note any limitations encountered so far in class or in your own investigations.\nTo what extent is the data ‘complete’? Reflecting on your earlier answers, and drawing on what you’ve learned about the data so far in class, to what extent is this data a ‘complete’ picture of Airbnb’s operations in London?\nWhat kinds of analysis would this support? Given the issues identified above, what kinds of analysis would this data support? You do not need to propose a specific analysis and should instead focus on generic classes of analysis.\nWhich of the analyses outlined above are ethical? Discuss the ethics of these classes of analysis with reference to the assigned readings."
  },
  {
    "objectID": "assessments/models.html#models-for-the-policy-briefing",
    "href": "assessments/models.html#models-for-the-policy-briefing",
    "title": "Foundations of Spatial Data Science",
    "section": "Models for the Policy Briefing",
    "text": "Models for the Policy Briefing\nAlthough the following examples are all much longer than permitted under the assessment format, they are exemplary in their communication of the data and key findings in a manner that is clear, straightforward, and well-illustrated:\n\n\n\n\n\n\nTip\n\n\n\nNotice how the format of these is broadly similar but differs from a traditional essay format. So instead of Introduction, Literature, etc. you will see Key Findings, potentially some Recommendations, and two or more sections or chapters in which the evidence is developed in parallel with the background material. This format provides for more flexibility in style and presentation, though you will note that they all refer to a mix of academic and grey literature as well!\n\n\n\nSmith, D.A. (2010), Valuing housing and green spaces: Understanding local amenities, the built environment and house prices in London, GLA Economics; URL.\nTravers, T. Sims, S. and Bosetti, N. (2016), Housing and Inequality in London, Centre for London; URL.\nBivens, J. (2019), The economic costs and benefits of Airbnb, Economic Policy Institute; URL.\nWachsmuth, D., Chaney, D., Kerrigan, D. Shillolo, A. and Basalaev-Binder, R. (2018), The High Cost of Short-Term Rentals in New York City, Urban Politics and Governance research group, McGill University; URL.\nChapple, K. (2009), Mapping Susceptibility to Gentrification: The Early Warning Toolkit, Centre for Community Innovation; URL\n\nThe last of these is bit more ‘academic’ in tone but still intended to be very accessible to a lay-reader (i.e. non-expert)."
  },
  {
    "objectID": "assessments/models.html#possible-topics",
    "href": "assessments/models.html#possible-topics",
    "title": "Foundations of Spatial Data Science",
    "section": "Possible Topics",
    "text": "Possible Topics\nStudents are free to select a narrower or different focus of their briefing; for example, they may wish to develop the evidence either for/against the regulation of listings on Airbnb in London, but the briefing should reference existing policies, where relevant, and may make recommendations based on the analysis undertaken. These are indicative topics and you should feel free to strike out if some other aspect of the topic and data interest you:\n\nImpact of Airbnb on local area rental markets — this would require some assumptions about listings and lettings based on available data but as long as these are clearly stated this would be a strong approach; there are good examples of models used in other cities that it may be possible to draw on, or adapt to, London. You may want to consider things like the type of listing and the issues around the Short- and Long-Term Rental markets.\nImpact of Airbnb on London’s Tourism Economy — this would look at the distribution of London’s tourism venues and, possibly, hotels alongside Airbnb listings in order to evaluate the extent to which tourism ‘dollars’ might be spent in ways that positively impact less tourist-oriented areas if we assume (again, detail the assumptions) that some percentage of a tourist’s dollars are spent locally in an area. Again, there may be models developed elsewhere that could be adapted for the London context.\nOpportunities and Risks arising from Covid-19 — it should/may be possible to assess the impact of Covid-19 on London’s short- and long-term rental markets by looking at entry to/exit from the Airbnb marketplace by comparing more than one snapshot of London data. Again, this will require some reasonable assumptions to be drawn (are all flats withdrawn from Airbnb going back on to the Long-Term Rental Sector?) but these can be documented and justified.\nOpportunities for Place- or Listing-Branding — identifying key terms and features/amenities used to market listings by area and using these to identify opportunities for investment or branding. This would benefit from the use of NLP approaches and, potentially, word embeddings to identify distinctive patterns of word use as well as, potentially, One-Hot encoding to identify specific amenities that appear associated in some way with particular areas.\nThe Challenge of Ghost Hotels — evaluating ways to automatically identify ghost hotels from the InsideAirbnb data and then, potentially, assessing their extent and impact on local areas where they dominate either ‘proper’ hotel provision or other types of listings. You will need to consider the way that Airbnb randomly shuffles listings to prevent exactly this type of application and textual similarity via NLP is an obvious application.\nThe Professionalisation of Airbnb — this could be treated either as a regulatory challenge (is Airbnb not benefiting locals) or an investment opportunity (is this a way to ‘scale’ or develop new service offers for small hosts) depending on your interests. You will need to consider the different types of hosts and evaluate ways of distinguishing between them (e.g. number of listings, spatial extent, etc.).\nImpact Profiles — a geodemographic classification of London neighbourhoods based on how they have, or have not, been impacted by Airbnb. This would require you to think about how to develop a classification/clustering of London neighbourhoods and use data to develop ‘pen portraits’ of each so that policy-makers could better-understand the range of environments in which Airbnb operates and why a 1-size-fits-all regulatory approach may be insufficient. Again, this could be argued from either standpoint or even both simultaneously: these areas are already so heavily impacted that regulation is too little, too late, while these other areas are ‘at risk’."
  },
  {
    "objectID": "assessments/models.html#partial-bibliography",
    "href": "assessments/models.html#partial-bibliography",
    "title": "Foundations of Spatial Data Science",
    "section": "Partial Bibliography",
    "text": "Partial Bibliography\nYou will also want to review the partial bibliography available here; this is by no means complete and you will likely find other relevant work ‘out there’ but you may find it useful for spurring your thinking on what to study and how to study it. You might also want to have a look at guidance for London:\n\nKeyNest (2019), Understanding Airbnb regulations in London, KeyNest; URL\nAirbnb (n.d.), I rent out my home in London. What short-term rental laws apply?, Airbnb; URLi-rent-out-my-home-in-london-what-shortterm-rental-laws-apply\nHostmaker (2018), Important Airbnb regulations and laws you should know about in London, Hostmaker; URL"
  },
  {
    "objectID": "sessions.html",
    "href": "sessions.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "We have ‘flipped’ the classroom for this module so we expect you to come to ‘lecture’ (except in Week 1!) having already watched the assigned videos and completed the assigned readings. You may be called upon to present a short summary of the key points in, and relevance of, an assigned video or reading to the rest of the class.\nThis means that there is a mix of ‘asynchronous’ (work that you do in your own time) and ‘synchronous’ (work that we do during scheduled hours) interaction. Synchronous activities will normally be recorded for review afterwards, but you should bear in the mind the following: 1) we cannot be responsible for equipment failure; 2) we are unable to record practicals and other small-group activities; and 3) a 2-hour video of a group discussion and live coding session will be rather less educational and informative than actually being there.\nIn short, recordings should not be used as a substitute for attendance save in exceptional circumstances.\n\n\nThe nature and amount of preparation will vary from week to week, but may include:\n\nReadings from both academic and non-academic sources.\nRecorded lectures from CASA staff.\nRecorded videos from non-CASA staff.\nShort Moodle quizzes to test your completion of readings and videos.\nPreparing contributions to set tasks (e.g. summaries, Q&A, etc.)\n\nTo get the most value from the module you must do the readings even if we are not specifically referencing them in-class because of time-constraints. In previous years students who did not do the readings often failed the first assessment and struggled with the final assessment, leaving a lot of easy marks on the table. More importantly, we believe that the single most important skill that you can acquire from FSDS is not the ability to code, it’s the ability to critically interrogate data and recognise the strengths and limitations that are relevant to the problem at-hand. You will learn the technical aspects of this in the practicals. You will learn the theoretical dimension from doing the readings.\n\n\n\nThe ‘lecture’ in your timetable will be used for a mix of discussion and ‘live coding’ (eeek!) using the following framework:\n\nWe will review questions and issues arising from the previous week’s practical session and the weekly Padlet. We will use this to prioritise discussion around concepts and readings with which students are struggling or wish to engage further.\nWe will have a ‘live coding’ session following an ‘I do/We do’ format: we will employ concepts covered in the week’s activities, as well as approaches that will be explored further in the practical, to look a real-world data set together using code.\n\n\n\n\nIn order to make use of these materials you will need to download and install the Spatial Data Science computing environment. Details for this are provided in the Appendix.\nPracticals are run in small groups to maximise your ability to ask questions and interact with other students. You will be notified of your group by the Professional Services team; there may be limited opportunities to switch, but the best way would be to swap with another student and then notify us of the arrangment. You may wish to download the week’s Jupyter notebook before the start of class in order to familiarise yourself with the material."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "This page contains all articles cited in the Schedule and a few more that may be useful.\n\n\nAmoore, L. 2019. “Doubt and the Algorithm: On the Partial Accounts of Machine Learning.” Theory, Culture, Society 36 (6):147–69. https://doi.org/https://doi.org/10.1177%2F0263276419851846.\n\n\nArribas-Bel, D., and J. Reades. 2018. “Geography and Computers: Past, Present, and Future.” Geography Compass 0 (0):e12403. https://doi.org/10.1111/gec3.12403.\n\n\nBadger, E., Q. Bui, and R. Gebeloff. 2019. “Neighborhood Is Mostly Black. The Home Buyers Are Mostly White. New York Times.” New York Times. https://www.nytimes.com/interactive/2019/04/27/upshot/diversity-housing-maps-raleigh-gentrification.html.\n\n\nBarron, K., E. Kung, and D. Proserpio. 2018. “The Sharing Economy and Housing Affordability: Evidence from Airbnb.” https://static1.squarespace.com/static/5bb2d447a9ab951efbf6d10a/t/5bea6881562fa7934045a3f0/1542088837594/The+Sharing+Economy+and+Housing+Affordability.pdf.\n\n\nBemt, V. van den, J. Doornbos, L. Meijering, M. Plegt, and N. Theunissen. 2018. “Teaching Ethics When Working with Geocoded Data: A Novel Experiential Learning Approach.” Journal of Geography in Higher Education 42 (2):293–310. https://doi.org/https://doi.org/10.1080/03098265.2018.1436534.\n\n\nBunday, B. D. n.d. “A Final Tale or You Can Prove Anything with Figures.” https://www.ucl.ac.uk/~ucahhwi/AFinalTale.pdf.\n\n\nBurton, I. 1963. “The Quantitative Revolution and Theoretical Geography.” The Canadian Geographer/Le Géographe Canadien 7 (4):151–62. https://doi.org/10.1111/j.1541-0064.1963.tb00796.x.\n\n\nCheng, M., and C. Foley. 2018. “The Sharing Economy and Digital Discrimination: The Case of Airbnb.” International Journal of Hospitality Management 70:95–98. https://doi.org/10.1016/j.ijhm.2017.11.002.\n\n\nCheng, M., and X. Jin. 2018. “What Do Airbnb Users Care about? An Analysis of Online Review Comment.” International Journal of Hospitality Management, 76 (A):58–70. https://doi.org/10.1016/j.ijhm.2018.04.004.\n\n\nCima, R. n.d. “The Most and Least Diverse Cities in America.” Priceonomics. https://priceonomics.com/the-most-and-least-diverse-cities-in-america/.\n\n\nCocola-Gant, A., and A. Gago. 2019. “Airbnb, Buy-to-Let Investment and Tourism-Driven Displacement: A Case Study in Lisbon.” Environment and Planning A: Economy and Space 0 (0):1–18. https://doi.org/https://doi.org/10.1177/0308518X19869012.\n\n\nCox, M., and T. Slee. 2016. “How Airbnb’s Data Hid the Facts in New York City.” Inside Airbnb. http://insideairbnb.com/reports/how-airbnbs-data-hid-the-facts-in-new-york-city.pdf.\n\n\nCrawford, K., and M. Finn. 2015. “The Limits of Crisis Data: Analytical and Ethical Challenges of Using Social and Mobile Data to Understand Disasters.” GeoJournal 80 (4):491–502. https://doi.org/https://doi.org/10.1007/s10708-014-9597-z.\n\n\nD’Ignazio, Catherine, and Lauren F. Klein. 2020. Data Feminism. MIT Press. https://bookbook.pubpub.org/data-feminism.\n\n\nDonoho, D. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4):745–66. https://doi.org/10.1007/978-3-642-23430-9_71.\n\n\nElwood, S., and A. Leszczynski. 2018. “Feminist Digital Geographies.” Gender, Place and Culture 25 (5):629–44. https://doi.org/https://doi.org/10.1080/0966369X.2018.1465396.\n\n\nElwood, S., and M. Wilson. 2017. “Critical GIS Pedagogies Beyond ‘Week 10: Ethics.” International Journal of Geographical Information Science 31 (10):2098–2116. https://doi.org/https://doi.org/10.1080/13658816.2017.1334892.\n\n\nErt, E., A. Fleischer, and N. Magen. 2016. “Trust and Reputation in the Sharing Economy: The Role of Personal Photos in Airbnb.” Tourism Management, 55:62–63. https://doi.org/10.1016/j.tourman.2016.01.013.\n\n\nEtherington, Thomas R. 2016. “Teaching Introductory GIS Programming to Geographers Using an Open Source Python Approach.” Journal of Geography in Higher Education 40 (1). Taylor & Francis:117–30.\n\n\nEugenio-Martin, J. L., J. M. Cazorla-Artiles, and C. Gonzàlez-Martel. 2019. “On the Determinants of Airbnb Location and Its Spatial Distribution.” Tourism Economics 25 (8):1224–24. https://doi.org/https://doi.org/10.1177/1354816618825415.\n\n\nFerreri, Mara, and Romola Sanyal. 2018. “Platform Economies and Urban Planning: Airbnb and Regulated Deregulation in London.” Urban Studies 55 (15):3353–68. https://doi.org/10.1177/0042098017751982.\n\n\nGibbs, C., D. Guttentag, U. Gretzel, J. Morton, and A. Goodwill. 2017. “Pricing in the Sharing Economy: A Hedonic Pricing Model Applied to Airbnb Listings.” Journal of Travel & Tourism Marketing 35 (1):46–56. https://doi.org/https://doi.org/10.1080/10548408.2017.1308292.\n\n\nGurran, N., and P. Phibbs. 2017. “When Tourists Move in: How Should Urban Planners Respond to Airbnb?” Journal of the American Planning Association 83 (1):80–92. https://doi.org/10.1080/01944363.2016.1249011.\n\n\nGutiérrez, J., J. C. Garcı́a-Palomares, G. Romanillos, and M. H. Salas-Olmedo. 2017. “The Eruption of Airbnb in Tourist Cities: Comparing Spatial Patterns of Hotels and Peer-to-Peer Accommodation in Barcelona.” Tourism Management 62:278–91. https://doi.org/10.1016/j.tourman.2017.05.003.\n\n\nGuttentag, Daniel A., and Stephen L. J. Smith. 2017. “Assessing Airbnb as a Disruptive Innovation Relative to Hotels: Substitution and Comparative Performance Expectations.” International Journal of Hospitality Management 64:1–10. https://doi.org/10.1016/j.ijhm.2017.02.003.\n\n\nHarris, J. 2018. “Profiteers Make a Killing on Airbnb - and Erode Communities.” The Guardian. https://www.theguardian.com/commentisfree/2018/feb/12/profiteers-killing-airbnb-erode-communities.\n\n\nHarris, R. n.d. “The Certain Uncertainty of University Rankings.” RPubs. https://rpubs.com/profrichharris/uni-rankings.\n\n\nHorn, K., and M. Merante. 2017. “Is Home Sharing Driving up Rents? Evidence from Airbnb in Boston.” Journal of Housing Economics 38:14–24. https://doi.org/10.1016/j.jhe.2017.08.002.\n\n\nKitchin, R., T. P. Lauriault, and G. McArdie. 2016. “Smart Cities and the Politics of Urban Data.” In Smart Urbanism, edited by McFarlane Marvin Luque-Ayala.\n\n\nLadd, John R. 2020. “Understanding and Using Common Similarity Measures for Text Analysis.” The Programming Historian, no. 9. https://doi.org/10.46430/phen0089.\n\n\nLavin, Matthew J. 2019. “Analyzing Documents with TF-IDF.” The Programming Historian, no. 8. https://doi.org/10.46430/phen0082.\n\n\nLee, D. 2016. “How Airbnb Short-Term Rentals Exacerbate Los Angeles’s Affordable Housing Crisis: Analysis and Policy Recommendations.” Harvard Law & Policy Review 10 (1):229–54. https://doi.org/https://heinonline.org/HOL/Page?handle=hein.journals/harlpolrv10&div=13&g_sent=1.\n\n\nLu, Yonggang, and Kevin SS Henning. 2013. “Are Statisticians Cold-Blooded Bosses? A New Perspective on the ‘Old’concept of Statistical Population.” Teaching Statistics 35 (1). Wiley Online Library:66–71.\n\n\nLutz, C., and G. Newlands. 2018. “Consumer Segmentation Within the Sharing Economy: The Case of Airbnb.” Journal of Business Research 88:187–96. https://doi.org/10.1016/j.jbusres.2018.03.019.\n\n\nMa, X., J. T. Hancock, K. L. Mingjie, and M. Naaman. 2017. “Self-Disclosure and Perceived Trustworthiness of Airbnb Host Profiles.” CSCW’17: Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computation, 2397–2409. https://doi.org/10.1145/2998181.2998269.\n\n\nMuller, C. L., and C. Kidd. 2014. “Debugging Geographers: Teaching Programming to Non-Computer Scientists.” Journal of Geography in Higher Education 38 (2). Taylor & Francis:175–92. https://doi.org/10.1080/03098265.2014.908275.\n\n\nQuattrone, G., A. Greatorex, D. Quercia, L. Capra, and M. Musolesi. 2018. “Analyzing and Predicting the Spatial Penetration of Airbnb in u.s. Cities.” EPJ Data Science 7 (31). https://doi.org/https://doi.org/10.1140/epjds/s13688-018-0156-6.\n\n\nQuattrone, Giovanni, Davide Proserpio, Daniele Quercia, Licia Capra, and Mirco Musolesi. 2016. “Who Benefits from the ‘Sharing’ Economy of Airbnb?” In Proceedings of the 25th International Conference on World Wide Web, 1385–94. WWW ’16. Republic; Canton of Geneva, CHE: International World Wide Web Conferences Steering Committee. https://doi.org/10.1145/2872427.2874815.\n\n\nShabrina, Z., E. Arcaute, and M. Batty. 2019. “Airbnb’s Disruption of the Housing Structure in London.” ArXiv Prepring. University College London. https://arxiv.org/pdf/1903.11205.pdf.\n\n\nShabrina, Z., Y. Zhang, E. Arcaute, and M. Batty. 2017. “Beyond Informality: The Rise of Peer-to-Peer (P2p) Renting.” CASA Working Paper 209. University College London. https://www.ucl.ac.uk/bartlett/casa/case-studies/2017/mar/casa-working-paper-209.\n\n\nShapiro, W., and M. Yavuz. 2017. “Rethinking ’distance’ in New York City.” Medium. https://medium.com/topos-ai/rethinking-distance-in-new-york-city-d17212d24919.\n\n\nSingleton, Alex, and Daniel Arribas-Bel. 2021. “Geographic Data Science.” Geographical Analysis 53 (1):61–75. https://doi.org/10.1111/gean.12194.\n\n\nSthapit, Erose, and Peter Björk. 2019. “Sources of Distrust: Airbnb Guests’ Perspectives.” Tourism Management Perspectives 31:245–53. https://doi.org/https://doi.org/10.1016/j.tmp.2019.05.009.\n\n\nUnwin, David. 1980. “Make Your Practicals Open-Ended.” Journal of Geography in Higher Education 4 (2). Taylor & Francis:39–42. https://doi.org/10.1080/03098268008708772.\n\n\nWachsmuth, D., D. Chaney, D. Kerrigan, A. Shillolo, and R. Basalaev-Binder. 2018. “The High Cost of Short-Term Rentals in New York City.” McGill University. https://www.mcgill.ca/newsroom/files/newsroom/channels/attach/airbnb-report.pdf.\n\n\nWachsmuth, D., and A. Weisler. 2018. “Airbnb and the Rent Gap: Gentrification Through the Sharing Economy.” Environment and Planning A: Economy and Space 50 (6):1147–70. https://doi.org/10.1177/0308518X18778038.\n\n\nWolf, Levi John, Sean Fox, Rich Harris, Ron Johnston, Kelvyn Jones, David Manley, Emmanouil Tranos, and Wenfei Winnie Wang. 2021. “Quantitative Geography III: Future Challenges and Challenging Futures.” Progress in Human Geography 45 (3). SAGE Publications Sage UK: London, England:596–608.\n\n\nZervas, G., D. Proserpio, and J. Byers. 2015. “A First Look at Online Reputation on Airbnb, Where Every Stay Is Above Average.” SSRN. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2554500."
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "Over the years, we have experimented with a range of approaches to setting you up with a programming environment: VirtualBox; Vagrant; Docker; and Anaconda Python directly. Each of these has pros and cons, but after careful consideration we have come to the conclusion that Docker is the most robust way to ensure a consistent experience in which all students end up with the same versions of each library, difficult-to-diagnose hardware/OS issues are minimised, and running/recovery is the most straightforward.\nSome students are unable to run Docker on their Windows machines, in which case Anaconda Python can be used with the configuration file that we provide. However, if your machine runs Docker then you must use Docker: this isolates the programming environment form your computer, ensuring that nothing is clobbered by accident, and guaranteeing that you are working with the same version of every library that we are. Anaconda is only for emergencies.\nTo install the Programming Environment follow the instructions in Practical 1 here (Task 5, but 4 and 3 are recommended)"
  },
  {
    "objectID": "lectures/Presentation.html#getting-up",
    "href": "lectures/Presentation.html#getting-up",
    "title": "Foo",
    "section": "Getting up",
    "text": "Getting up\n\nSpeaker notes go here: - https://quarto.org/docs/presentations/revealjs/ - https://quarto.org/docs/presentations/revealjs/themes.html - https://quarto.org/docs/presentations/revealjs/advanced.html\n\n\nTurn off alarm\nGet out of bed"
  },
  {
    "objectID": "setup/git.html",
    "href": "setup/git.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "Screen Grab from Git and GitHub"
  },
  {
    "objectID": "setup/health.html",
    "href": "setup/health.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "When answering the questions below, ‘your computer’ is the machine on which you plan to do the programming. You will also have some ability to run code remotely on UCL’s JupyterHub or on Google Collab (see No Install), but you will find that limitations in processing power on these cloud platforms can make this a frustrating experience.\n\n\nWe try to support as many different configurations as possible, but there is no programming environment that installs and runs seamlessly on all computers. However, in our experience the students most likely to encounter problems share one or more of the following:\n\nYour computer 8GB or less of RAM.\nYour computer has less than 20GB of free disk space remaining.\n\nRead on below to check what specification you have…\n\n\nYou will need to look up:\n\nHow much RAM does your main computer have? Help for Mac.\nHow much free disk space does your main computer have? Help for Mac.\n\n\n\n\nYou will need to look up:\n\nHow much RAM does your main computer have? Help for Windows.\nHow much free disk space does your main computer have? Help for Windows.\n\n\n\n\nWe’re going to assume that you know what you’re doing. If you want a recommendation, we’d probably go with the latest Ubuntu desktop release.\n\n\n\nIf you are looking for a recommendation as to what to buy:\n\nDon’t worry about getting the fastest chip, get as much RAM as you can. You should aim for 32GB of RAM, but get more if you can afford it.\nDon’t worry about getting the biggest hard drive, get the fastest one you can. You should get a SSD (Solid State Drive), but get a M2 type SSD if you can afford it and it’s available for your system.\nOnly after you’ve sorted this out should you look for the fastest chipset that’s still within your budget.\n\nShould you buy a Windows, Linux, or Apple machine? You should probably stick with whatever you’re familiar with since learning your way around a new Operating System while also learning to code is just raising the bar unnecessarily. Around CASA we use a mix of all three, and you can probably find as many opinions as there are staff members.\n\n\n\n\n\n\nSaving Money\n\n\n\nTwo easy things to do to save money on a new machine are:\n\nUnless there is a specific reason to do so (e.g. getting a system with Apple’s M1/M2 chip), don’t buy the latest machine, buy a model from earlier in the year/the previous year instead and upgrade the RAM and hard drive instead.\nOnly buy the machine when you have access to a student discount. The discount for Apple computers is relatively modest (ca. 10%) compared to ‘back to school’ offers for Windows machines, but as the Brits would say: even 10% is better than a kick in the teeth!\n\n\n\n\n\n\n\nWe try to support as many different configurations as possible, but there is no programming environment that installs and runs seamlessly on all computers. However, in our experience the students most likely to encounter problems share one or more of the following:\n\nYour computer runs Windows 10 Home or older, or\nYour computer runs MacOS 10.13 (High Sierra) or older.\n\nAs long as your computer is running one of the last two major releases of the Operating System you should encounter few issues.\n\n\nYou will need to look up:\n\nWhat Operating System and Version is your main computer running? Help for Mac.\n\n\n\n\nYou will need to look up:\n\nWhat Operating System and Version is your main computer running? Help for Windows.\n\n\n\n\nIf you are using a Mac then your system should have the option to update to the latest version of the MacOS at no charge. If you are unable to update then it is likely that you have an older machine that is not fully supported by the most recent Operating System and, in all probability, you will also encounter issues running the programming environment.\nIf you are using a Windows PC then try to update to either Windows 11 or to Windows 10 Pro as this will ‘unlock’ additional features that are useful for supporting the programming environment. As a student you are likely to qualify for significantly cheaper/free updates, so make sure you do this when you have access to a discount.\n\n\n\n\nNow that you know your computer is ‘up-to-spec’, please ensure that your computer is fully up-to-date with all Operating System and application updates before following any of the other steps in this brief guide to getting started.\n\n\n\n\n\n\nAutomatic Updates\n\n\n\nOnce you have installed the programming environment we strongly recommend that you turn off automatic updates for your computer until the end of the academic year. This is not to say that you should not install security and other updates over the course of the year, but to emphasise that it should be at a time of your choosing. In previous years, students’ computers have automatically updated to a whole new operating system version two nights before an assessment deadline, breaking existing code and causing lots of needless stress."
  },
  {
    "objectID": "setup/docker.html",
    "href": "setup/docker.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "With Docker on Windows there is sometimes an issue when closing down Docker because the files that you created are not saved and/or you cannot see any of the files on your main system. If this happens to you, the please try replacing this part of the Docker command (detailed elsewhere):\n-v \"$WORK_DIR\":/home/jovyan/work\nwith:\n--mount type=bind,source=\"$(pwd)\",target=/home/jovyan/work\nThis should enable you to see any existing files that you have, while also being able to save any files that you create in JupyterLab."
  },
  {
    "objectID": "setup/index.html",
    "href": "setup/index.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "In order to get you started on your data science ‘journey’ you will need to follow the guidance provided on the pages we’ve linked to below. These are divided into three sections:\n\nRequirements: these are the things we’ll need you to do (or try to do) before you get started on trying to set up the programming environment and related services.\nInstallation: these are the things you’ll need to sign up for/install/configure once you know that your computer is up-to-date and ready to have the environment and related services installed.\nNo Install: these provide options for running code without installing anything on your own computer; they represent a compromise in terms of performance but constitute a good ‘backup’ plan.\n\n\n\nBefore trying to do anything else please complete the basic health check, which also includes our recommendations if you are considering buying a new computer when you start your studies. Once you know that your machine and operating system are up-to-date, you should install the basic utilities that will enable you to complete installation of the programming environment. We also provide information about Code Camp which is a self-paced introduction to the fundamentals of programming in Python.\n\n\n\nOnce you’ve ticked off the Requirements, you can start installing the tools that you will use to write and run both code and documentation. You will need set up Git and GitHub in order to manage, share, and version code. To write documentation and comments on code you will be want a Markdown editor and to familiarise yourself with Markdown’s syntax. And, finally, you will need to install the programming environment.\n\n\n\nIf you are unable to get your hands on a machine that meets the basic requirements or on to which you can install the necessary tools, then you should look at the no install options. These are generally cloud-based options and are necessarily a ‘second best’ since limitations imposed by the provider mean that you probably won’t be able to process the full data set with which we’ll be working, but as a stop-gap they are perfectly useable."
  },
  {
    "objectID": "setup/base.html",
    "href": "setup/base.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "In order to access the majority of the features that this module uses, you will need to install several ‘base’ utilities:\n\n\nYou will need to install the following:\n\nThe Xcode Command Line Tools, which can usually be installed directly from the Terminal. If that does not work for you, then you can use the Apple Developer website to download and install manually. Note: this may require you to get a free developer account.\nAlthough not strictly necessary, you’ll eventually want the Homebrew package manager, which can also be installed directly from the Terminal.\n\n\n\n\nYou will need to install the following:\n\nWSL 2, for which you can follow these instructions."
  },
  {
    "objectID": "setup/code_camp.html",
    "href": "setup/code_camp.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "Prior to (re)joining CASA from the Department of Geography at King’s College London, Dr. Mic Ferretti, Dr. Zara Shabrina, Dr. James Millington, and I developed Code Camp.\nCode Camp provides a gentle introduction to the basics of programming in Python. Across ten sessions you will learn about syntax, variables, ‘simple’ data structures such as lists and dictionaries, and about the fundamentals of writing functions for reusable code. The sessions are entirely self-led: you should follow along at your own pace and remember that ‘it’s a marathon, not a sprint’. Don’t cram it all into the last week of summer holidays!\nStudents from previous years have repeatedly said that completing Code Camp helped them to feel more ready for the Foundations of Spatial Data Science module, enabling them to do better on assessments and on the programme as a whole.\nSo please, complete Code Camp by the start of term and, if possible, go through the materials more than once!"
  },
  {
    "objectID": "sessions/index.html",
    "href": "sessions/index.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "We have ‘flipped’ the classroom for this module so we expect you to come to ‘lecture’ (except in Week 1!) having already watched the assigned videos and completed the assigned readings. You may be called upon to present a short summary of the key points in, and relevance of, an assigned video or reading to the rest of the class.\nThis means that there is a mix of ‘asynchronous’ (work that you do in your own time) and ‘synchronous’ (work that we do during scheduled hours) interaction. Synchronous activities will normally be recorded for review afterwards, but you should bear in the mind the following: 1) we cannot be responsible for equipment failure; 2) we are unable to record practicals and other small-group activities; and 3) a 2-hour video of a group discussion and live coding session will be rather less educational and informative than actually being there.\nIn short, recordings should not be used as a substitute for attendance save in exceptional circumstances.\n\n\nThe nature and amount of preparation will vary from week to week, but may include:\n\nReadings from both academic and non-academic sources.\nRecorded lectures from CASA staff.\nRecorded videos from non-CASA staff.\nShort Moodle quizzes to test your completion of readings and videos.\nPreparing contributions to set tasks (e.g. summaries, Q&A, etc.)\n\nTo get the most value from the module you must do the readings even if we are not specifically referencing them in-class because of time-constraints. In previous years students who did not do the readings often failed the first assessment and struggled with the final assessment, leaving a lot of easy marks on the table. More importantly, we believe that the single most important skill that you can acquire from FSDS is not the ability to code, it’s the ability to critically interrogate data and recognise the strengths and limitations that are relevant to the problem at-hand. You will learn the technical aspects of this in the practicals. You will learn the theoretical dimension from doing the readings.\n\n\n\nThe ‘lecture’ in your timetable will be used for a mix of discussion and ‘live coding’ (eeek!) using the following framework:\n\nWe will review questions and issues arising from the previous week’s practical session and the weekly Padlet. We will use this to prioritise discussion around concepts and readings with which students are struggling or wish to engage further.\nWe will have a ‘live coding’ session following an ‘I do/We do’ format: we will employ concepts covered in the week’s activities, as well as approaches that will be explored further in the practical, to look a real-world data set together using code.\n\n\n\n\nIn order to make use of these materials you will need to install the Spatial Data Science programming environment.\nPracticals are run in small groups to maximise your ability to ask questions and interact with other students. You will be notified of your group by the Professional Services team; there may be limited opportunities to switch, but the best way would be to swap with another student and then notify us of the arrangment. You may wish to download the week’s Jupyter notebook before the start of class in order to familiarise yourself with the material."
  },
  {
    "objectID": "sessions/week1.html",
    "href": "sessions/week1.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "Changes before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nMore time on Git/GitHub and pull/push/browsing version history; point to setting quantitative research questions (CASA0007 Week 1) but shift focus on to policy. Explain why there are two assessments and why one has group work with a peer component.\n\nRemove Docker installation practical from FSDS; keep in QM.\n\nJon to create short video on purpose of Docker that can be used in QM and discussed in FSDS (as part of a data science ‘workflow’)\n\nAdd discussion of .gitignore file to GitHub practical (and make sure they add .gz, .csv, and .zip!)\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis week’s learning objectives are:\n\nA basic understanding of the data science ‘pipeline’.\nAn understanding of how data scientists use a wide range of ‘tools’ to do data science.\nA completed installation/configuration of these tools.\n\n\n\nThis week is focussed on getting you set up for the rest of the course in terms of having the requisite software installed and accounts configured so that you can keep track of your work, write code, and track changes. However, you should also see this as preparing the foundation not only for your remaining CASA modules (especially those in Term 2) but also for your post-MSc career: the ability to manage and version code (GitHub); collaborate around a shared codebase (GitHub+Markdown); and produce reproducible code (GitHub+Docker) is integral to modern software development and data science.\nYou should also see this session as connecting to Quantitative Methods Week 1 content on ‘setting quantitative research questions’ since the main assessment will require you to develop a data-led policy briefing. In other words, you’ll need to map current policy on to one or more research questions that can be quantitatively examined using the tools and techniques acquired over the course of the term! While you don’t need to start work on this yet, you should keep it in the back of your mind for when you come across readings/results that you’d like to explore in more detail.\n\n\n\nAlthough none of these activities are compulsory in advance of the first session, getting your computer set up to code does take time and most of these preparatory activites are fairly straightforward… with a few exceptions noted below. If you are able to get these tools installed in advance then you can focus on the taught content in the first two practicals rather than also wrestling with an installation process. This will also give us more time to help you if you discover that you’re one of the unlucky few for whom getting set up is a lot more work!\n\n\n\n\n\n\nTip\n\n\n\nComplete as many of these activities as you can:\n\nGo through the computer health check.\nHave a go at installing the Command Line Tools for your operating system.\nHave a go at installing the programming environment.\n\n\n\nThe last of these is the stage where you’re most likely to encounter problems that will need our assistance, so knowing that you need our help in Week 1 means that you can ask for it much sooner in the practical!\n\n\n\nIn this week’s workshop we will review the module aims, learning outcomes, and expectations with a general introduction to the course.\n\n\n\nSession\nPresentation\n\n\n\n\nGetting Oriented\nSlides\n\n\nTools of the Trade\nSlides\n\n\nWriting Code\nSlides\n\n\n\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\n\nStandardise delivery by TAs where practicals are TA-led (e.g. FSDS): clearer guidance on how to take students through each week.\n\nClearer articulation of difficulty levels/targets for components within each week’s practicals.\n\nMake links between lecture and practical content explicit; ideally trace a question through the whole process (e.g. “If I wanted to know if and where blue plaques are clustered in London how would I find out?”)\n\n\n\n\n\n\nThis week’s practical is focussed on getting you set up with the tools and accounts that you’ll need to across many of the CASA modules in Terms 1 and 2, and familiarising you with ‘how people do data science’. Outside of academia, it’s rare to find a data scientist who works entirely on their own: most code is collaborative, as is most analysis! But collaborating effectively requires tools that: get out of the way of doing ‘stuff’; support teams in negotating conflicts in code; make it easy to share results; and make it easy to ensure that everyone is ‘on the same page’.\n\n\n\n\n\n\nImportant\n\n\n\nThe practical’s learning objectives are:\n\nGet you up and running with coding and collaboration tools.\nProvide you with hands-on experience of using these tools.\nConfigure your programming environment for the rest of the programme.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. From there, move it to your fsds folder and follow the steps to add and push the file to GitHub.\n\n\nIf you have already installed the programming environment then the practical can be downloaded from GitHub. Since there is no code to run this week, you can also view it online in your web browser."
  },
  {
    "objectID": "sessions/week2.html",
    "href": "sessions/week2.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "Changes before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nMore focus on code re-use and simplicity.\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis week’s learning objectives are:\n\nA review of basic Python syntax.\nA review of logical operators in Python.\nA review of basic list operations.\nAn introduction to making use of Git+GitHub.\nAn introdution to make use of the Shell/Terminal.\nAn understanding of how none of this all that new.\n\n\n\nThis week we will be (briefly) reviewing the basics of Python with a view to recapping the more complex aspects of simple data structures. And yes, that’s a bit of an oxymoron, but we’re focussing here on how computers ‘think’ and how that differs from what you might be expecting as an intelligen human being!\nWe will be contextualising all of this within the longer history of the study of geography through computation. I hope to convince you that many of the problems we face today are not new and why that should encourage you to continue to do the readings!\n\n\n\nAs we’re working in a ‘flipped’ environment, you should watch these videos before class so that the ‘live’ workshop can focus on demonstration, discussion, and clarification. This week is very busy because we need to cover off the basics for those of you who were unable to engage with Code Camp, while recapping only the crucial bits for those of you who were able to do so.\nCome to class prepared to present:\n\n\n\nSession\nVideo\nPresentation\nNotes\n\n\n\n\nComputers in Urban Studies\n\nSlides\nNotes\n\n\nPrinciples of Programming\n\nSlides\nNotes\n\n\nPython: the Basics\nVideo\nSlides\nNotes\n\n\nLists\nVideo\nSlides\nNotes\n\n\nIteration\nVideo\nSlides\nNotes\n\n\nThe Command Line\nVideo\nSlides\nNotes\n\n\nGetting Stuck Into Git\nVideo\nSlides\nNotes\n\n\n\n\n\nCome to class prepared to present:\n\n(Burton 1963) DOI\n(Arribas-Bel and Reades 2018) DOI\n\n\n\n\n\nPadlet: [Collaborative Agenda]\nComplete the short Moodle quiz associated with this week’s activities.\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis week’s practical requires you to have completed installation of the programming environment. Make sure you have completed setup of the environment.\n\n\nIn principle, we fully support students who want to do things their own way: running their own Python installation, tweaking Docker… However, one thing to keep in mind is that there are more than 100 students taking this class, so we are also not able to sit down with each person and develop a custom learning environment.\nThe approach that we’ve developed is intended to hide some of the complexity of getting started with managing your own programming environment: we give you full access to the cutting-edge Python libraries and other tools needed to ‘do’ spatial data science, but with Docker only need to install 1 application, download 1 (big) file, and run 1 command. When it works…\nThere are alternatives, such as installing everything using Anaconda Python’s conda tool. But there are more things that can go wrong and they can go wrong in more complex ways. Solving the Anaconda environment can take over an hour before it even starts installing. I’ve had conda fail on me after solving all night!\nSimilarly, we suggested a way to manage your files so that we had a starting point for trying to help you resolve challenges that you might be having with reading/writing files or pushing/pulling files to Git. Imagine what happens if each and every person puts their files in their own place and then every one of the 100 students asks for help with their setup. It will confuse everyone else (especially the people who are already confused) and take up whole practicals!\nSo here’s what we ask: if you know enough to know what to do with an Anaconda YAML file, or can work out how to edit the Dockerfile yourself and build a new image, then by all means knock yourself out! We are not going to tell you that cannot do something, and eventually you will need to learn to stand on your own two feet. But please do not expect us to support you individually if you’ve gone off and done your own thing and now ‘it doesn’t work’. OK? We’ll offer advice (if we can) but only during Office Hours and only if no one else is waiting for help.\nWe hope that sounds fair.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis week’s learning objectives are:\n\n\n\n\n\n\n\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nStandardise delivery by TAs where practicals are TA-led (e.g. FSDS): clearer guidance on how to take students through each week.\n\nClearer articulation of difficulty levels/targets for components within each week’s practicals.\n\nMake links between lecture and practical content explicit; ideally trace a question through the whole process (e.g. “If I wanted to know if and where blue plaques are clustered in London how would I find out?”)\n\n\n\nThe practical can be downloaded from GitHub.\n\n\n\n\n\n\n\nArribas-Bel, D., and J. Reades. 2018. “Geography and Computers: Past, Present, and Future.” Geography Compass 0 (0):e12403. https://doi.org/10.1111/gec3.12403.\n\n\nBurton, I. 1963. “The Quantitative Revolution and Theoretical Geography.” The Canadian Geographer/Le Géographe Canadien 7 (4):151–62. https://doi.org/10.1111/j.1541-0064.1963.tb00796.x."
  },
  {
    "objectID": "sessions/week3.html",
    "href": "sessions/week3.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "Changes before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nMore focus on code re-use and simplicity.\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis week’s Learning Outcomes are:\n\n\n\n\n\n\n\nThis week we start to move beyond Code Camp. So although you should recognise many of the parts that we discuss, you’ll see that we start to put them together in a new way.\n\n\n\n\n\nAll students should complete/revisit Code Camp Notebooks 8–11.\nNote: there is an issue with the GeoJSON tasks in Notebooks 8 and 9. We can discuss in the Class.\n\n\n\nCome to class prepared to present:\n\n\n\nSession\nVideo\nPresentation\nNotes\n\n\n\n\nDictionaries\nVideo\nSlides\nNotes\n\n\nLOLs\nVideo\nNotes\nNotes\n\n\nDOLs to Data\nVideo\nSlides\nNotes\n\n\nFunctions\nVideo\nSlides\nNotes\n\n\nPackages\nVideo\nSlides\nNotes\n\n\n\n\n\n\nCome to class prepared to present:\n\n(Etherington 2016) DOI\n(Donoho 2017) DOI\n(Unwin 1980) DOI\n\n\n\n\n\nPadlet: [Collaborative Agenda]\nComplete the short Moodle quiz associated with this week’s activities.\n\n\n\n\n\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nStandardise delivery by TAs where practicals are TA-led (e.g. FSDS): clearer guidance on how to take students through each week.\n\nClearer articulation of difficulty levels/targets for components within each week’s practicals.\n\nMake links between lecture and practical content explicit; ideally trace a question through the whole process (e.g. “If I wanted to know if and where blue plaques are clustered in London how would I find out?”)\n\n\n\nThe practical can be downloaded from GitHub.\n\n\n\n\n\n\n\nDonoho, D. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4):745–66. https://doi.org/10.1007/978-3-642-23430-9_71.\n\n\nEtherington, Thomas R. 2016. “Teaching Introductory GIS Programming to Geographers Using an Open Source Python Approach.” Journal of Geography in Higher Education 40 (1). Taylor & Francis:117–30.\n\n\nUnwin, David. 1980. “Make Your Practicals Open-Ended.” Journal of Geography in Higher Education 4 (2). Taylor & Francis:39–42. https://doi.org/10.1080/03098268008708772."
  },
  {
    "objectID": "sessions/week6.html",
    "href": "sessions/week6.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "Changes before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nLink back to Week 5 CASA0005 (Mapping) and Weeks 1–3 CASA0005 (Spatial Data)\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis week’s Learning Outcomes are:\n\n\n\n\n\n\n\n\n\n\n\n\nCome to class prepared to present:\n\n\n\nSession\nVideo\nPresentation\nNotes\n\n\n\n\nMapping\nVideo\nSlides\nNotes\n\n\nGeoPandas\nVideo\nSlides\nNotes\n\n\nEDA\nVideo\nNotes\nNotes\n\n\nESDA\nVideo\nSlides\nNotes\n\n\n\n\n\n\nCome to class prepared to present:\n\n(D’Ignazio and Klein 2020, chap. 6) Pre-review URL, The Numbers Don’t Speak for Themselves.\n(Elwood and Wilson 2017) DOI\n\n\n\n\n\nPadlet: [Collaborative Agenda]\nComplete the short Moodle quiz associated with this week’s activities.\n\n\n\n\n\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nStandardise delivery by TAs where practicals are TA-led (e.g. FSDS): clearer guidance on how to take students through each week.\n\nClearer articulation of difficulty levels/targets for components within each week’s practicals.\n\nMake links between lecture and practical content explicit; ideally trace a question through the whole process (e.g. “If I wanted to know if and where blue plaques are clustered in London how would I find out?”)\n\n\n\nThe practical can be downloaded from GitHub.\n\n\n\n\n\n\n\nD’Ignazio, Catherine, and Lauren F. Klein. 2020. Data Feminism. MIT Press. https://bookbook.pubpub.org/data-feminism.\n\n\nElwood, S., and M. Wilson. 2017. “Critical GIS Pedagogies Beyond ‘Week 10: Ethics.” International Journal of Geographical Information Science 31 (10):2098–2116. https://doi.org/https://doi.org/10.1080/13658816.2017.1334892."
  },
  {
    "objectID": "sessions/week4.html",
    "href": "sessions/week4.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "Changes before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nMore focus on code re-use and simplicity.\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis week’s Learning Outcomes are:\n\n\n\n\n\n\n\nThis week deals with ‘objects’ and ‘classes’, which is fundamental to mastering the Python programming language: in Python, everything is an object, you just didn’t need to know it until now. Understanding how classes and objects work is essential to using Python effectively, but it will also make you a better programmer in any language because it will help you to think about how data and code work together to achieve your goals.\n\n\n\n\n\nCome to class prepared to present:\n\n\n\n\n\n\nFocus\n\n\n\n\n\n\n\n\n\nSession\nVideo\nPresentation\nNotes\n\n\n\n\nMethods\nVideo\nSlides\nNotes\n\n\nClasses\nVideo\nSlides\nNotes\n\n\nDesign\nVideo\nSlides\nNotes\n\n\nExceptions\nVideo\nSlides\nNotes\n\n\n\n\n\n\nCome to class prepared to present:\n\n(Cox and Slee 2016) PDF\n(D’Ignazio and Klein 2020, chap. 5) Pre-review URL: Unicorns, Janitors, Ninjas, Wizards, and Rock Stars\n\n\n\n\n\nPadlet: [Collaborative Agenda]\nComplete the early feedback questionnaire please!\nComplete the short quiz\n\n\n\n\n\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nStandardise delivery by TAs where practicals are TA-led (e.g. FSDS): clearer guidance on how to take students through each week.\n\nClearer articulation of difficulty levels/targets for components within each week’s practicals.\n\nMake links between lecture and practical content explicit; ideally trace a question through the whole process (e.g. “If I wanted to know if and where blue plaques are clustered in London how would I find out?”“)\n\n\n\nThe practical can be downloaded from GitHub.\n\n\n\n\n\n\n\nCox, M., and T. Slee. 2016. “How Airbnb’s Data Hid the Facts in New York City.” Inside Airbnb. http://insideairbnb.com/reports/how-airbnbs-data-hid-the-facts-in-new-york-city.pdf.\n\n\nD’Ignazio, Catherine, and Lauren F. Klein. 2020. Data Feminism. MIT Press. https://bookbook.pubpub.org/data-feminism."
  },
  {
    "objectID": "sessions/week5.html",
    "href": "sessions/week5.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "Changes before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nHighlight differences from CASA0007; Airbnb data from CASA0005\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis week’s Learning Outcomes are:\n\n\n\n\n\n\n\n\n\n\n\n\nCome to class prepared to present:\n\n\n\nSession\nVideo\nPresentation\nNotes\n\n\n\n\nLogic\nVideo\nSlides\nNotes\n\n\nRandomness\nVideo\nSlides\nNotes\n\n\nData\nVideo\nSlides\nNotes\n\n\nPandas\nVideo\nSlides\nNotes\n\n\n\n\n\n\nCome to class prepared to present:\n\n(D’Ignazio and Klein 2020, chap. 4) Pre-review URL What Gets Counted Counts\n(Wachsmuth and Weisler 2018) DOI\n(Harris 2018) URL\n\n\n\n\n\nPadlet: [Collaborative Agenda]\nShort Moodle quiz\n\n\n\n\n\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nStandardise delivery by TAs where practicals are TA-led (e.g. FSDS): clearer guidance on how to take students through each week.\n\nClearer articulation of difficulty levels/targets for components within each week’s practicals.\n\nMake links between lecture and practical content explicit; ideally trace a question through the whole process (e.g. “If I wanted to know if and where blue plaques are clustered in London how would I find out?”)\n\n\n\nThe practical can be downloaded from GitHub.\n\n\n\n\n\n\n\nD’Ignazio, Catherine, and Lauren F. Klein. 2020. Data Feminism. MIT Press. https://bookbook.pubpub.org/data-feminism.\n\n\nHarris, J. 2018. “Profiteers Make a Killing on Airbnb - and Erode Communities.” The Guardian. https://www.theguardian.com/commentisfree/2018/feb/12/profiteers-killing-airbnb-erode-communities.\n\n\nWachsmuth, D., and A. Weisler. 2018. “Airbnb and the Rent Gap: Gentrification Through the Sharing Economy.” Environment and Planning A: Economy and Space 50 (6):1147–70. https://doi.org/10.1177/0308518X18778038."
  },
  {
    "objectID": "sessions/week7.html",
    "href": "sessions/week7.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "Changes before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nLink to regex/working with strings in Week 2 CASA0005\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis week’s Learning Outcomes are:\n\n\n\n\n\n\n\n\n\n\n\n\nCOme to class prepared to present:\n\n\n\nSession\nVideo\nPresentation\nNotes\n\n\n\n\nNotebooks as Documents\nVideo\nSlides\nNotes\n\n\nPatterns in Text\nVideo\nSlides\nNotes\n\n\nCleaning Text\nVideo\nSlides\nNotes\n\n\nAnalysing Text\nVideo\nSlides\nNotes\n\n\n\n\n\n\nCome to class prepared to present:\n\n(Ladd 2020) URL\n(Lavin 2019) URL\n\n\n\n\n\nPadlet: [Collaborative Agenda]\nComplete the short Moodle quiz associated with this week’s activities.\n\n\n\n\n\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nStandardise delivery by TAs where practicals are TA-led (e.g. FSDS): clearer guidance on how to take students through each week.\n\nClearer articulation of difficulty levels/targets for components within each week’s practicals.\n\nMake links between lecture and practical content explicit; ideally trace a question through the whole process (e.g. “If I wanted to know if and where blue plaques are clustered in London how would I find out?”)\n\n\n\nThe practical can be downloaded from GitHub.\n\n\n\n\n\n\n\nLadd, John R. 2020. “Understanding and Using Common Similarity Measures for Text Analysis.” The Programming Historian, no. 9. https://doi.org/10.46430/phen0089.\n\n\nLavin, Matthew J. 2019. “Analyzing Documents with TF-IDF.” The Programming Historian, no. 8. https://doi.org/10.46430/phen0082."
  },
  {
    "objectID": "sessions/week8.html",
    "href": "sessions/week8.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "Changes before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nLink to clustering (Week 6 GIS & QM)\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis week’s Learning Outcomes are:\n\n\n\n\n\n\n\n\n\n\n\n\nCome to class prepared to present:\n\n\n\nSession\nVideo\nPresentation\nNotes\n\n\n\n\nThe Data Space\nVideo\nSlides\nNotes\n\n\nTransformation\nVideo\nSlides\nNotes\n\n\nDimensionality\nVideo\nSlides\nNotes\n\n\n\n\n\n\nCome to class prepared to present:\n\n(Bunday n.d.) URL\n(Harris, n.d.) URL\n(Cima, n.d.) URL (PDF with Figures)\n(Lu and Henning 2013) DOI\n\n\n\n\n\nPadlet: [Collaborative Agenda]\nComplete the short Moodle quiz associated with this week’s activities.\n\n\n\n\n\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nStandardise delivery by TAs where practicals are TA-led (e.g. FSDS): clearer guidance on how to take students through each week.\n\nClearer articulation of difficulty levels/targets for components within each week’s practicals.\n\nMake links between lecture and practical content explicit; ideally trace a question through the whole process (e.g. “If I wanted to know if and where blue plaques are clustered in London how would I find out?”)\n\n\n\nThe practical can be downloaded from GitHub.\n\n\n\n\n\n\n\nBunday, B. D. n.d. “A Final Tale or You Can Prove Anything with Figures.” https://www.ucl.ac.uk/~ucahhwi/AFinalTale.pdf.\n\n\nCima, R. n.d. “The Most and Least Diverse Cities in America.” Priceonomics. https://priceonomics.com/the-most-and-least-diverse-cities-in-america/.\n\n\nHarris, R. n.d. “The Certain Uncertainty of University Rankings.” RPubs. https://rpubs.com/profrichharris/uni-rankings.\n\n\nLu, Yonggang, and Kevin SS Henning. 2013. “Are Statisticians Cold-Blooded Bosses? A New Perspective on the ‘Old’concept of Statistical Population.” Teaching Statistics 35 (1). Wiley Online Library:66–71."
  },
  {
    "objectID": "sessions/week9.html",
    "href": "sessions/week9.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "Changes before 22/23\n\n\n\n\n\nAdd log-scale option, data joins (link to CASA0005)\n\nAdd Learning Outcomes for each week\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis week’s Learning Outcomes are:\n\n\n\n\n\n\n\n\n\n\n\n\nCome to class prepared to present:\n\n\n\nSession\nVideo\nPresentation\nNotes\n\n\n\n\nLinking Data\nVideo\nSlides\nNotes\n\n\nLinking Spatial Data\nVideo\nSlides\nNotes\n\n\nGrouping Data\nVideo\nSlides\nNotes\n\n\nData Visualisation\nVideo\nSlides\nNotes\n\n\n\n\n\n\nCome to class prepared to present:\n\n(D’Ignazio and Klein 2020, chap. 3) Pre-review URL, On Rational, Scientific, Objective Viewpoints from Mythical, Imaginary, Impossible Standpoints in Data Feminism.\n(Badger, Bui, and Gebeloff 2019) URL\n\n\n\n\n\nPadlet: [Collaborative Agenda]\nComplete the short Moodle quiz associated with this week’s activities.\n\n\n\n\n\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nStandardise delivery by TAs where practicals are TA-led (e.g. FSDS): clearer guidance on how to take students through each week.\n\nClearer articulation of difficulty levels/targets for components within each week’s practicals.\n\nMake links between lecture and practical content explicit; ideally trace a question through the whole process (e.g. “If I wanted to know if and where blue plaques are clustered in London how would I find out?”)\n\n\n\nThe practical can be downloaded from GitHub.\n\n\n\n\n\n\n\nBadger, E., Q. Bui, and R. Gebeloff. 2019. “Neighborhood Is Mostly Black. The Home Buyers Are Mostly White. New York Times.” New York Times. https://www.nytimes.com/interactive/2019/04/27/upshot/diversity-housing-maps-raleigh-gentrification.html.\n\n\nD’Ignazio, Catherine, and Lauren F. Klein. 2020. Data Feminism. MIT Press. https://bookbook.pubpub.org/data-feminism."
  },
  {
    "objectID": "sessions/week10.html",
    "href": "sessions/week10.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "Changes before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nPoint to clustering code and show DBSCAN, k-Means, etc. as applied to the Airbnb data.\n\nStress different view of clustering as part of a pipeline, not an absolute “just k-means it”; link to thinking about paradigms in CASA0001;\n\nthinking about proxies (why would you find ppsqm more/less useful than price? what are you really measuring?\n\nConnects to both CASA0001 and CASA0007 and CASA0005.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis week’s Learning Outcomes are:\n\n\n\n\n\n\n\n\n\n\n\n\nCome to class prepared to briefly present:\n\n\n\nSession\nVideo\nPresentation\nNotes\n\n\n\n\nClassification\nVideo\nSlides\nNotes\n\n\nClustering\nVideo\nSlides\nNotes\n\n\n\n\n\n\nCome to class prepared to briefly present:\n\n(Shapiro and Yavuz 2017) URL\n(Wolf et al. 2021) DOI\n(Singleton and Arribas-Bel 2021) DOI\n\nYou may also want to look at the following reports or profiles:\n\nGeospatial Skills Report\nAAG Profile of Nicolas Saravia\n\n\n\n\n\n\nPadlet: [Collaborative Agenda]\nComplete the short Moodle quiz associated with this week’s activities.\n\n\n\n\n\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nStandardise delivery by TAs where practicals are TA-led (e.g. FSDS): clearer guidance on how to take students through each week.\n\nClearer articulation of difficulty levels/targets for components within each week’s practicals.\n\nMake links between lecture and practical content explicit; ideally trace a question through the whole process (e.g. “If I wanted to know if and where blue plaques are clustered in London how would I find out?”)\n\n\n\nThe practical can be downloaded from GitHub.\n\n\n\n\n\n\n\nShapiro, W., and M. Yavuz. 2017. “Rethinking ’distance’ in New York City.” Medium. https://medium.com/topos-ai/rethinking-distance-in-new-york-city-d17212d24919.\n\n\nSingleton, Alex, and Daniel Arribas-Bel. 2021. “Geographic Data Science.” Geographical Analysis 53 (1):61–75. https://doi.org/10.1111/gean.12194.\n\n\nWolf, Levi John, Sean Fox, Rich Harris, Ron Johnston, Kelvyn Jones, David Manley, Emmanouil Tranos, and Wenfei Winnie Wang. 2021. “Quantitative Geography III: Future Challenges and Challenging Futures.” Progress in Human Geography 45 (3). SAGE Publications Sage UK: London, England:596–608."
  },
  {
    "objectID": "sessions/week11.html",
    "href": "sessions/week11.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "Changes before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nPoint to clustering code and show DBSCAN, k-Means, etc. as applied to the Airbnb data.\n\nStress different view of clustering as part of a pipeline, not an absolute “just k-means it”; link to thinking about paradigms in CASA0001;\n\nthinking about proxies (why would you find ppsqm more/less useful than price? what are you really measuring?\n\nConnects to both CASA0001 and CASA0007 and CASA0005.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis week’s Learning Outcomes are:\n\n\n\n\n\n\n\n\n\n\n\n\nCome to class prepared to briefly present:\n\n\n\nVideo\nPresentation\n\n\n\n\nClassification\nSlides\n\n\nClustering\nSlides\n\n\n\n\n\n\nCome to class prepared to briefly present:\n\n(Shapiro and Yavuz 2017) URL\n(Wolf et al. 2021) DOI\n(Singleton and Arribas-Bel 2021) DOI\n\n\n\n\n\nPadlet: [Collaborative Agenda]\nComplete the short Moodle quiz associated with this week’s activities.\n\n\n\n\n\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nStandardise delivery by TAs where practicals are TA-led (e.g. FSDS): clearer guidance on how to take students through each week.\n\nClearer articulation of difficulty levels/targets for components within each week’s practicals.\n\nMake links between lecture and practical content explicit; ideally trace a question through the whole process (e.g. “If I wanted to know if and where blue plaques are clustered in London how would I find out?”)\n\n\n\nThe practical can be downloaded from GitHub.\n\n\n\n\n\n\n\nShapiro, W., and M. Yavuz. 2017. “Rethinking ’distance’ in New York City.” Medium. https://medium.com/topos-ai/rethinking-distance-in-new-york-city-d17212d24919.\n\n\nSingleton, Alex, and Daniel Arribas-Bel. 2021. “Geographic Data Science.” Geographical Analysis 53 (1):61–75. https://doi.org/10.1111/gean.12194.\n\n\nWolf, Levi John, Sean Fox, Rich Harris, Ron Johnston, Kelvyn Jones, David Manley, Emmanouil Tranos, and Wenfei Winnie Wang. 2021. “Quantitative Geography III: Future Challenges and Challenging Futures.” Progress in Human Geography 45 (3). SAGE Publications Sage UK: London, England:596–608."
  },
  {
    "objectID": "lectures/3.4-Functions.html#lets-get-lazy",
    "href": "lectures/3.4-Functions.html#lets-get-lazy",
    "title": "Functions",
    "section": "Let’s Get Lazy!",
    "text": "Let’s Get Lazy!\n\nWhy do any work you don’t need to?\nFunctions allow us to  code!\nBonus: they can make our code more legible."
  },
  {
    "objectID": "lectures/3.4-Functions.html#in-action",
    "href": "lectures/3.4-Functions.html#in-action",
    "title": "Functions",
    "section": "In Action!",
    "text": "In Action!\ndef calc_mean(numbers):\n  total = 0.0\n  count = len(numbers)\n  for i in numbers:\n    total += i\n  return total/count\n\ndata = [1,25,-4,14,7,9]\nprint(calc_mean(data))\n> 8.666666666666666\ndata2 = [200000,2500000,-4,1400000,70,900000]\nprint(calc_mean(data2))\n> 833344.3333333334"
  },
  {
    "objectID": "lectures/3.4-Functions.html#any-time-you-type-the-same-code-more-than-twice-consider-a-function",
    "href": "lectures/3.4-Functions.html#any-time-you-type-the-same-code-more-than-twice-consider-a-function",
    "title": "Functions",
    "section": "Any time you type the same code more than twice… consider a function!",
    "text": "Any time you type the same code more than twice… consider a function!"
  },
  {
    "objectID": "lectures/3.4-Functions.html#resources",
    "href": "lectures/3.4-Functions.html#resources",
    "title": "Functions",
    "section": "Resources",
    "text": "Resources\n\nWhat is a function?\nPython functions\nBuilt-in functions\nDefine your own functions\nTypes of functions\nDefining a function\nFunction arguments\nArgument lists\nKeyword arguments\nReturn values\nDecorators\nVariable Scopes\nRobust Python with Type Hints"
  },
  {
    "objectID": "lectures/5.3-Files.html#mapping-data-types",
    "href": "lectures/5.3-Files.html#mapping-data-types",
    "title": "File Formats",
    "section": "‘Mapping’ Data Types",
    "text": "‘Mapping’ Data Types\nYou will often see the term ‘mapping’ used in connection to data that is not spatial, what do they mean? A map is the term used in some programming languages for a dict! So it’s about key : value pairs again.\nHere’s a mapping\n\n\n\n\n\n\n\nInput (e.g. Excel)\nOutput (e.g. Python)\n\n\n\n\nNULL, N/A, “”\nNone or np.nan\n\n\n0..n\nint\n\n\n0.00…n\nfloat\n\n\nTrue/False, Y/N, 1/0\nbool\n\n\nR, G, B (etc.)\nint or str (technically a set, but hard to use with data sets)\n\n\n‘Jon Reades’, ‘Huanfa Chen’, etc.\nstr\n\n\n‘3-FEB-2020’, ‘10/25/20’, etc.\ndatetime module (date, datetime or time)\n\n\n\n\nThese would be a mapping of variables between two formats. We talk of mapping any time we are taking inputs from one data set/format/data structure as a lookup for use with another data set/format/data structure.\nHave a think about how you can use an int to represent nominal data. There are two ways: one of which will be familiar to students who have taken a stats class (with regression) and one of which is more intuitive to ‘normal’ users…"
  },
  {
    "objectID": "lectures/5.3-Files.html#why-this-isnt-easy-part-2",
    "href": "lectures/5.3-Files.html#why-this-isnt-easy-part-2",
    "title": "File Formats",
    "section": "Why This Isn’t Easy (Part 2)",
    "text": "Why This Isn’t Easy (Part 2)\nThings That Can Go Wrong…\nA selection of real issues I’ve seen in my life:\n\nTruncation: server ran out of diskspace or memory, or a file transfer was interrupted.\nTranslation: headers don’t line up with data.\nSwapping: column order differs from spec.\nIncompleteness: range of real values differs from spec.\nCorruption: field delimitters included in field values.\nErrors: data entry errors resulted in incorrect values or the spec is downright wrong.\nIrrelevance: fields that simply aren’t relevant to your analysis.\n\nThese will generally require you to engage with columns and rows (via sampling) on an individual level."
  },
  {
    "objectID": "lectures/5.3-Files.html#resources",
    "href": "lectures/5.3-Files.html#resources",
    "title": "File Formats",
    "section": "Resources",
    "text": "Resources\n\nReading and writing files\nWorking with OS path utilities\nFiles and file writing\nUsing file system shell methods\nOpening files\nText vs. binary mode\nText files\npetl"
  },
  {
    "objectID": "lectures/5.3-Files.html#thank-you",
    "href": "lectures/5.3-Files.html#thank-you",
    "title": "File Formats",
    "section": "Thank you!",
    "text": "Thank you!"
  },
  {
    "objectID": "setup/markdown.html",
    "href": "setup/markdown.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "Not all of these are still free.↩︎"
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#basic-principles",
    "href": "lectures/1.2-Tools_of_the_Trade.html#basic-principles",
    "title": "Tools of the Trade",
    "section": "Basic Principles",
    "text": "Basic Principles\n\nSoftware should be free (as far as practicable).\nSoftware should be open (as far as practicable).\nSoftware should run on all platforms.\nSoftware should reflect what you will encounter in the ‘real world’."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#tools-to-make-your-life-easier",
    "href": "lectures/1.2-Tools_of_the_Trade.html#tools-to-make-your-life-easier",
    "title": "Tools of the Trade",
    "section": "Tools to Make Your Life Easier",
    "text": "Tools to Make Your Life Easier\n\nDropbox: keep your stuff backed up in the cloud.\nSlack: get help (or just tips and tricks) from peers and staff\nDocker/Vagrant/VirtualBox: virtualisation platforms to ensure you don’t ‘hose’ your computer.\nPython: how we do ‘data science’.\nGitHub: manage your code, your data, even your essays/reports.\nMarkdown: focus on the right things while you write and treat your essays like code!"
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#dropbox",
    "href": "lectures/1.2-Tools_of_the_Trade.html#dropbox",
    "title": "Tools of the Trade",
    "section": "Dropbox",
    "text": "Dropbox\n\nDropbox is a ‘cloud-based file synchronisation tool’: files placed in the special Dropbox folder are automatically uploaded to their servers, and automatically downloaded to any other computer on which you have set up Dropbox. Changes are also synchronised every time you save the file."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#slack",
    "href": "lectures/1.2-Tools_of_the_Trade.html#slack",
    "title": "Tools of the Trade",
    "section": "Slack",
    "text": "Slack\n\nSlack is a “messaging app for teams” that is designed to reduce email, organise conversations & topics of discussion, and pull in relevant data from a host of other services in a flexible, fully-searchable way."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#anaconda-python",
    "href": "lectures/1.2-Tools_of_the_Trade.html#anaconda-python",
    "title": "Tools of the Trade",
    "section": "Anaconda Python",
    "text": "Anaconda Python\n\nAnaconda Python is a ‘flavour’ of Python that comes packaged with useful tools for configuring and management. If virtualisation is too resource-intensive for your computer (e.g. because it’s older or doesn’t have enough RAM) then installing Python directly is the next-best option."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#recap",
    "href": "lectures/1.2-Tools_of_the_Trade.html#recap",
    "title": "Tools of the Trade",
    "section": "Recap",
    "text": "Recap\n\nWith Anaconda Python we have a tool for assembling coding environments ‘easily’ from configuration files.\nWith Docker we have a way to create a coding environment that is isolated from the computer and highly portable across machiens.\nWith Dropbox we have a place to store, backup, and share files (size limits apply).\nWith Slack we have a place to ask for/provide help."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#resources",
    "href": "lectures/1.2-Tools_of_the_Trade.html#resources",
    "title": "Tools of the Trade",
    "section": "Resources",
    "text": "Resources\n\nWhat is Python?\nWhy Python?\nPython is eating the world\nWhat can you do with Python?\nStack Overflow\nProgramming Foundations: Fundamentals\nVersion Control with Git\nSetting up and managing your GitHub user account\nPersonal Access Tokens on Git\nGit Cheat Sheet"
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#thank-you",
    "href": "lectures/1.2-Tools_of_the_Trade.html#thank-you",
    "title": "Tools of the Trade",
    "section": "Thank you!",
    "text": "Thank you!"
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#thank-you",
    "href": "lectures/1.1-Getting_Oriented.html#thank-you",
    "title": "Getting Oriented",
    "section": "Thank you!",
    "text": "Thank you!"
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#key-information-to-get-you-started",
    "href": "lectures/1.1-Getting_Oriented.html#key-information-to-get-you-started",
    "title": "Getting Oriented",
    "section": "Key information to get you started…",
    "text": "Key information to get you started…\n\nThis presentation contains key contact and delivery information about the module."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#one-more-thing",
    "href": "lectures/1.1-Getting_Oriented.html#one-more-thing",
    "title": "Getting Oriented",
    "section": "One More Thing…",
    "text": "One More Thing…\nYou will get things wrong. I will get things wrong.\nWe will assume that you are trying your best. Please assume the same about us!\nIt’s going to be messy, but I’m really excited about it!"
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#learn-from-your-mistakes",
    "href": "lectures/1.1-Getting_Oriented.html#learn-from-your-mistakes",
    "title": "Getting Oriented",
    "section": "Learn from Your Mistakes",
    "text": "Learn from Your Mistakes"
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#how-we-are-running-things",
    "href": "lectures/1.1-Getting_Oriented.html#how-we-are-running-things",
    "title": "Getting Oriented",
    "section": "How We Are Running Things…",
    "text": "How We Are Running Things…\n\nPreparation: readings, pre-recorded lectures, contributions to a weekly ‘Padlet’\nClasses: summarising readings and lectures; discussing questions and issues arising from the previous week’s practical. Your input will guide the class! The class will also involve ‘live coding’ in an I do/We do format.\nPractical: working through a weekly ‘programming notebook’ in a small group with support from your PGTA."
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#thank-you",
    "href": "lectures/1.3-Writing_Code.html#thank-you",
    "title": "Writing Code",
    "section": "Thank you!",
    "text": "Thank you!"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#dont-just-start-coding",
    "href": "lectures/1.3-Writing_Code.html#dont-just-start-coding",
    "title": "Writing Code",
    "section": "Don’t Just Start Coding…",
    "text": "Don’t Just Start Coding…\nThere are three parts to writing code:\n\nManaging it\nDocumenting it\nWriting it\n\nNotice that writing comes last."
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#recap",
    "href": "lectures/1.3-Writing_Code.html#recap",
    "title": "Writing Code",
    "section": "Recap",
    "text": "Recap\n\nWith Git + GitHub we have a way to ‘version’ text, code, images, etc. and distribute them online via both a browser and the command line.\nWith Markdown we have a way to easily produce content (essays, documentation, web sites, presentations) and convert or distribute it via both a browser and the commenad line.\nWith JupyterLab we have a way to write code and documentation in an ‘woven’ manner using Markdown and GitHub."
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#managing-code",
    "href": "lectures/1.3-Writing_Code.html#managing-code",
    "title": "Writing Code",
    "section": "Managing Code",
    "text": "Managing Code\n\nWhy can’t we write code the same way that we write an essay?"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#jupyterlab-python",
    "href": "lectures/1.3-Writing_Code.html#jupyterlab-python",
    "title": "Writing Code",
    "section": "JupyterLab + Python",
    "text": "JupyterLab + Python"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#jupyterlab-uses-markdown",
    "href": "lectures/1.3-Writing_Code.html#jupyterlab-uses-markdown",
    "title": "Writing Code",
    "section": "JupyterLab Uses Markdown",
    "text": "JupyterLab Uses Markdown"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#tie-these-together",
    "href": "lectures/1.3-Writing_Code.html#tie-these-together",
    "title": "Writing Code",
    "section": "Tie These Together…",
    "text": "Tie These Together…"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#gitgithub-is-for-anything",
    "href": "lectures/1.3-Writing_Code.html#gitgithub-is-for-anything",
    "title": "Writing Code",
    "section": "Git+GitHub is for… anything!",
    "text": "Git+GitHub is for… anything!\n\n\nThis whole course (minus videos and assessments) is on GitHub."
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#why-use-it",
    "href": "lectures/1.3-Writing_Code.html#why-use-it",
    "title": "Writing Code",
    "section": "Why Use It?",
    "text": "Why Use It?\nThe features that make it easy to use have real advantages for you:\n\nYou will spend less time wrestling with Microsoft Word and its formatting; this means that…\nYou will spend more time focussing on the important stuff: writing and coding!\nYou will be able to combine Code and Documentation easily because Python/R and Markdown all coexist happily on GitHub.\nThanks to knitr and, now, Quarto your code, your output, and your analysis are always in one place.\n\nThis entire course—and the web site and slide desck you’re using now—is generated by Quarto."
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#markdown-examples",
    "href": "lectures/1.3-Writing_Code.html#markdown-examples",
    "title": "Writing Code",
    "section": "Markdown Examples",
    "text": "Markdown Examples\nSee CommonMark and the Markdown Guide for more :\n\n\n\n\nFormat\n\n\nOutput\n\n\n\n\nPlain text…\n\n\nPlain text\n\n\n\n\n## A Large Heading\n\n\n\nA Large Heading\n\n\n\n\n\n### A Medium Heading\n\n\n\nA Medium Heading\n\n\n\n\n\n- A list- More list\n\n\n\n\nA list\n\n\nMore list\n\n\n\n\n\n\n1. An ordered list2. More ordered list\n\n\n\n\nAn ordered list\n\n\nMore ordered list\n\n\n\n\n\n\n[A link](http://casa.ucl.ac.uk)\n\n\nA link\n\n\n\n\n\nThis guide is good for HTML entities, though Google will also give you them pretty easily if you type HTML entity code for copyright…"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#more-markdown",
    "href": "lectures/1.3-Writing_Code.html#more-markdown",
    "title": "Writing Code",
    "section": "More Markdown…",
    "text": "More Markdown…\nHere are some more resources:\n\nGetting Started\nAn online interactive tutorial\nCheatsheet\n\nAnd once you’re ready to get ‘serious’, check out this tutorial on Sustainable Authorship in Plain Text using Pandoc and Markdown from The Programming Historian! That’s what actually underpins Knitr and Quarto, but you can do so much more…"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#markdown-is-for-anything",
    "href": "lectures/1.3-Writing_Code.html#markdown-is-for-anything",
    "title": "Writing Code",
    "section": "Markdown is for… anything!",
    "text": "Markdown is for… anything!\nThis whole course, including this presentation was writting in Markdown."
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#literate-programming",
    "href": "lectures/1.3-Writing_Code.html#literate-programming",
    "title": "Writing Code",
    "section": "Literate Programming",
    "text": "Literate Programming\nIdeally, we want to write code in ways that are ‘literate’.\n\nThe best programs are written so that computing machines can perform them quickly and so that human beings can understand them clearly. A programmer is ideally an essayist who works with traditional aesthetic and literary forms as well as mathematical concepts, to communicate the way that an algorithm works and to convince a reader that the results will be correct.\n\n\n– Donald Knuth, Selected Papers on Computer Science"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#key-tenets",
    "href": "lectures/1.3-Writing_Code.html#key-tenets",
    "title": "Writing Code",
    "section": "Key Tenets",
    "text": "Key Tenets\nWhat we want:\n\nWeaving: the code and its documentation are together in one file.\nTangling: the code can be run directly from this file.\n\nWhy do we want this?"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#jupyterlab-notebooks",
    "href": "lectures/1.3-Writing_Code.html#jupyterlab-notebooks",
    "title": "Writing Code",
    "section": "Jupyter(Lab) & Notebooks",
    "text": "Jupyter(Lab) & Notebooks\n\nModern Browser + Jupyter == Tangled, Woven code in (m)any languages\nIncluding maths:\n\\[\nf(a) = \\frac{1}{2\\pi i} \\oint_{\\gamma} \\frac{f(z)}{z-a} dz\n\\]\n\nNote: you can set equations in Markdown too!"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#all-kinds-of-features",
    "href": "lectures/1.3-Writing_Code.html#all-kinds-of-features",
    "title": "Writing Code",
    "section": "All Kinds of Features",
    "text": "All Kinds of Features\nJupyterLab is basically a web application:"
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#how-to-ace-the-assessments",
    "href": "lectures/1.1-Getting_Oriented.html#how-to-ace-the-assessments",
    "title": "Getting Oriented",
    "section": "How to ‘Ace’ the Assessments?",
    "text": "How to ‘Ace’ the Assessments?\n\nStudy like you’re learning a new language. Do the readings. Talk to other students. Ask for help when you need it!\n\n\nMore on how to ask for help below!"
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#also",
    "href": "lectures/1.1-Getting_Oriented.html#also",
    "title": "Getting Oriented",
    "section": "Also…",
    "text": "Also…\nWe hope to convince you t-hat:\n\nAnyone—and this includes you—can code.\nLearning to code does not require mathematical ability.\nLearning to code does not require linguistic ability.\nLearning to code does require practice. And more practice. And more again."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#consequences",
    "href": "lectures/1.1-Getting_Oriented.html#consequences",
    "title": "Getting Oriented",
    "section": "Consequences…",
    "text": "Consequences…\n\nIf you only code during the practical session then you will not learn how to code.\nIf you cram the night before then you will not learn how to code.\nIf you practice for 45 minutes a day then you will learn how to code."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#the-challenges",
    "href": "lectures/1.1-Getting_Oriented.html#the-challenges",
    "title": "Getting Oriented",
    "section": "The Challenges",
    "text": "The Challenges\n\nDifferent style of learning from what you might be used to (“I didn’t anticipate, or rather factor into my schedule, the amount of out-of-hours practice that was required to stay up to date.”).\nDoing stats and programming at the same time and connecting this all back to the bigger picture.\nDelayed gratification (you have to walk before you can run).\nEasy to fall behind, but hard to catch up (“the pace is relentless”)."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#the-rewards",
    "href": "lectures/1.1-Getting_Oriented.html#the-rewards",
    "title": "Getting Oriented",
    "section": "The Rewards",
    "text": "The Rewards\n\nSkills that are highly transferrable and highly sought-after professionally.\nProblem-solving and practical skills that are valued by the private and public sectors.\nA whole new way of seeing the world and interacting with it.\nLots of support along the way… if you remember to ask for it!\n\nSee this thread on moving from academia to data science."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#the-implications",
    "href": "lectures/1.1-Getting_Oriented.html#the-implications",
    "title": "Getting Oriented",
    "section": "The Implications",
    "text": "The Implications\nYou will learn to code best if you treat it like learning a new language:\n\nStart simple and work up.\nGoogle is your friend (really).\nTalk with your friends (i.e. Slack).\nImmerse yourself and practice regularly.\nDo the readings even if we don’t address them specifically.\nLearn how to ask questions (i.e. Search Stack Overflow).\nSubscribe to a ‘magazine’ or two (e.g. Medium or Pocket)."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#study-aids",
    "href": "lectures/1.1-Getting_Oriented.html#study-aids",
    "title": "Getting Oriented",
    "section": "Study Aids",
    "text": "Study Aids\nWhen you need an answer right now:\n\nGoogle\nStack Overflow\nSlack\n\nWhen you want to learn more:\n\nMedium\nPocket\n\n\nGoogle will become more useful as you learn more and this is definitely one class in which “I Googled it” is a good answer.\nAs of early September 2020, Stack Overflow contains over 1.5 million Python questions alone! Chances are someone else has had your question before."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#study-right",
    "href": "lectures/1.1-Getting_Oriented.html#study-right",
    "title": "Getting Oriented",
    "section": "Study ‘Right’",
    "text": "Study ‘Right’\nI’ve tried to throw together some ideas on how you can study effectively that covers things relating to managing distractions when you’ve only got limited time.\nThere’s also useful ideas on how to get help that covers things like ‘how to get a reply from your Prof’ and ‘where to look for help’.\n\nIf you have a trick or technique that works for you then I want to hear about it! And I’d encourage you all to share with your peers anything that helps you to stay focussed but also relaxed!"
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#before-you-ask-for-help",
    "href": "lectures/1.1-Getting_Oriented.html#before-you-ask-for-help",
    "title": "Getting Oriented",
    "section": "Before You Ask for Help",
    "text": "Before You Ask for Help\nFrom the Computer Science Wiki:\n\nDraw a picture of the problem\nExplain the problem to a rubber duck, teddy bear or whatever (really!)\nForget about a computer; how would you solve this with a pencil and paper?\nThink out loud\nExplain the problem to a friend\n\nTo which I would add:\n\nUse print(variable) statements liberally in your code!\n\n\nWe’ll cover this last bit as we get more used to coding!"
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#where-to-ask-for-help",
    "href": "lectures/1.1-Getting_Oriented.html#where-to-ask-for-help",
    "title": "Getting Oriented",
    "section": "Where to Ask for Help",
    "text": "Where to Ask for Help\nThere is no shame in asking for help. None. We are here to support your learning and we have chosen a range of tools to support that:\n\nSlack: use public #intro-to-programming channel for help with coding, practical, and related course questions.\nOffice Hours: use Teams to contact PGTAs and lecturers for 1:1 help with software, hardware, and more complex issues.\nOut-of-Hours: use email to raise personal circumstances and related issues for focussed support. Make use of Professional Services support as-needed to preserve privacy.\n\n\nWe’ll talk about Slack more later, but we think that this is the best way to get help when you need it. Slack enables us to support you as a community of learners across computer / tablet / phone."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#when-to-ask-for-help",
    "href": "lectures/1.1-Getting_Oriented.html#when-to-ask-for-help",
    "title": "Getting Oriented",
    "section": "When to Ask for Help",
    "text": "When to Ask for Help\n\nWhen you get warning messages from your computer’s Operating System.\nWhen you cannot get the coding environment to run at all.\nWhen even simple commands return line after line of error code.\nWhen you have no clue what is going on or why.\nWhen you have been wrestling with a coding question for more than 20 minutes (but see: How to Ask for Help!)\n\n\nIn order to learn you do need to struggle, but only up to a point! So we don’t think that giving you the answer to a coding question as soon as you get stuck is a good way for you to learn. At the same time, I remain sad to this day that one of the most insightful students I’ve ever taught in a lecture context dropped out of our module because they were having trouble with their computer and thought it was their fault nothing was working right. By we had realised what was going on it was too late: they were so far behind that they didn’t feel able to catch up. We’d rather that you asked and we said “Close, but try it again” than you didn’t ask and checked out thinking that you couldn’t ‘do’ programming."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#how-to-ask-for-help",
    "href": "lectures/1.1-Getting_Oriented.html#how-to-ask-for-help",
    "title": "Getting Oriented",
    "section": "How to Ask for Help",
    "text": "How to Ask for Help\nI liked the “How to ask programming questions” page provided by ProPublica:\n\nDo some research first.\nBe specific.\nRepeat.\nDocument and share.\n\nIf you find yourself wanting to ask a question on Stack Exchange then they also have a guide, and there are plenty of other checklists."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#and-finally",
    "href": "lectures/1.1-Getting_Oriented.html#and-finally",
    "title": "Getting Oriented",
    "section": "And Finally…",
    "text": "And Finally…\nDo not allow your computer to auto-update during term. Inevitably, major upgrades will break developer tools. Do this by choice only when you have time.\n\nMany students allowed their computer to update to Big Sur last year and it broke their entire computing environment. Some did this shortly before a submission was due. Do not do this!"
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#additional-resources",
    "href": "lectures/1.1-Getting_Oriented.html#additional-resources",
    "title": "Getting Oriented",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nSee the GIS&T Body of Knowledge (BoK) for quick overview of concepts, techniques, and tools: gistbok.ucgis.org.\nA degree of ‘plagiarism’ is acceptable in code since that’s how we learn; however, mindless copy+pasting of Stack Overflow code leads to sphagetti and, often, incorrect results or difficult-to-squash bugs. Think of it like paraphrasing.\nTo distinguish between plagiarism and paraphrasing here’s a nice tutorial that you can also use to help you with your ‘regular’ writing."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#why-use-it",
    "href": "lectures/1.2-Tools_of_the_Trade.html#why-use-it",
    "title": "Tools of the Trade",
    "section": "Why Use It?",
    "text": "Why Use It?\nWe want you to use Dropbox for four reasons:\n\nYou can access your Dropbox files anywhere in the world via the Desktop or Web.\nYou have an backup of all of your work, even if your computer has a complete meltdown.\nYou have limited ‘versioning’ support, so if you accidentally overwrite an essay or file, you can recover a previous version.\nDropbox is how we collaborate, and it’s how many businesses work as well."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#we-recommend",
    "href": "lectures/1.2-Tools_of_the_Trade.html#we-recommend",
    "title": "Tools of the Trade",
    "section": "We Recommend…",
    "text": "We Recommend…\nThat you keep all files that aren’t in GitHub in your Dropbox folder. This applies to all your CASA MSc work but could be especially useful for ensuring that data files used as part of your group work are readily accessible!\nDropbox signup: bit.ly/32jhdvN"
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#why-use-it-1",
    "href": "lectures/1.2-Tools_of_the_Trade.html#why-use-it-1",
    "title": "Tools of the Trade",
    "section": "Why Use It?",
    "text": "Why Use It?\nWe want you to use Slack for four reasons:\n\nMoodle is clunky and formal—it works well for one-to-many communication, but not so much for ‘chat’.\nSlack offers a searchable history—you will have access to this archive for as long as you need it.\nYou (and we) can access Slack on every major OS (OSX, Windows, iOS, Android, and Windows Phone) and via a browser quickly.\nSlack is used in the ‘real world’ by everyone from Apple to PayPal and the JPL. This is how developers work."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#we-recommend-1",
    "href": "lectures/1.2-Tools_of_the_Trade.html#we-recommend-1",
    "title": "Tools of the Trade",
    "section": "We Recommend…",
    "text": "We Recommend…\nInstall the Slack client on your phone and on your personal computer and start using it as the way to ask questions, share answers, and generally keep ‘up to date’ on things across the entire MSc.\nSlack signup: casa-students-2022.slack.com\nP.S. Unless a question is personal it should normally be asked in the appropriate module channel."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#why-use-it-2",
    "href": "lectures/1.2-Tools_of_the_Trade.html#why-use-it-2",
    "title": "Tools of the Trade",
    "section": "Why Use It?",
    "text": "Why Use It?\nDocker is a ‘virtualisation platform’ that allows you to run a second (virtual) computer on your personal computer. We use it for four reasons:\n\nEasier installation than Anaconda Python and everyone has the same versions of every library.\nNo spillover effects since each container is isolated.\nEasy to tidy up when you’re done or add new containers when you start something new (e.g. PostgreSQL).\nUsed in the ‘real world’ by many companies (JP Morgan Chase, GSK, PayPal, Twitter, Spotify, Uber…)."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#we-recommend-2",
    "href": "lectures/1.2-Tools_of_the_Trade.html#we-recommend-2",
    "title": "Tools of the Trade",
    "section": "We Recommend…",
    "text": "We Recommend…\nUsing Docker because configuring a development machine is hard, this makes it simple. If a Docker image works for us then we know 1 it works for you.\nDocker Desktop is your starting point.\nNot always true, alas."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#why-use-it-3",
    "href": "lectures/1.2-Tools_of_the_Trade.html#why-use-it-3",
    "title": "Tools of the Trade",
    "section": "Why Use It?",
    "text": "Why Use It?\nWe use Anaconda Python for three reasons:\n\nIt is easy to create and configure virtual environments (each research project has its own environment).\nUse of channels allows installation of cutting-edge libraries not yet packaged for ‘regular’ Python (install from GitHub, etc.)\nWidely supported by developers with builds for most Operating Systems and a focus on data science applications.\n\nIt’s what we use on the Docker image as well."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#we-recommend-3",
    "href": "lectures/1.2-Tools_of_the_Trade.html#we-recommend-3",
    "title": "Tools of the Trade",
    "section": "We Recommend…",
    "text": "We Recommend…\nIf Docker doesn’t work on your computer, then this is how we will get you up and running because it’s (fairly) robust and ‘standard issue’. However, we can’t guarantee you’ll get the same versions of every package as installed on the virtualised systems so differences may emerge.\nYou’ll need to download the ‘Individual Edition’: 64-bit Graphical Installer 1\nUnless your computer is very, very old."
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#documenting-code",
    "href": "lectures/1.3-Writing_Code.html#documenting-code",
    "title": "Writing Code",
    "section": "Documenting Code",
    "text": "Documenting Code\nThere is a lot more to documenting your code than just adding comments."
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#writing-running-code",
    "href": "lectures/1.3-Writing_Code.html#writing-running-code",
    "title": "Writing Code",
    "section": "Writing & Running Code",
    "text": "Writing & Running Code\n\nIn an ideal world, we would say what we wanted the computer to do and it would do it. More frequently, we rush off with some half-formed thoughts, write some code that kind of does what we want, and then can’t recall why wanted that in the first place. This has… consequences:\n\n\nNot a joke: I have been known to delete poorly-commented and documented code on the basis that it was ‘inefficient’ or ‘unnecessary’… Results have included (briefly) taking down a corporate web site.\n\n\nNote: I do not mean Siri here.\nAlso: thank god for version control (even if was, then, only CVS)."
  },
  {
    "objectID": "lectures/4.2-Classes.html#whats-an-object",
    "href": "lectures/4.2-Classes.html#whats-an-object",
    "title": "Classes",
    "section": "What’s an Object?",
    "text": "What’s an Object?\nObjects are instantiated versions of classes: \"hello world\" is an instance of a string, and ['A','B',1,3] is an instance of a list.\nOr: the class is your recipe, the object is your 🍕…"
  },
  {
    "objectID": "lectures/4.2-Classes.html#really-like-a-pizza",
    "href": "lectures/4.2-Classes.html#really-like-a-pizza",
    "title": "Classes",
    "section": "Really… Like a Pizza!",
    "text": "Really… Like a Pizza!\nclass pizza(object):\n  base = 'sourdough'\n  \n  def __init__(self, sauce='tomato', cheese='mozzarella'):\n    self.toppings = []\n    self.sauce = sauce\n    self.cheese = cheese\n    \n  def add_topping(self, topping: str) -> None:\n    self.toppings.insert(len(self.toppings), topping)\n  \n  def get_pizza(self) -> list:\n    ingredients = [self.base, self.sauce, self.cheese]\n    ingredients.extend(self.toppings)\n    return ingredients"
  },
  {
    "objectID": "lectures/4.2-Classes.html#class-definition",
    "href": "lectures/4.2-Classes.html#class-definition",
    "title": "Classes",
    "section": "Class Definition",
    "text": "Class Definition\nclass pizza(object):\n\n    base = 'sourdough'\n    ...\nFollows the pattern: class <name>(<parent class>):.\nYou can find many examples in: /opt/conda/envs/sds2020/lib/python3.7/site-packages (Docker)."
  },
  {
    "objectID": "lectures/4.2-Classes.html#the-constructor",
    "href": "lectures/4.2-Classes.html#the-constructor",
    "title": "Classes",
    "section": "The Constructor",
    "text": "The Constructor\n  def __init__(self, sauce='tomato', cheese='mozzarella'):\n    self.toppings = []\n    self.sauce = sauce\n    self.cheese = cheese\nFollows the pattern: def __init__(self, <params>):"
  },
  {
    "objectID": "lectures/4.2-Classes.html#adding-toppings",
    "href": "lectures/4.2-Classes.html#adding-toppings",
    "title": "Classes",
    "section": "Adding Toppings",
    "text": "Adding Toppings\ndef add_topping(self, topping: str) -> None:\n    self.toppings.insert(len(self.toppings), topping)\nFollows the pattern: def <function>(self, <params>):"
  },
  {
    "objectID": "lectures/4.2-Classes.html#getting-the-pizza",
    "href": "lectures/4.2-Classes.html#getting-the-pizza",
    "title": "Classes",
    "section": "Getting the Pizza",
    "text": "Getting the Pizza\ndef get_pizza(self) -> list:\n    ingredients = [self.base, self.sauce, self.cheese]\n    ingredients.extend(self.toppings)\n    return ingredients"
  },
  {
    "objectID": "lectures/4.2-Classes.html#pizza-in-action",
    "href": "lectures/4.2-Classes.html#pizza-in-action",
    "title": "Classes",
    "section": "Pizza in Action",
    "text": "Pizza in Action\np = pizza(sauce='white')\np.add_topping('peppers')\np.add_topping('chilis')\np.get_pizza()\n> ['sourdough', 'white', 'mozzarella', 'peppers', 'chilis']"
  },
  {
    "objectID": "lectures/4.2-Classes.html#check-it-out",
    "href": "lectures/4.2-Classes.html#check-it-out",
    "title": "Classes",
    "section": "Check it Out",
    "text": "Check it Out\np1 = pizza(sauce='white')\np1.add_topping('peppers')\np1.add_topping('chilis')\n\np2 = pizza()\np2.base = \"Plain old base\"\np2.add_topping('pineapple')\np2.add_topping('ham')\n\np1.get_pizza()\n> ['sourdough', 'white', 'mozzarella', 'peppers', 'chilis']\np2.get_pizza()\n> ['Plain old base', 'tomato', 'mozzarella', 'pineapple', 'ham']"
  },
  {
    "objectID": "lectures/4.2-Classes.html#recap-how-to-make-a-pizza",
    "href": "lectures/4.2-Classes.html#recap-how-to-make-a-pizza",
    "title": "Classes",
    "section": "Recap: How to Make a Pizza",
    "text": "Recap: How to Make a Pizza\nA class is defined by:\nclass <name>(<parent class):\n  ...\nA class is initialised by:\n  def __init__(self, <any parameters>):\n    ...\nAll methods have to have this:\n  def <method>(self, <any parameters>):\n    ..."
  },
  {
    "objectID": "lectures/4.2-Classes.html#recap-how-to-make-a-pizza-contd",
    "href": "lectures/4.2-Classes.html#recap-how-to-make-a-pizza-contd",
    "title": "Classes",
    "section": "Recap: How to Make a Pizza (cont’d)",
    "text": "Recap: How to Make a Pizza (cont’d)\nThis is an instance variable:\n  self.<var> = <something>\nThis is a class variable:\n  <var> = <something>"
  },
  {
    "objectID": "lectures/4.2-Classes.html#recap-remember-your-self",
    "href": "lectures/4.2-Classes.html#recap-remember-your-self",
    "title": "Classes",
    "section": "Recap: Remember Your Self",
    "text": "Recap: Remember Your Self\n\nSo the keyword self refers to the instantiated object: the object always passes a reference to itself as the first parameter in any method. And self in a variable definition (self.base) is an instance variable, while base is a class variable."
  },
  {
    "objectID": "lectures/4.2-Classes.html#resources",
    "href": "lectures/4.2-Classes.html#resources",
    "title": "Classes",
    "section": "Resources",
    "text": "Resources\n\nClasses\nObjects\nBasic class definition\nInstance methods and attributes\nChecking instance types\nClass methods and members\nCreating a class\nConstructing an object\nClass methods\nClass vs Instance Variables\nObject data\nInheritance"
  },
  {
    "objectID": "lectures/4.4-Errors.html#helpfully",
    "href": "lectures/4.4-Errors.html#helpfully",
    "title": "Errors",
    "section": "Helpfully…",
    "text": "Helpfully…\nPython will always try to tell you what it thinks when wrong: “I didn’t understand what you meant by this…” or “I’m sorry, I can’t let you do that Dave…”\nThe challenges are: 1. Python tends to give you a lot of information about the error: this can be very helpful for programmers dealing with complex problems and totally overwhelming for beginners. 2. That what Python thinks the problem is doesn’t always line up with where the problem actually is. In cases of syntax, for instance, the problem could be an unclosed parenthesis three lines earlier!"
  },
  {
    "objectID": "lectures/4.4-Errors.html#challenge-1",
    "href": "lectures/4.4-Errors.html#challenge-1",
    "title": "Errors",
    "section": "Challenge 1",
    "text": "Challenge 1\nThat the ‘error’ isn’t always the error…\ntotal = 0\nprint(\"About to start loop\"\nfor i in range(1,10):\n  total += i\nprint(total)\nThis outputs:\nprint(\"About to start loop\"\n... for i in range(1,10):\n  File \"<stdin>\", line 2\n    for i in range(1,10):\n                        ^\nSyntaxError: invalid syntax"
  },
  {
    "objectID": "lectures/4.4-Errors.html#errors-have-types",
    "href": "lectures/4.4-Errors.html#errors-have-types",
    "title": "Errors",
    "section": "Errors Have Types",
    "text": "Errors Have Types\n\nIn the same way that variables have types, so do errors. These types give us insight into the issue, but they also allow us to distinguish between different types of errors.\n\n Why might the ability to distinguish between different types of errors be helpful?"
  },
  {
    "objectID": "lectures/4.4-Errors.html#trapping-errors",
    "href": "lectures/4.4-Errors.html#trapping-errors",
    "title": "Errors",
    "section": "Trapping Errors",
    "text": "Trapping Errors\nWe might reasonably want to distinguish between errors that we could reasonably expect or that are not serious, from those that we did not expect or that call the results of the program into question.\n\nSo it makes sense to think: “Well, let’s try this and see what happens. If we have a problem of this type then it’s not serious and we should carry on. But if we have a problem that type then we need to stop what we’re doing right away.”"
  },
  {
    "objectID": "lectures/4.4-Errors.html#pythons-approach",
    "href": "lectures/4.4-Errors.html#pythons-approach",
    "title": "Errors",
    "section": "Python’s Approach",
    "text": "Python’s Approach\nPython calls errors exceptions, so this leads to:\ntry:\n  ... some code that might fail...\nexcept <Named Error Type>:\n  ... what do it if it fails for a specific reason...\nexcept:\n  ... what to do if it fails for any other reason...\nfinally:\n  ... always do this, even if it fails...\nWe can even create our own problems:\nraise Exception(\"Sorry, I can't let you do that, Dave.\")"
  },
  {
    "objectID": "lectures/4.4-Errors.html#for-example",
    "href": "lectures/4.4-Errors.html#for-example",
    "title": "Errors",
    "section": "For Example",
    "text": "For Example\nIf x=10 and y=0 then:\nprint(x/y)\n> Traceback (most recent call last):\n>   File \"<stdin>\", line 1, in <module>\n> ZeroDivisionError: division by zero\nBut if you ‘trap’ the error using except then:\ntry:\n  print(x/y)\nexcept ZeroDivisionError:\n  print(\"You can't divide by zero!\")\nexcept:\n  print(\"Something has gone wrong.\")\nfinally: \n  print(\"Division is fun!\")\n\n> You can't divide by zero!\n> Division is fun!\nNote: if we need to access the actual exception: except ZeroDivisionError as e:"
  },
  {
    "objectID": "lectures/4.4-Errors.html#im-just-creating-problems-now",
    "href": "lectures/4.4-Errors.html#im-just-creating-problems-now",
    "title": "Errors",
    "section": "I’m Just Creating Problems Now",
    "text": "I’m Just Creating Problems Now\nYou can trigger your own exceptions using raise.\ntry:\n  print(x/y)\nexcept ZeroDivisionError:\n  print(\"You can't divide by zero!\")\n  raise Exception(\"Please don't do that again!\")\nfinally: \n  print(\"Division is fun!\")"
  },
  {
    "objectID": "lectures/4.4-Errors.html#errors-more-errors",
    "href": "lectures/4.4-Errors.html#errors-more-errors",
    "title": "Errors",
    "section": "Errors & More Errors!",
    "text": "Errors & More Errors!\nIf x=10 and y=0 then the previous slide’s code produces:\nYou can't divide by zero!\nDivision is fun!\nTraceback (most recent call last):\n  File \"<stdin>\", line 2, in <module>\nZeroDivisionError: division by zero\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"<stdin>\", line 5, in <module>\nException: Please don't do that again!"
  },
  {
    "objectID": "lectures/4.4-Errors.html#understanding-multiple-errors",
    "href": "lectures/4.4-Errors.html#understanding-multiple-errors",
    "title": "Errors",
    "section": "Understanding Multiple Errors",
    "text": "Understanding Multiple Errors\n\nThe code we try triggers the ZeroDivisionError block.\nThis prints \"You can't divide by zero!\"\nWe then raise a new exception that is not caught.\nThe finally code executes because it always does before Python exits.\nPython exits with the message from our newly raised Exception.\n\nThus: ‘During handling of above (ZeroDivisionError) another exception (our Exception) occurred…’"
  },
  {
    "objectID": "lectures/4.4-Errors.html#typed-errors",
    "href": "lectures/4.4-Errors.html#typed-errors",
    "title": "Errors",
    "section": "Typed Errors",
    "text": "Typed Errors\nWe can create our own classes of error very easily:\nclass CustomError(Exception):\n  pass\nThis can then be triggered with:\nraise CustomError(\"Our custom error\")\nAnd (very importantly) this can be caught with:\nexcept CustomError: \n  ... do something ...\nThis means that exceptions could accept custom arguments, perform tidying-up or rollback operations, etc."
  },
  {
    "objectID": "lectures/4.4-Errors.html#test-based-development",
    "href": "lectures/4.4-Errors.html#test-based-development",
    "title": "Errors",
    "section": "Test-Based Development",
    "text": "Test-Based Development\nWe can actually think of exceptions as a way to develop our code; for example:\n# Testing the 'addition' operator\ntest(1+1, 2)           # Should equal 2\ntest(1+'1', TypeError) # Should equal TypeError\ntest('1'+'1', '11')    # Should equal '11'\ntest(-1+1, 0)          # Should equal 0 \nOur test(A,B) function takes an input (A) and the expected output (B) and then compares them. The test returns True if A==B and False otherwise."
  },
  {
    "objectID": "lectures/4.4-Errors.html#unit-tests",
    "href": "lectures/4.4-Errors.html#unit-tests",
    "title": "Errors",
    "section": "Unit Tests",
    "text": "Unit Tests\nEach test is called a Unit Test because it tests one thing and one thing only. So if you had three functions to ‘do stuff’ then you’d need at least three unit tests. A Unit Test may be composed of one or more assertions. Our code on the previous slide contained 4 assertions.\nA Unit Test does not mean that your code is correct or will perform properly under all circumstances. It means that your code returns the expected value for a specified set of inputs.\nPython considers this approach so important that it’s built in."
  },
  {
    "objectID": "lectures/4.4-Errors.html#approach-1",
    "href": "lectures/4.4-Errors.html#approach-1",
    "title": "Errors",
    "section": "Approach 1",
    "text": "Approach 1\nThis is an explict assertion to test fun:\nimport unittest\n\ndef fun(x):\n  return x + 1\n\nclass MyTest(unittest.TestCase):\n  def test(self):\n    self.assertEqual(fun(3), 4)\n    self.assertEqual(fun(3), 5)\n\nm = MyTest()\nm.test()"
  },
  {
    "objectID": "lectures/4.4-Errors.html#approach-2",
    "href": "lectures/4.4-Errors.html#approach-2",
    "title": "Errors",
    "section": "Approach 2",
    "text": "Approach 2\nThis approach uses the ‘docstring’ (the bits between \"\"\") to test the results of the function. This is intended to encourage good documentation of functions using examples:\ndef square(x):\n    \"\"\"Return the square of x.\n\n    >>> square(2)\n    4\n    >>> square(-2)\n    4\n    \"\"\"\n    return x * x\n\nif __name__ == '__main__':\n    import doctest\n    doctest.testmod()"
  },
  {
    "objectID": "lectures/4.4-Errors.html#collaboration-continuous-integration",
    "href": "lectures/4.4-Errors.html#collaboration-continuous-integration",
    "title": "Errors",
    "section": "Collaboration & Continuous Integration",
    "text": "Collaboration & Continuous Integration\nThe Unit Test approach is often used on collaborative projects, especially in the Open Source world.\nA commit, merge, or pull on GitHub can all be used to trigger the testing process for the entire software ‘stack’: the running of all tests for multiple components is called ‘integration testing’.\nGitHub offers this service as ‘TravisCI’, with CI standing for Continuous Integration.\n\nThis is heavily used by PySAL and other robust FOSS projects since TravisCI is free for FOSS projects!"
  },
  {
    "objectID": "lectures/4.4-Errors.html#resources",
    "href": "lectures/4.4-Errors.html#resources",
    "title": "Errors",
    "section": "Resources",
    "text": "Resources\n\nHandling exceptions\nReporting errors\nPython Custom Exceptions\nWriting and Using Custom Exceptions in Python\nPython Documentation\nTowards Data Science\nUnit Testing in Python\nUnderstanding Unit Testing\nTesting Your Code\nGetting Started with Testing in Python\nPython’s unittest Library\nVideo: Unit Testing Your Code"
  },
  {
    "objectID": "lectures/5.3-Files.html#structure-of-a-tabular-data-file",
    "href": "lectures/5.3-Files.html#structure-of-a-tabular-data-file",
    "title": "File Formats",
    "section": "Structure of a Tabular Data File",
    "text": "Structure of a Tabular Data File\nRow and column names make it a lot easier to find and refer to data (e.g. the ‘East of England row’ or the ‘Total column’) but they are not data and don’t belong in the data set itself.\nUsually, one record (a.k.a. observation) finishes and the next one starts with a ‘newline’ (\\n) or ’carriage return (\\r) or both (\\r\\n) but it could be anything (e.g. EOR).\nUsually, one field (a.k.a. attribute or value) finishes and the next one starts with a comma (,) which gives rise to CSV (Comma-Separate Values), but it could be tabs (\\t) or anything else too (; or | or EOF).\n How would we choose a good field separator?\n\nPro tip: if we store column and row names separately from the data then we can access everything easily without having to factor in any ‘special’ values!\nNoice also the nd here. This is the escape sequence again that you also encountered when dealing with the Shell as well. Remember that \\ is necessary if you have a space in your file name or path."
  },
  {
    "objectID": "lectures/5.3-Files.html#most-common-formats",
    "href": "lectures/5.3-Files.html#most-common-formats",
    "title": "File Formats",
    "section": "Most Common Formats",
    "text": "Most Common Formats\n\n\n\n\n\n\n\n\n\nExtension\nField Separator\nRecord Separator\nPython Package\n\n\n\n\n.csv\n, but can appear in fields enclosed by \".\n\\n but could be \\r or \\r\\n.\ncsv\n\n\n.tsv or .tab\n\\t and unlikely to appear in fields.\n\\n but could be \\r or \\r\\n.\ncsv (!)\n\n\n.xls or .xlsx\nBinary, you need a library to read.\nBinary, you need a library to read.\nxlrd/xlsxwriter\n\n\n.sav or .sas\nBinary, you need a library to read.\nBinary, you need a library to read.\npyreadstat\n\n\n\n\nOne of the reasons we like CSV and TSV files is that they can be opened and interacted with using the Command Line directly. As soon as you get into binary file formats you either need the original tool (and then export) or you need a tool that can read those formats. So the complexity level rises very quickly.\nOf course, sometimes you can gain (e.g. SPSS or SAS) in terms of obtaining information about variable types, levels, etc. but usually you use these when that’s all that’s available or when you want to write a file for others to use."
  },
  {
    "objectID": "lectures/5.3-Files.html#testing-a-mapping",
    "href": "lectures/5.3-Files.html#testing-a-mapping",
    "title": "File Formats",
    "section": "Testing a Mapping",
    "text": "Testing a Mapping\nAs we’ll see over the remainder of the term, working out an appropriate mapping (representation of the data) can take up enormous amounts of time. You should never assume that the data matches the spec.\n\nIt’s commonly held that 80% of data science is data cleaning.\n\nThe Unix utilities (grep, awk, tail, head) can be very useful for quickly exploring the data in order to develop a basic understanding of the data and to catch obvious errors."
  },
  {
    "objectID": "lectures/5.3-Files.html#why-this-isnt-easy",
    "href": "lectures/5.3-Files.html#why-this-isnt-easy",
    "title": "File Formats",
    "section": "Why This Isn’t Easy",
    "text": "Why This Isn’t Easy"
  },
  {
    "objectID": "lectures/5.3-Files.html#label-these",
    "href": "lectures/5.3-Files.html#label-these",
    "title": "File Formats",
    "section": "Label These",
    "text": "Label These\n\n\n\n\nMetadata is relevant to our understanding of the data and so is important, but it’s not relevant to treating the data as data so we need to be able to skip it.\nColumn names are going to be how we access a given attribute for each observation.\nRow names are not normally data themselves, but are basically labels or identifiers for observations. Another term for this would be the data index.\nIf we store row and column names/indices separately from the data then we don’t have to treat them as ‘special’ or factor them into, for example, the calculation of summary stats.\nAlso have to consider trade-offs around mapping the full column names on to something a little faster and easier to type!"
  },
  {
    "objectID": "setup/no_install.html",
    "href": "setup/no_install.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "The following options provide alternatives for those who are unable to install the full programming environment on their main computer and would otherwise be unable to continue with the module. Each of these has pros and cons but they allow you to run code ‘in the cloud’ and so mean that any internet-connected device can be used to write and run code."
  },
  {
    "objectID": "setup/env.html",
    "href": "setup/env.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "Over the years, we have experimented with a range of approaches to setting you up with a programming environment: VirtualBox; Vagrant; Docker; and Anaconda Python. Each of these has pros and cons, but after careful consideration we have come to the conclusion that Docker is the most robust way to ensure a consistent experience in which all students end up with the same versions of each library, difficult-to-diagnose hardware/OS issues are minimised, and running/recovery is the most straightforward.\nSome students are unable to run Docker on their older Windows 10 Home machines, in which case Anaconda Python can be used with the configuration file that we provide. However, if your machine runs Docker then you must use Docker: this isolates the programming environment form your computer, ensuring that nothing is clobbered by accident, and guaranteeing that you are working with the same version of every Python library as the rest of the class (and for which the practicals are tested).\nAnaconda is only for emergencies.\n\n\n\n\n\n\n\n\nKnown issues\n\n\n\nOn Windows, if you cannot see any of the files on your main system when you start up Docker with the SDS image, then please try replacing this part of the Docker command (see details below):\n-v \"$WORK_DIR\":/home/jovyan/work\nwith:\n--mount type=bind,source=\"$(pwd)\",target=/home/jovyan/work\nThis should enable you to see any existing files that you have, while also allowing you to save any files that you create.\n\n\nAs well…\n\n\n\n\n\n\nTip\n\n\n\nOn Mac, if you are using one of the new M1 or M2 chips on your main system then you need to tell Docker to emulate the older Intel chipset (which is all I have access to!) by changing this part of the Docker run command (see details below):\n--name sds -p 8888:8888\nto:\n--name sds --platform linux/amd64 -p 8888:8888\n\n\n\n\n\nAgain, we only support Anaconda Python as a fallback for students who would otherwise be unable to complete the module because their computer does not support Docker. You are always free to install Anaconda Python and to use our YAML configuration script to install the SDS environment, but you should do this in your own time: in previous years students have encountered difficult-to-diagnose bugs in their code (and lost marks in the Assessments!) because they had installed an older or more recent version of a Python library that the one configured and tested in the SDS environment.\nWe believe that the replication advantages of virtualisation outweigh the disbenefits in terms of performance. It also means that you will spend less time installing libraries and more time running code, which is where your attention should really be when you are familiarising yourself with the foundations of data science. Eventually you will, of course, want to install and manage your own programming environment (possibly even by building and sharing Docker images!) but this can be left for later when you have developed an appreciation of how and when virtualisation is (or is not) an appropriate solution to your needs."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#variables-have-types",
    "href": "lectures/2.3-Python_the_Basics.html#variables-have-types",
    "title": "Python, the Basics",
    "section": "Variables Have Types",
    "text": "Variables Have Types\n\n\n\n\n\n\n\n\nName\nValue\nType\n\n\n\n\nmsg\n‘Hello world’\ntype(msg)==str\n\n\nanswer\n42\ntype(answer)==int\n\n\npi\n3.14159\ntype(pi)==float\n\n\nc\ncomplex(5,2)\ntype(c)==complex\n\n\ncorrect\nTrue\ntype(correct)==bool\n\n\n\n\nAs we’ll see in Week 4, everything is also an object."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#we-can-change-those-types",
    "href": "lectures/2.3-Python_the_Basics.html#we-can-change-those-types",
    "title": "Python, the Basics",
    "section": "We Can Change Those Types",
    "text": "We Can Change Those Types\nMessage starts as a string:\nmsg = '42'\ntype(msg)\nBut we can change it to an integer like this:\nmsg = int(msg)\ntype(msg)\nAnd back to a string:\nmsg = str(msg)\ntype(msg)"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#variables-have-names",
    "href": "lectures/2.3-Python_the_Basics.html#variables-have-names",
    "title": "Python, the Basics",
    "section": "Variables Have Names",
    "text": "Variables Have Names\nRules for variable names:\n\nThey cannot start with a number (e.g. 3items is invalid)\nWhite space and symbols (e.g. $, +, etc.) are not allowed, but _ (underscore) is allowed (e.g. my_variable is fine)\nCase matters: myVar is different from both myvar and MYVAR\nBe consistent: my_var is more ‘Pythonic’, though myVar is also widely used, but don’t mix these!\nVariable names should be long enough to be clear but not so long as to be impractical: bldg_height vs. bh vs. max_building_height_at_eaves."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#we-cant-use-these-names",
    "href": "lectures/2.3-Python_the_Basics.html#we-cant-use-these-names",
    "title": "Python, the Basics",
    "section": "We Can’t Use These Names",
    "text": "We Can’t Use These Names\nYou should not try to use any of these words as a variable. Python may not complain, but strange things will happen when you run your code.\n\n\n\nand\ndel\nfrom\nnot\nwhile\n\n\nas\nelif\nglobal\nor\nwith\n\n\nassert\nelse\nif\npass\nyield\n\n\nbreak\nexcept\nimport\nprint\n\n\n\nclass\nexec\nin\nraise\n\n\n\ncontinue\nfinally\nis\nreturn\n\n\n\ndef\nfor\nlambda\ntry"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#operators",
    "href": "lectures/2.3-Python_the_Basics.html#operators",
    "title": "Python, the Basics",
    "section": "Operators",
    "text": "Operators"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#simple-operations-on-variables",
    "href": "lectures/2.3-Python_the_Basics.html#simple-operations-on-variables",
    "title": "Python, the Basics",
    "section": "Simple Operations on Variables",
    "text": "Simple Operations on Variables\nLet’s start with x=10 and y=5…\n\n\n\nOperator\nInput\nResult\n\n\n\n\nSum\nx + y\n15\n\n\nDifference\nx - y\n5\n\n\nProduct\nx * y\n50\n\n\nQuotient\nx / y\n2.0\n\n\nFloored Quotient\nx // y\n2\n\n\nRemainder\nx % y\n0\n\n\nPower\npow(x,y) or x**y\n100000\n\n\nEquality\nx==y\nFalse"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#strings-are-different",
    "href": "lectures/2.3-Python_the_Basics.html#strings-are-different",
    "title": "Python, the Basics",
    "section": "Strings Are Different",
    "text": "Strings Are Different\nWhen you do things with strings the answers can look a little different…\nLet’s start with x=\"Hello\" and y=\"You\" and z=2…\n\n\n\nOperator\nInput\nResult\n\n\n\n\nSum\nx + y\n'HelloYou'\n\n\nDifference\nx - y\nTypeError\n\n\nProduct\nx * z\nHelloHello\n\n\nEquality\nx==y\nFalse"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#using-strings-to-output-information",
    "href": "lectures/2.3-Python_the_Basics.html#using-strings-to-output-information",
    "title": "Python, the Basics",
    "section": "Using Strings to Output Information",
    "text": "Using Strings to Output Information\nObviously, a really common requirement that programmers have is ‘output a nicely formatted string containing information about the variables in my program’.\nOver time, Python has acquired no fewer than three ways to do this: 1) string concatenation using +; 2) string formatting using <str>.format(<variables>); and 3) the new f-strings using f”{variable_1}… {variable_n}“. There are pros and cons to each:\nx = 24\ny = 'Something'\nprint(\"The value of \" + y + \" is \" + str(x))\nprint(\"The value of {0} is {1}\".format(y, x))\nprint(f\"The value of {y} is {x}\")\n\nI rather like f-strings because they can actually contain any code you like (you could, for instance, write `f”The square root of {y} is {x**(1/2)}” and it would work. However, concatenation is the easiest to learn."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#operators-assemble",
    "href": "lectures/2.3-Python_the_Basics.html#operators-assemble",
    "title": "Python, the Basics",
    "section": "Operators Assemble!",
    "text": "Operators Assemble!\nOperators follow clear rules about precedence: what calculations are performed in what order. For instance, the following lines of code are not the same:\nx + y * 2         # == 20\n(x + y) * 2       # == 30\nx + y * 2 / 3     # == 13.3333333333334\nx + y * (2/3)     # == 13.3333333333332\n(x + y) * (2/3)   # == 10.0\nAnd here’s a subtle one:\n(x * y) ** 2/3    # == 833.333333333334\n(x * y) ** (2/3)  # == 13.5720880829745\nThe full list is here.\n\nIf you’re a little rusty on exponents, that last example is the cube root of (x*y)**2. So x**(1/2) * x**(1/2) == x).\nAlso notice that with floats you do not always get the same result from operations that should give the same answer. This is because non-terminating decimals (e.g. 1/3) will always be rounded by the computer because it doesn’t have infinite memory. The process of rounding means that you need to be very careful comparing (more on this later) floats."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#comparing-variables",
    "href": "lectures/2.3-Python_the_Basics.html#comparing-variables",
    "title": "Python, the Basics",
    "section": "Comparing Variables",
    "text": "Comparing Variables\nWhen variables are of the same type then comparing them is easy. Let’s go back to x=10 and y=5…\n\n\n\nOperator\nInput\nResult\n\n\n\n\n==\nx == y\nFalse\n\n\n!=\nx != y\nTrue\n\n\n<, <=\nx < y\nFalse\n\n\n>, >=\nx > y\nTrue\n\n\n\nBut notice, if x='4a', y='365', and z='42':\nx > y   # True\nx > z   # True\n\nNotice that the result of any (successful) comparison is a Boolean (True/False). We can save the output of this comparison to a new variable (e.g. z = x > y).\nThis last example has to do with the way that strings are compared."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#danger-will-robinson",
    "href": "lectures/2.3-Python_the_Basics.html#danger-will-robinson",
    "title": "Python, the Basics",
    "section": "Danger, Will Robinson!",
    "text": "Danger, Will Robinson!\nNotice the very subtle difference between = and ==!\nConfusing these two operators is the most common source of mistakes early on when learning to code in Python! One (=) does assignment, the other (==) does comparison.\nx = 5\nz = 10\nx = z   # Probably a mistake: setting x to the value of z\nx == z  # True, because x and z are now both set to the value of 10\nRemember this!"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#forcing-a-change-in-type",
    "href": "lectures/2.3-Python_the_Basics.html#forcing-a-change-in-type",
    "title": "Python, the Basics",
    "section": "Forcing a Change in Type",
    "text": "Forcing a Change in Type\nA really common mistake is to think that string (str) \"42\" is the same as the integer (int) 42. Here’s the output from some attempts at comparison:\nx='42'\ny=42\nx==y   # False\nx>y\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: '>' not supported between instances of 'str' and 'int'\nIf we want to compare them then we’ll need to change their type:\nx > str(y)  # False\nint(x) <= y # True\nx >= str(y) # Also True\n\nNotice that in the first example we can say that 42 is clearly not the same as ‘42’, but we can’t say whether it’s more or less because that’s non-sensical in this context. So this is a computer being totally logical but not always sensible.\nAlso notice the syntax for this: we have str(<something>) and int(<something>) to convert between types. These are functions, which we’ll spend a lot more time on next week!\nWhy might it be (fractionally) faster to compare integers than strings?"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#conditions",
    "href": "lectures/2.3-Python_the_Basics.html#conditions",
    "title": "Python, the Basics",
    "section": "Conditions",
    "text": "Conditions"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#conditions-consequences",
    "href": "lectures/2.3-Python_the_Basics.html#conditions-consequences",
    "title": "Python, the Basics",
    "section": "Conditions & Consequences",
    "text": "Conditions & Consequences\nNo matter how complex, conditions always ultimately evaluate to True or False.\nThe simplest condition only considers one outcome:\nif <condition is true>:\n    ...do something...\nBut you’ll often needs something a little more sophisticated:\nif <condition is true>:\n    ...do something...\nelif <some other condition is true>:\n    ...do something else...\nelse:\n    ...if no conditions are true...\n\n...code continues..."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#for-example",
    "href": "lectures/2.3-Python_the_Basics.html#for-example",
    "title": "Python, the Basics",
    "section": "For Example",
    "text": "For Example\nif x < y:\n  print(\"x is less than y\")\nelse:\n  print(\"x is not less than y\"\nOr:\nif x < y:\n  print(\"x is less than y\")\nelif x > y:\n  print(\"x is greater than y\")\nelse:\n  print(\"x equals y\")"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#conditional-syntax",
    "href": "lectures/2.3-Python_the_Basics.html#conditional-syntax",
    "title": "Python, the Basics",
    "section": "Conditional Syntax",
    "text": "Conditional Syntax\nThe most common sources of syntax errors in conditions are:\n\nIncorrect indenting;\nMissing colons on conditional code;\nUnbalanced parentheses;\nIncorrect logic."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#all-of-them-together-input",
    "href": "lectures/2.3-Python_the_Basics.html#all-of-them-together-input",
    "title": "Python, the Basics",
    "section": "All of Them Together (Input)!",
    "text": "All of Them Together (Input)!\nif hours >= 0:\nprint(\"Hours were worked.\")\nelse\n    print \"No hours were worked.\")\nAll four errors can be found here, can you spot them?"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#all-of-them-together-output",
    "href": "lectures/2.3-Python_the_Basics.html#all-of-them-together-output",
    "title": "Python, the Basics",
    "section": "All of Them Together (Output)!",
    "text": "All of Them Together (Output)!\nOutput from the Python interpreter:\n>>> if hours >= 0:\n... print(\"Hours were worked.\")\n  File \"<stdin>\", line 2\n    print(\"Hours were worked.\")\n    ^\nIndentationError: expected an indented block\n>>> else\n  File \"<stdin>\", line 1\n    else\n    ^\nSyntaxError: invalid syntax\n>>>     print \"No hours were worked.\")\n  File \"<stdin>\", line 1\n    print \"No hours were worked.\")\n    ^\nIndentationError: unexpected indent"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#thats-better",
    "href": "lectures/2.3-Python_the_Basics.html#thats-better",
    "title": "Python, the Basics",
    "section": "That’s Better!",
    "text": "That’s Better!\nNotice that it’s relatively straightforward to figure out the syntax errors, but the logical error is much less obvious. Over time, you become far more likely to make logical errors than syntactical ones.\nif hours >= 0:\n    print(\"Hours were worked.\")\nelse:\n    print(\"No hours were worked.\")"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#comment-your-code",
    "href": "lectures/2.3-Python_the_Basics.html#comment-your-code",
    "title": "Python, the Basics",
    "section": "Comment Your Code",
    "text": "Comment Your Code\n\nOne way to reduce the risk of logic errors is to comment your code!"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#make-your-life-easy-well-easier",
    "href": "lectures/2.3-Python_the_Basics.html#make-your-life-easy-well-easier",
    "title": "Python, the Basics",
    "section": "Make Your Life Easy (Well, Easier)",
    "text": "Make Your Life Easy (Well, Easier)\nAlways comment your code:\n\nSo that you know what is going on.\nSo that you know why it is going on.\nSo that others can read your code.\nTo help you plan your code\n\n\nYou are reminding your future self what your code was for and helping to give it structure (explaining==thinking!)."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#different-comment-styles",
    "href": "lectures/2.3-Python_the_Basics.html#different-comment-styles",
    "title": "Python, the Basics",
    "section": "Different comment styles",
    "text": "Different comment styles\n# This is a short comment\nprint(\"Foo\")\nprint(\"Bar\") # Also a short comment\n\n# You can have comments span multiple\n# lines just by adding more '#' at the \n# start of the line.\n\n# You can keep code from running\n# print(\"Baz\")"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#comments-should-follow-indentation",
    "href": "lectures/2.3-Python_the_Basics.html#comments-should-follow-indentation",
    "title": "Python, the Basics",
    "section": "Comments should follow indentation",
    "text": "Comments should follow indentation\n# Function for processing occupational data\n# from the 2001 and 2011 Censuses.\ndef occ_data(df):\n    #  Columns of interest in data set\n    occ = ['Managerial','Professional','Technical',\n           'Administrative','Skilled','Personal Service',\n           'Customer Service','Operators','Elementary']\n    \n    # Integrate results into single dataset -- \n    # right now we don't replicate Jordan's approach of\n    # grouping them into 'knowledge worker' and 'other'. \n    occ_data = pd.DataFrame()\n    ..."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#easier-multi-line-comments",
    "href": "lectures/2.3-Python_the_Basics.html#easier-multi-line-comments",
    "title": "Python, the Basics",
    "section": "Easier Multi-Line Comments",
    "text": "Easier Multi-Line Comments\nThe below are not real comments, but they can help when you have a really long comment that you want to make. They are also used to help explain what a function does (called a docstring).\n\"\"\"\nSo I was thinking that what we need here is \na way to handle the case where the data is\nincomplete or contains an observation that we\nweren't expecting (e.g. \"N/A\" instead of \"0\").\n\"\"\""
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#commenting-tips",
    "href": "lectures/2.3-Python_the_Basics.html#commenting-tips",
    "title": "Python, the Basics",
    "section": "Commenting Tips",
    "text": "Commenting Tips\nSome useful tips for commenting your code: - Include general information at the top of your programming file. - Assume the person reading the code is a coder themselves. - Good commenting is sparse in the sense that it is used judiciously, and concise without being gnomic. - Use comments to track the logic of your code (especially in conditionals and loops)"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#more-resources",
    "href": "lectures/2.3-Python_the_Basics.html#more-resources",
    "title": "Python, the Basics",
    "section": "More Resources",
    "text": "More Resources\nHere are some links to videos on LinkedIn Learning that might help, and YouTube will undoubtedly have lots more options and styles of learning… - Types of Data - Variables and expressions - Strings - The string type - Common string methods - Formatting strings - Splitting and joining - Numeric types - The bool type - Storing Data in Variables - Conditional structures - If Statements - If-Else Statements - If-Elif - Whitespace and comments - Using print() - Conditional syntax - Conditional operators - Conditional assignment"
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#assessments",
    "href": "lectures/1.1-Getting_Oriented.html#assessments",
    "title": "Getting Oriented",
    "section": "Assessments",
    "text": "Assessments\n\nData Audit (30% of module grade; 1,050 words max): A structured individual essay in which students are prompted to draw on the assigned readings to critically engage with the assigned data set. It should be seen as an opportunity to begin integrating the technical and theoretical elements of the module.\nData+Policy Briefing (50% of module grade; 2,500 words max): A two-part small-group submission in which students develop and write up an analysis of the assigned data set so as to inform public policy/political decision-making.\nIndividual Reflection (20% of module grade; XXX words max): A structured individual reflection in which students are asked to reflect on their experience of working as part of a data analytics team in order to identify behaviours and practices that helped/hindered the project.\n\n\n1 A Markdown document submitted in Week 6 (after Reading Week). The word limit for this submission is 1,050 words (including reproduction of the questions). The data set for which the ‘biography’ will be written is the Inside Airbnb data for London.\n2 A Jupyter notebook submitted at the start of Term 2. There are two parts to the notebook: a Policy Briefing and a Reproducible Analysis. The word limit for the Executive Briefing portion of the notebook is 2,500 words. There is no limit to the amount of code in the Reproducible Analysis portion of the notebook.\n3 A document submitted at the start of Term 2."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#where-does-fsds-fit",
    "href": "lectures/1.1-Getting_Oriented.html#where-does-fsds-fit",
    "title": "Getting Oriented",
    "section": "Where Does FSDS Fit?",
    "text": "Where Does FSDS Fit?\n\nGeographic Information Systems (GIS)\n\nFoundations of spatial analysis\nWorking with geo-data\n\nQuantitative Methods (QM)\n\nFoundations of statistical analysis\nWorking with data\n\nFoundations of Spatial Data Science (FSDS)\n\nFoundations of applied spatial and statistical analysis\nIntegrating and applying concepts from GIS & QM to a problem\nDeveloping programming and practical analysis skills\nSeeing the ‘data science’ pipeline from end to end"
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#what-are-we-trying-to-do",
    "href": "lectures/1.1-Getting_Oriented.html#what-are-we-trying-to-do",
    "title": "Getting Oriented",
    "section": "What Are We Trying to Do?",
    "text": "What Are We Trying to Do?\nThis class hopes to achieve four things:\n\nTo teach you the basics of how to code in Python.\nTo teach you the basics of how to think in Python.\nTo teach you how to engage with data critically.\nTo show you how the concepts taught across Term 1 can be applied to a practical (spatial) data analysis problem.\n\nThese skills are intended to be transferrable to post-degree employment or research."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#overall-structure",
    "href": "lectures/1.1-Getting_Oriented.html#overall-structure",
    "title": "Getting Oriented",
    "section": "Overall Structure",
    "text": "Overall Structure\n\nWeeks 1-4: baking in the ‘basics’\nWeeks 5-7: engaging with data.\nWeeks 8-10: making sense of data\n\n\n1-4 means tackling the ‘basics’ of Python, foundational concepts in programming, and practicing with the ‘tools of the trade’ for programmers.\n5-7 means different types of data (numeric, spatial and textual) with a view to understanding how such data can be cleaned, processed, and aggregated for use in a subsequent analysis. It is commonly held that 80% of ‘data science’ involves data cleaning, so this is a critical phase in developing an understanding of data.\n8-10 is about visualisation, classification, dimensionality reduction, and clustering. These concepts will have been encountered in other modules, so the intention is that the student will see how these fit into the ‘bigger picture’ of applied spatial analysis."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#docker-vagrant-virtualbox",
    "href": "lectures/1.2-Tools_of_the_Trade.html#docker-vagrant-virtualbox",
    "title": "Tools of the Trade",
    "section": "Docker, Vagrant & VirtualBox",
    "text": "Docker, Vagrant & VirtualBox\n+++  +++  +++"
  },
  {
    "objectID": "lectures/10.1-Classification.html#map-classification-choices",
    "href": "lectures/10.1-Classification.html#map-classification-choices",
    "title": "Classification",
    "section": "Map Classification Choices",
    "text": "Map Classification Choices\n\nAssign classes manually.\nSplit range evenly.\nSplit data evenly\nSplit data according to distribution\nSplit data according to their similarity to each other.\n\n\nAccording to some logic/theory/regulatory or policy fact or objective.\nEqual intervals for cases without heavy skew\nQuantiles or HeadTailBreaks for cases with heavy skew\nSD for cases with normal distribution; BoxPlot for others.\nNatural breaks/FIsher Jenks for cases where distribution is discontinuous"
  },
  {
    "objectID": "lectures/10.1-Classification.html#in-practice",
    "href": "lectures/10.1-Classification.html#in-practice",
    "title": "Classification",
    "section": "In Practice…",
    "text": "In Practice…"
  },
  {
    "objectID": "lectures/10.1-Classification.html#mapclassify",
    "href": "lectures/10.1-Classification.html#mapclassify",
    "title": "Classification",
    "section": "Mapclassify",
    "text": "Mapclassify\nMapclassify is part of the PySAL project and provides a wide range of classifiers:\n\n\n\nNo Parameters\nk Parameter\n\n\n\n\nBoxPlot\nUserDefined\n\n\nStdMean\nPercentiles\n\n\nMaxP\nQuantiles\n\n\nHeadTailBreaks\nNatural Breaks\n\n\nEqualInterval\nMaximum Breaks\n\n\n\nJenksCaspall/Sampled/Forced\n\n\n\nFisherJenks/Sampled"
  },
  {
    "objectID": "lectures/10.1-Classification.html#raw",
    "href": "lectures/10.1-Classification.html#raw",
    "title": "Classification",
    "section": "Raw",
    "text": "Raw"
  },
  {
    "objectID": "lectures/10.1-Classification.html#summary",
    "href": "lectures/10.1-Classification.html#summary",
    "title": "Classification",
    "section": "Summary",
    "text": "Summary\n\nThe choice of classification scheme should be data- and distribution-led. This is simply a demonstration of how wildly the different schemes can vary in their effects in terms of how you understand your data."
  },
  {
    "objectID": "lectures/10.1-Classification.html#code-useful-tips",
    "href": "lectures/10.1-Classification.html#code-useful-tips",
    "title": "Classification",
    "section": "Code (Useful Tips)",
    "text": "Code (Useful Tips)\nSetting up the classes:\nkl = 7\ncls = [mapclassify.BoxPlot, ...,  mapclassify.JenksCaspall]\nSetting up the loop:\nfor cl in cls:\n    try: \n        m = cl(ppd.Value, k=kl)\n    except TypeError:\n        m = cl(ppd.Value)\n    \n    f = plt.figure()\n    gs = f.add_gridspec(nrows=2, ncols=1, height_ratios=[1,4])\n\n    ax1 = f.add_subplot(gs[0,0])\n    ...\n\n    ax2 = f.add_subplot(gs[1,0])\n    ..."
  },
  {
    "objectID": "lectures/10.1-Classification.html#code-useful-tips-1",
    "href": "lectures/10.1-Classification.html#code-useful-tips-1",
    "title": "Classification",
    "section": "Code (Useful Tips)",
    "text": "Code (Useful Tips)\nSetting up the distribution:\n    ax1 = f.add_subplot(gs[0,0])\n    sns.kdeplot(ppd.Value, ax=ax1, color='r')\n    ax1.ticklabel_format(style='plain', axis='x') \n\n    y = ax1.get_ylim()[1]\n    for b in m.bins:\n        ax1.vlines(b, 0, y, linestyles='dotted')"
  },
  {
    "objectID": "lectures/10.1-Classification.html#code-useful-tips-2",
    "href": "lectures/10.1-Classification.html#code-useful-tips-2",
    "title": "Classification",
    "section": "Code (Useful Tips)",
    "text": "Code (Useful Tips)\nAdjusting the legend text:\ndef replace_legend_items(legend, mapping):\n    for txt in legend.texts:\n        for k,v in mapping.items():\n            if txt.get_text() == str(k):\n                txt.set_text(v)\nSetting up the map:\n    ax2 = f.add_subplot(gs[1,0])\n    ppd.assign(cl=m.yb).plot(column='cl', k=len(m.bins), categorical=True, legend=True, ax=ax2)\n    \n    mapping = dict([(i,s) for i,s in enumerate(m.get_legend_classes())])\n    ax2.set_axis_off()\n    replace_legend_items(ax2.get_legend(), mapping)"
  },
  {
    "objectID": "lectures/10.2-Clustering.html#worlds-first-geodemographic-classification",
    "href": "lectures/10.2-Clustering.html#worlds-first-geodemographic-classification",
    "title": "Clustering",
    "section": "World’s First Geodemographic Classification?",
    "text": "World’s First Geodemographic Classification?\n\nSource: booth.lse.ac.uk/map/"
  },
  {
    "objectID": "lectures/10.2-Clustering.html#more-than-100-years-later",
    "href": "lectures/10.2-Clustering.html#more-than-100-years-later",
    "title": "Clustering",
    "section": "More than 100 Years Later",
    "text": "More than 100 Years Later\n\nSource: vis.oobrien.com/booth/"
  },
  {
    "objectID": "lectures/10.2-Clustering.html#intimately-linked-to-rise-of-the-state",
    "href": "lectures/10.2-Clustering.html#intimately-linked-to-rise-of-the-state",
    "title": "Clustering",
    "section": "Intimately Linked to Rise of The State",
    "text": "Intimately Linked to Rise of The State\n\nGeodemographics only possible in context of a State – without a Census it simply wouldn’t work… until now?\nClearly tied to social and economic ‘control’ and intervention: regeneration, poverty & exclusion, crime, etc.\nPresumes that areas are the relevant unit of analysis; in geodemographics these are usually called neighbourhoods… which should ring a few bells.\nIn practice, we are in the realm of ‘homophily’, a.k.a. Tobler’s First Law of Geography"
  },
  {
    "objectID": "lectures/10.2-Clustering.html#where-is-it-used",
    "href": "lectures/10.2-Clustering.html#where-is-it-used",
    "title": "Clustering",
    "section": "Where is it used?",
    "text": "Where is it used?\nAnything involving grouping individuals, households, or areas into larger ‘groups’…\n\nStrategic marketing (above the line, targeted, etc.)\nRetail analysis (store location, demand modelling, etc.)\nPublic sector planning (resource allocation, service development, etc.)\n\nCould see it as a subset of customer segmentation."
  },
  {
    "objectID": "lectures/10.2-Clustering.html#computational-context",
    "href": "lectures/10.2-Clustering.html#computational-context",
    "title": "Clustering",
    "section": "Computational Context",
    "text": "Computational Context"
  },
  {
    "objectID": "lectures/10.2-Clustering.html#problem-domains",
    "href": "lectures/10.2-Clustering.html#problem-domains",
    "title": "Clustering",
    "section": "Problem Domains",
    "text": "Problem Domains\n\n\n\n\nContinuous\nCategorical\n\n\n\n\nSupervised\nRegression\nClassification\n\n\nUnsupervised\nDimensionality Reduction\nClustering\n\n\n\n\nIn classification we ‘know’ the answer (labels).\nIn clustering we don’t ‘know’ the answer (clusters)."
  },
  {
    "objectID": "lectures/10.2-Clustering.html#measuring-fit",
    "href": "lectures/10.2-Clustering.html#measuring-fit",
    "title": "Clustering",
    "section": "Measuring ‘Fit’",
    "text": "Measuring ‘Fit’\n\nUsually working towards an ‘objective criterion’ for quality… these are known as cohesion and separation measures."
  },
  {
    "objectID": "lectures/10.2-Clustering.html#how-your-data-looks",
    "href": "lectures/10.2-Clustering.html#how-your-data-looks",
    "title": "Clustering",
    "section": "How Your Data Looks…",
    "text": "How Your Data Looks…\nClustering is one area where standardisation (and, frequently, normalisation) are essential:\n\nYou don’t (normally) want scale in any one dimension to matter more than scale in another.\nYou don’t want differences between values in one dimension to matter more than differences in another.\nYou don’t want skew in one dimension to matter more than skew in another.\n\nYou also want uncorrelated variables… why?"
  },
  {
    "objectID": "lectures/10.2-Clustering.html#first-steps",
    "href": "lectures/10.2-Clustering.html#first-steps",
    "title": "Clustering",
    "section": "First Steps",
    "text": "First Steps\nYou will normally want a continuous variable… why?\nThese types of data are especially problematic:\n\nDummies / One-Hot Encoded\nCategorical / Ordinal\nPossible solutions: k-modes, CCA, etc."
  },
  {
    "objectID": "lectures/10.2-Clustering.html#typical-performance",
    "href": "lectures/10.2-Clustering.html#typical-performance",
    "title": "Clustering",
    "section": "Typical Performance",
    "text": "Typical Performance\nTypically about trade-offs between:\n\nAccuracy\nGeneralisation"
  },
  {
    "objectID": "lectures/10.2-Clustering.html#choosing-a-clustering-algorithm",
    "href": "lectures/10.2-Clustering.html#choosing-a-clustering-algorithm",
    "title": "Clustering",
    "section": "Choosing a Clustering Algorithm",
    "text": "Choosing a Clustering Algorithm\nNeed to consider trade-offs between:\n\nAbility to cluster at speed.\nAbility to replicate clustering results.\nAbility to cope with fuzzy/indeterminate boundaries.\nAbility to cope with curse of dimensionality.\nUnderlying representation of group membership…"
  },
  {
    "objectID": "lectures/10.2-Clustering.html#putting-it-all-into-context",
    "href": "lectures/10.2-Clustering.html#putting-it-all-into-context",
    "title": "Clustering",
    "section": "Putting it All into Context",
    "text": "Putting it All into Context"
  },
  {
    "objectID": "lectures/10.2-Clustering.html#inserting-geography",
    "href": "lectures/10.2-Clustering.html#inserting-geography",
    "title": "Clustering",
    "section": "Inserting Geography",
    "text": "Inserting Geography"
  },
  {
    "objectID": "lectures/10.2-Clustering.html#space-adds-complexity",
    "href": "lectures/10.2-Clustering.html#space-adds-complexity",
    "title": "Clustering",
    "section": "Space Adds Complexity",
    "text": "Space Adds Complexity\nWe now have to consider two types of clustering:\n\nWith respect to polygons: regions are built from adjacent zones that are more similar to one another than to other adjacent zones.\nWith respect to points: points are distributed in a way that indicates ‘clumping’ at particular scales.\n\n\nType 1 is probably what you were thinking of in terms of clustering.\nType 2 is point pattern analysis and should be considered a substantially different area of research and type of analysis."
  },
  {
    "objectID": "lectures/10.2-Clustering.html#the-trade-offs",
    "href": "lectures/10.2-Clustering.html#the-trade-offs",
    "title": "Clustering",
    "section": "The trade-offs…",
    "text": "The trade-offs…\nConsider:\n\nClustering algorithms are inherently spatial.\nClustering algorithms do not take space geography into account.\n\nDoes this matter?\n\nAll clustering algorithms are about inter-observation and intra-cluster distances so they have some conceptualisation of ‘space’.\nSpatially-aware clustering algorithms exist but are generally much more computationally-intensive than ‘regular ones’."
  },
  {
    "objectID": "lectures/10.2-Clustering.html#different-approaches",
    "href": "lectures/10.2-Clustering.html#different-approaches",
    "title": "Clustering",
    "section": "Different Approaches",
    "text": "Different Approaches\n\n\n\nAlgorithm\nPros\nCons\nGeographically Aware?\n\n\n\n\nk-Means\nFast. Deterministic.\nEvery observation to cluster.\nN.\n\n\nDBSCAN\nAllows for clusters and outliers.\nSlower. Choice of \\[\\epsilon\\] critical. Can end up with all outliers.\nN, but implicit in \\[\\epsilon\\].\n\n\nOPTICS\nFewer parameters than DBSCAN.\nEven slower.\nN, but implicit in \\[\\epsilon\\].\n\n\nHierarchical\nCan cut at any number of clusters.\nNo ‘ideal’ solution.\nY, with connectivity parameter\n\n\nADBSCAN\nScales. Confidence levels.\nMay need large data set to be useful. Choice of \\[\\epsilon\\] critical.\nY.\n\n\nMax-p\nCoherent regions returned.\nVery slow if model poorly specified.\nY."
  },
  {
    "objectID": "lectures/10.2-Clustering.html#setting-the-relevant-distance",
    "href": "lectures/10.2-Clustering.html#setting-the-relevant-distance",
    "title": "Clustering",
    "section": "Setting the Relevant Distance",
    "text": "Setting the Relevant Distance\nMany clustering algorithms rely on a distance specification (usually \\[\\epsilon\\]). So how do we set this threshold?\n\nIn high-dimensional spaces this threshold will need to be large.\nIn high-dimensional spaces the scale will be meaningless (i.e. not have a real-world meaning, only an abstract one).\nIn 2- or 3-dimensional (geographical) space this threshold could be meaningful (i.e. a value in metres could work)."
  },
  {
    "objectID": "lectures/10.2-Clustering.html#choosing-a-distance-metric",
    "href": "lectures/10.2-Clustering.html#choosing-a-distance-metric",
    "title": "Clustering",
    "section": "Choosing a Distance Metric",
    "text": "Choosing a Distance Metric\n\n\n\nn-Dimensions\nHow to Set\nExamples\n\n\n\n\n2 or 3\nTheory/Empirical Data\nWalking speed; Commute distance\n\n\n2 or 3\nK/L Measures\nPlot with Simulation for CIs to identify significant ‘knees’.\n\n\n3\nMarked Point Pattern?\n\n\n\n> 3\nkNN\nCalculate average kNN distance based on some expectation of connectivity.\n\n\n\n\nRemember: inter-observation distance increases with dimensionality!"
  },
  {
    "objectID": "lectures/10.2-Clustering.html#so-whats-the-best-way",
    "href": "lectures/10.2-Clustering.html#so-whats-the-best-way",
    "title": "Clustering",
    "section": "So What’s the Best Way?",
    "text": "So What’s the Best Way?\n\nThere is no one best way, it’s all about judgement."
  },
  {
    "objectID": "lectures/10.2-Clustering.html#geodemographics-as-a-business",
    "href": "lectures/10.2-Clustering.html#geodemographics-as-a-business",
    "title": "Clustering",
    "section": "Geodemographics as a Business",
    "text": "Geodemographics as a Business"
  },
  {
    "objectID": "lectures/10.2-Clustering.html#experian",
    "href": "lectures/10.2-Clustering.html#experian",
    "title": "Clustering",
    "section": "Experian",
    "text": "Experian\nSpecialist in consumer segmentation and geodemographics (bit.ly/2jMRhAW).\n\nMarket cap: £14.3 billion.\nMosaic: “synthesises of 850 million pieces of information… to create a segmentation that allocates 49 million individuals and 26 million households into one of 15 Groups and 66 detailed Types.”\nMore than 450 variables used.\n\nMost retail companies will have their own segmentation scheme. Competitors: CACI, Nielsen, etc."
  },
  {
    "objectID": "lectures/10.2-Clustering.html#experian-groups",
    "href": "lectures/10.2-Clustering.html#experian-groups",
    "title": "Clustering",
    "section": "Experian Groups",
    "text": "Experian Groups"
  },
  {
    "objectID": "lectures/10.2-Clustering.html#experian-mapping",
    "href": "lectures/10.2-Clustering.html#experian-mapping",
    "title": "Clustering",
    "section": "Experian Mapping",
    "text": "Experian Mapping"
  },
  {
    "objectID": "lectures/10.2-Clustering.html#output-area-classification",
    "href": "lectures/10.2-Clustering.html#output-area-classification",
    "title": "Clustering",
    "section": "Output Area Classification",
    "text": "Output Area Classification\nOAC set up as ‘open source’ alternative to Mosaic:\n\nWell documented (UCL Geography a major contributor)\nDoesn’t require a license or payment\nCan be tweaked/extended/reweighted by users as needed"
  },
  {
    "objectID": "lectures/2.1-Computers_in_Planning_and_Geography.html#wave-1",
    "href": "lectures/2.1-Computers_in_Planning_and_Geography.html#wave-1",
    "title": "Computers in Urban Studies",
    "section": "Wave 1",
    "text": "Wave 1\n“A computer in every institution”:\n\nRoughly the 1950s–70s\nComputers as ‘super-human’ calculators for data\nData + models as theory-testing tool\n\nRetrospectively: the 1st quantitative revolution.\n\nMany see this as incorrect and focus on the theoretical aspect."
  },
  {
    "objectID": "lectures/2.1-Computers_in_Planning_and_Geography.html#wave-2",
    "href": "lectures/2.1-Computers_in_Planning_and_Geography.html#wave-2",
    "title": "Computers in Urban Studies",
    "section": "Wave 2",
    "text": "Wave 2\n“A computer in every office”:\n\nRoughly the 1980s–2000s\nComputers as tools for thinking about spatial relationships\nExplicit modelling of local spatial effects\n\nRetrospectively: the GIS revolution.\n\nI personally see this as incorrect because GIS is Wave 1."
  },
  {
    "objectID": "lectures/2.1-Computers_in_Planning_and_Geography.html#wave-3",
    "href": "lectures/2.1-Computers_in_Planning_and_Geography.html#wave-3",
    "title": "Computers in Urban Studies",
    "section": "Wave 3",
    "text": "Wave 3\n“A computer in every thing”:\n\nRoughly the mid-2000s–?\nComputers as tools for generating data (pace ABM researchers)\nGeodata being continuously produced as byproduct of other activities\nShift from researching attributes to behaviours (pace Hägerstrand)\n\nRetrospectively: the big data revolution or 2nd quantitative revolution.\n\nShift from computers as processors of data to integrated, pervasive systems that spew out data on everything."
  },
  {
    "objectID": "lectures/2.1-Computers_in_Planning_and_Geography.html#all-waves-still-going",
    "href": "lectures/2.1-Computers_in_Planning_and_Geography.html#all-waves-still-going",
    "title": "Computers in Urban Studies",
    "section": "All Waves Still Going!",
    "text": "All Waves Still Going!\nWave 1: Computers help me do it faster\n\nGIS is ‘just’ the industrial revolution impacting cartography.\n\nWave 2: Computers help me to think\n\nGeocomputation & local stats are qualitatively & meaningfully different.\n\nWave 3: Computers help me to learn\n\nNot ‘just’ about the ‘bigness’ of data, though that is important.\n\n\nWave 2 is about implementing ideas such as recursion and iteration – these could, in theory, have been tackled in Wave 1, but in practice that’s not what people were doing.\nWave 3 is about more explicitly allowing computers to learn about data so that we can extract insight from these models – these could also, in theory, have been tackled in Wave 2 but in practice that’s not what people were doing.\nI’m not totally happy about my description of Wave 3 and will try to dig into this in a little more detail but suggestions welcome!"
  },
  {
    "objectID": "lectures/2.1-Computers_in_Planning_and_Geography.html#anticipated-by-hägerstrand-1967",
    "href": "lectures/2.1-Computers_in_Planning_and_Geography.html#anticipated-by-hägerstrand-1967",
    "title": "Computers in Urban Studies",
    "section": "Anticipated by Hägerstrand (1967)",
    "text": "Anticipated by Hägerstrand (1967)\n\nI think that the computer can do three different and useful things for us. The first and simplest operation is… descriptive mapping the second… is the analytical one The third kind of service is… to run process models by which we might try to reproduce observed or create hypothetical chains of events of a geographical nature.\nT. Hägerstrand (1967), ‘The Computer and the Geographer’, TiBG"
  },
  {
    "objectID": "lectures/2.1-Computers_in_Planning_and_Geography.html#but-persistent-critiques",
    "href": "lectures/2.1-Computers_in_Planning_and_Geography.html#but-persistent-critiques",
    "title": "Computers in Urban Studies",
    "section": "But Persistent Critiques",
    "text": "But Persistent Critiques\n\nThere is a clear disparity between the sophisticated theoretical and methodological framework which we are using and our ability to say anything really meaningful about events as they unfold around us. There are too many anomalies… There is an ecological problem, an urban problem… and yet we seem incapable of saying anything of any depth or profundity about any of them. When we do say something it appears trite and rather ludicrous.\nHarvey (1972, p.6)"
  },
  {
    "objectID": "lectures/2.1-Computers_in_Planning_and_Geography.html#what-is-different",
    "href": "lectures/2.1-Computers_in_Planning_and_Geography.html#what-is-different",
    "title": "Computers in Urban Studies",
    "section": "What is Different?",
    "text": "What is Different?\nAccording to Donoho (2017) ‘data science’ differs from plain old ‘statistics’ through an interest in:\n\nData gathering, preparation, and exploration;\nData representation and transformation;\nComputing with data;\nData visualisation and presentation;\nData modelling; and\nA reflexive ‘science of data science’."
  },
  {
    "objectID": "lectures/2.1-Computers_in_Planning_and_Geography.html#in-practice",
    "href": "lectures/2.1-Computers_in_Planning_and_Geography.html#in-practice",
    "title": "Computers in Urban Studies",
    "section": "In Practice…",
    "text": "In Practice…\nI think there are several distinguishing features that I encounter in day-to-day (geography) work:\n\nData-driven methods development & deployment\nExplicit tuning/meta-parameterisation\nExplicit feature optimisation/engineering\nExplicit training/testing from ‘one shot’ data\n‘Black boxes’ feature prominently & ‘online learning’ emerging quickly\n\nData science as process and pipeline, not just input to research."
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#what-about-computational-thinking",
    "href": "lectures/2.2-Principles_of_Programming.html#what-about-computational-thinking",
    "title": "Principles of Programming",
    "section": "What About Computational Thinking?",
    "text": "What About Computational Thinking?\n\nComputational thinking is not thinking like a Computer Scientist. It is about recognising how to code can help us to understand, and manipulate, the world."
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#key-features",
    "href": "lectures/2.2-Principles_of_Programming.html#key-features",
    "title": "Principles of Programming",
    "section": "Key Features",
    "text": "Key Features\nAspects of computational thinking include:\n\nRecognising how one problem connects to other problems.\nRecognising when and how to make things simpler and faster\nRecognising how different ways of tackling a problem gives you power to tackle new problems.\n\nSee this keynote by Lorena Barba (2014); esp. from 52:00 onwards.\n\nYou already do a lot of this when you generalise from your readings to your ideas/understanding!"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#why-are-these-virtues",
    "href": "lectures/2.2-Principles_of_Programming.html#why-are-these-virtues",
    "title": "Principles of Programming",
    "section": "Why Are These Virtues?",
    "text": "Why Are These Virtues?\nAccording to Larry Wall the three virtues of the programmer are:\n\nLaziness\nImpatience\nHubris\n\nThese are not to be taken literally (see Larry Wall’s “Three Virtues of a Programmer” are Utter Bull💩).\n\nAutomate the boring stuff, focus on the interesting bits! And it’s not about quantity of code, it’s about quality.\nUse code to save time, but don’t just jump head-first into problems.\nWhen something isn’t working well you want to make it work better/faster/more efficiently…"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#four-quotes-to-remember",
    "href": "lectures/2.2-Principles_of_Programming.html#four-quotes-to-remember",
    "title": "Principles of Programming",
    "section": "Four Quotes to Remember",
    "text": "Four Quotes to Remember\n\nComputers make very fast, very accurate mistakes.\nA computer program does what you tell it to do, not what you want it to do.\nOnly half of programming is coding. The other 90% is debugging.\nWeeks of coding can save you hours of planning."
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#and-one-more",
    "href": "lectures/2.2-Principles_of_Programming.html#and-one-more",
    "title": "Principles of Programming",
    "section": "And One More…",
    "text": "And One More…\n\nExperience is the name everyone gives to their mistakes.\nOscar Wilde"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#algorithm-as-recipe",
    "href": "lectures/2.2-Principles_of_Programming.html#algorithm-as-recipe",
    "title": "Principles of Programming",
    "section": "Algorithm as Recipe…",
    "text": "Algorithm as Recipe…"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#following-a-recipe-is-easy-right",
    "href": "lectures/2.2-Principles_of_Programming.html#following-a-recipe-is-easy-right",
    "title": "Principles of Programming",
    "section": "Following a Recipe is Easy, Right?",
    "text": "Following a Recipe is Easy, Right?\n\nSource"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#calculating-the-mean",
    "href": "lectures/2.2-Principles_of_Programming.html#calculating-the-mean",
    "title": "Principles of Programming",
    "section": "Calculating the Mean",
    "text": "Calculating the Mean\nGiven these numbers, what’s the average?\n1, 4, 7, 6, 4, 2"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#as-a-recipe",
    "href": "lectures/2.2-Principles_of_Programming.html#as-a-recipe",
    "title": "Principles of Programming",
    "section": "As a Recipe",
    "text": "As a Recipe\n\nTake a list of numbers\nStart a count of numbers in the list at 0\nStart a sum of numbers in the list at 0\nTake a number from the list:\n\nAdd 1 to the count\nAdd the value of the number to the sum\n\nRepeat step #4 until no numbers are left in the list.\nDivide the sum by the count\nReport this number back to the user"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#as-python",
    "href": "lectures/2.2-Principles_of_Programming.html#as-python",
    "title": "Principles of Programming",
    "section": "As Python",
    "text": "As Python\nnumbers = [1, 4, 7, 6, 4, 2]\ntotal   = 0\ncount   = 0\nfor num in numbers:\n  total = total + num \n  count = count + 1\nmean = total / count\nprint(mean)"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#why-we-still-havent-solved-it",
    "href": "lectures/2.2-Principles_of_Programming.html#why-we-still-havent-solved-it",
    "title": "Principles of Programming",
    "section": "Why We Still Haven’t ‘Solved It’",
    "text": "Why We Still Haven’t ‘Solved It’\n\nSource"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#languages",
    "href": "lectures/2.2-Principles_of_Programming.html#languages",
    "title": "Principles of Programming",
    "section": "Languages",
    "text": "Languages\nComputer languages come with all of the ‘baggage’ of human languages; they have:\n\nA vocabulary (reserved words)\nA grammar (syntax)\nRules about the kinds of things you can say (grammar)\nStyles and idiosyncrasies all their own (history)\n\nIn this module we will use the Python programming language. We could also teach this same content in R."
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#python",
    "href": "lectures/2.2-Principles_of_Programming.html#python",
    "title": "Principles of Programming",
    "section": "Python",
    "text": "Python\nnumbers = [1, 4, 7, 6, 4, 2]\ntotal   = 0\ncount   = 0\nfor num in numbers:\n  total = total + num \n  count += 1 # An alternative to count = count + 1\nmean = total / count\nprint(mean)"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#r",
    "href": "lectures/2.2-Principles_of_Programming.html#r",
    "title": "Principles of Programming",
    "section": "R",
    "text": "R\nnumbers = c(1, 4, 7, 6, 4, 2)\ntotal   = 0\ncount   = 0\nfor (num in numbers) {\n  total = total + num\n  count = count + 1\n}\nmean = total / count\nprint(mean)"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#finally-style",
    "href": "lectures/2.2-Principles_of_Programming.html#finally-style",
    "title": "Principles of Programming",
    "section": "Finally: Style",
    "text": "Finally: Style\nAlthough all programmers develop their own style (see: writing in any language), Python encourages coders to use a consistent style so that others can pick up your code and make sense of what’s going on (see: comments!).\nTwo useful resources:\n\nThe Hitchhiker’s Guide to Python\nA summary of Python code style conventions"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-bak.html#variables-have-types",
    "href": "lectures/2.3-Python_the_Basics-bak.html#variables-have-types",
    "title": "Python, the Basics",
    "section": "Variables Have Types",
    "text": "Variables Have Types\n\n\n\n\n\n\n\n\nName\nValue\nType\n\n\n\n\nmsg\n‘Hello world’\ntype(msg)==str\n\n\nanswer\n42\ntype(answer)==int\n\n\npi\n3.14159\ntype(pi)==float\n\n\nc\ncomplex(5,2)\ntype(c)==complex\n\n\ncorrect\nTrue\ntype(correct)==bool\n\n\n\n\nAs we’ll see in Week 4, everything is also an object."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-bak.html#we-can-change-those-types",
    "href": "lectures/2.3-Python_the_Basics-bak.html#we-can-change-those-types",
    "title": "Python, the Basics",
    "section": "We Can Change Those Types",
    "text": "We Can Change Those Types\nMessage starts as a string:\nmsg = '42'\ntype(msg)\nBut we can change it to an integer like this:\nmsg = int(msg)\ntype(msg)\nAnd back to a string:\nmsg = str(msg)\ntype(msg)"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-bak.html#variables-have-names",
    "href": "lectures/2.3-Python_the_Basics-bak.html#variables-have-names",
    "title": "Python, the Basics",
    "section": "Variables Have Names",
    "text": "Variables Have Names\nRules for variable names:\n\nThey cannot start with a number (e.g. 3items is invalid)\nWhite space and symbols (e.g. $, +, etc.) are not allowed, but _ (underscore) is allowed (e.g. my_variable is fine)\nCase matters: myVar is different from both myvar and MYVAR\nBe consistent: my_var is more ‘Pythonic’, though myVar is also widely used, but don’t mix these!\nVariable names should be long enough to be clear but not so long as to be impractical: bldg_height vs. bh vs. max_building_height_at_eaves."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-bak.html#we-cant-use-these-names",
    "href": "lectures/2.3-Python_the_Basics-bak.html#we-cant-use-these-names",
    "title": "Python, the Basics",
    "section": "We Can’t Use These Names",
    "text": "We Can’t Use These Names\nYou should not try to use any of these words as a variable. Python may not complain, but strange things will happen when you run your code.\n\n\n\nand\ndel\nfrom\nnot\nwhile\n\n\nas\nelif\nglobal\nor\nwith\n\n\nassert\nelse\nif\npass\nyield\n\n\nbreak\nexcept\nimport\nprint\n\n\n\nclass\nexec\nin\nraise\n\n\n\ncontinue\nfinally\nis\nreturn\n\n\n\ndef\nfor\nlambda\ntry"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-bak.html#simple-operations-on-variables",
    "href": "lectures/2.3-Python_the_Basics-bak.html#simple-operations-on-variables",
    "title": "Python, the Basics",
    "section": "Simple Operations on Variables",
    "text": "Simple Operations on Variables\nLet’s start with x=10 and y=5…\n\n\n\nOperator\nInput\nResult\n\n\n\n\nSum\nx + y\n15\n\n\nDifference\nx - y\n5\n\n\nProduct\nx * y\n50\n\n\nQuotient\nx / y\n2.0\n\n\nFloored Quotient\nx // y\n2\n\n\nRemainder\nx % y\n0\n\n\nPower\npow(x,y) or x**y\n100000\n\n\nEquality\nx==y\nFalse"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-bak.html#strings-are-different",
    "href": "lectures/2.3-Python_the_Basics-bak.html#strings-are-different",
    "title": "Python, the Basics",
    "section": "Strings Are Different",
    "text": "Strings Are Different\nWhen you do things with strings the answers can look a little different…\nLet’s start with x=\"Hello\" and y=\"You\" and z=2…\n\n\n\nOperator\nInput\nResult\n\n\n\n\nSum\nx + y\n'HelloYou'\n\n\nDifference\nx - y\nTypeError\n\n\nProduct\nx * z\nHelloHello\n\n\nEquality\nx==y\nFalse"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-bak.html#using-strings-to-output-information",
    "href": "lectures/2.3-Python_the_Basics-bak.html#using-strings-to-output-information",
    "title": "Python, the Basics",
    "section": "Using Strings to Output Information",
    "text": "Using Strings to Output Information\nObviously, a really common requirement that programmers have is ‘output a nicely formatted string containing information about the variables in my program’.\nOver time, Python has acquired no fewer than three ways to do this: 1) string concatenation using +; 2) string formatting using <str>.format(<variables>); and 3) the new f-strings using f”{variable_1}… {variable_n}“. There are pros and cons to each:\nx = 24\ny = 'Something'\nprint(\"The value of \" + y + \" is \" + str(x))\nprint(\"The value of {0} is {1}\".format(y, x))\nprint(f\"The value of {y} is {x}\")\n\nI rather like f-strings because they can actually contain any code you like (you could, for instance, write `f”The square root of {y} is {x**(1/2)}” and it would work. However, concatenation is the easiest to learn."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-bak.html#operators-assemble",
    "href": "lectures/2.3-Python_the_Basics-bak.html#operators-assemble",
    "title": "Python, the Basics",
    "section": "Operators Assemble!",
    "text": "Operators Assemble!\nOperators follow clear rules about precedence: what calculations are performed in what order. For instance, the following lines of code are not the same:\nx + y * 2         # == 20\n(x + y) * 2       # == 30\nx + y * 2 / 3     # == 13.3333333333334\nx + y * (2/3)     # == 13.3333333333332\n(x + y) * (2/3)   # == 10.0\nAnd here’s a subtle one:\n(x * y) ** 2/3    # == 833.333333333334\n(x * y) ** (2/3)  # == 13.5720880829745\nThe full list is here.\n\nIf you’re a little rusty on exponents, that last example is the cube root of (x*y)**2. So x**(1/2) * x**(1/2) == x).\nAlso notice that with floats you do not always get the same result from operations that should give the same answer. This is because non-terminating decimals (e.g. 1/3) will always be rounded by the computer because it doesn’t have infinite memory. The process of rounding means that you need to be very careful comparing (more on this later) floats."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-bak.html#comparing-variables",
    "href": "lectures/2.3-Python_the_Basics-bak.html#comparing-variables",
    "title": "Python, the Basics",
    "section": "Comparing Variables",
    "text": "Comparing Variables\nWhen variables are of the same type then comparing them is easy. Let’s go back to x=10 and y=5…\n\n\n\nOperator\nInput\nResult\n\n\n\n\n==\nx == y\nFalse\n\n\n!=\nx != y\nTrue\n\n\n<, <=\nx < y\nFalse\n\n\n>, >=\nx > y\nTrue\n\n\n\nBut notice, if x='4a', y='365', and z='42':\nx > y   # True\nx > z   # True\n\nNotice that the result of any (successful) comparison is a Boolean (True/False). We can save the output of this comparison to a new variable (e.g. z = x > y).\nThis last example has to do with the way that strings are compared."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-bak.html#danger-will-robinson",
    "href": "lectures/2.3-Python_the_Basics-bak.html#danger-will-robinson",
    "title": "Python, the Basics",
    "section": "Danger, Will Robinson!",
    "text": "Danger, Will Robinson!\nNotice the very subtle difference between = and ==!\nConfusing these two operators is the most common source of mistakes early on when learning to code in Python! One (=) does assignment, the other (==) does comparison.\nx = 5\nz = 10\nx = z   # Probably a mistake: setting x to the value of z\nx == z  # True, because x and z are now both set to the value of 10\nRemember this!"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-bak.html#forcing-a-change-in-type",
    "href": "lectures/2.3-Python_the_Basics-bak.html#forcing-a-change-in-type",
    "title": "Python, the Basics",
    "section": "Forcing a Change in Type",
    "text": "Forcing a Change in Type\nA really common mistake is to think that string (str) \"42\" is the same as the integer (int) 42. Here’s the output from some attempts at comparison:\nx='42'\ny=42\nx==y   # False\nx>y\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: '>' not supported between instances of 'str' and 'int'\nIf we want to compare them then we’ll need to change their type:\nx > str(y)  # False\nint(x) <= y # True\nx >= str(y) # Also True\n\nNotice that in the first example we can say that 42 is clearly not the same as ‘42’, but we can’t say whether it’s more or less because that’s non-sensical in this context. So this is a computer being totally logical but not always sensible.\nAlso notice the syntax for this: we have str(<something>) and int(<something>) to convert between types. These are functions, which we’ll spend a lot more time on next week!\nWhy might it be (fractionally) faster to compare integers than strings?"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-bak.html#conditions-consequences",
    "href": "lectures/2.3-Python_the_Basics-bak.html#conditions-consequences",
    "title": "Python, the Basics",
    "section": "Conditions & Consequences",
    "text": "Conditions & Consequences\nNo matter how complex, conditions always ultimately evaluate to True or False.\nThe simplest condition only considers one outcome:\nif <condition is true>:\n    ...do something...\nBut you’ll often needs something a little more sophisticated:\nif <condition is true>:\n    ...do something...\nelif <some other condition is true>:\n    ...do something else...\nelse:\n    ...if no conditions are true...\n\n...code continues..."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-bak.html#for-example",
    "href": "lectures/2.3-Python_the_Basics-bak.html#for-example",
    "title": "Python, the Basics",
    "section": "For Example",
    "text": "For Example\nif x < y:\n  print(\"x is less than y\")\nelse:\n  print(\"x is not less than y\"\nOr:\nif x < y:\n  print(\"x is less than y\")\nelif x > y:\n  print(\"x is greater than y\")\nelse:\n  print(\"x equals y\")"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-bak.html#conditional-syntax",
    "href": "lectures/2.3-Python_the_Basics-bak.html#conditional-syntax",
    "title": "Python, the Basics",
    "section": "Conditional Syntax",
    "text": "Conditional Syntax\nThe most common sources of syntax errors in conditions are:\n\nIncorrect indenting;\nMissing colons on conditional code;\nUnbalanced parentheses;\nIncorrect logic."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-bak.html#all-of-them-together-input",
    "href": "lectures/2.3-Python_the_Basics-bak.html#all-of-them-together-input",
    "title": "Python, the Basics",
    "section": "All of Them Together (Input)!",
    "text": "All of Them Together (Input)!\nif hours >= 0:\nprint(\"Hours were worked.\")\nelse\n    print \"No hours were worked.\")\nAll four errors can be found here, can you spot them?"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-bak.html#all-of-them-together-output",
    "href": "lectures/2.3-Python_the_Basics-bak.html#all-of-them-together-output",
    "title": "Python, the Basics",
    "section": "All of Them Together (Output)!",
    "text": "All of Them Together (Output)!\nOutput from the Python interpreter:\n>>> if hours >= 0:\n... print(\"Hours were worked.\")\n  File \"<stdin>\", line 2\n    print(\"Hours were worked.\")\n    ^\nIndentationError: expected an indented block\n>>> else\n  File \"<stdin>\", line 1\n    else\n    ^\nSyntaxError: invalid syntax\n>>>     print \"No hours were worked.\")\n  File \"<stdin>\", line 1\n    print \"No hours were worked.\")\n    ^\nIndentationError: unexpected indent"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-bak.html#thats-better",
    "href": "lectures/2.3-Python_the_Basics-bak.html#thats-better",
    "title": "Python, the Basics",
    "section": "That’s Better!",
    "text": "That’s Better!\nNotice that it’s relatively straightforward to figure out the syntax errors, but the logical error is much less obvious. Over time, you become far more likely to make logical errors than syntactical ones.\nif hours >= 0:\n    print(\"Hours were worked.\")\nelse:\n    print(\"No hours were worked.\")"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-bak.html#make-your-life-easy-well-easier",
    "href": "lectures/2.3-Python_the_Basics-bak.html#make-your-life-easy-well-easier",
    "title": "Python, the Basics",
    "section": "Make Your Life Easy (Well, Easier)",
    "text": "Make Your Life Easy (Well, Easier)\nAlways comment your code:\n\nSo that you know what is going on.\nSo that you know why it is going on.\nSo that others can read your code.\nTo help you plan your code\n\n\nYou are reminding your future self what your code was for and helping to give it structure (explaining==thinking!)."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-bak.html#different-comment-styles",
    "href": "lectures/2.3-Python_the_Basics-bak.html#different-comment-styles",
    "title": "Python, the Basics",
    "section": "Different comment styles",
    "text": "Different comment styles\n# This is a short comment\nprint(\"Foo\")\nprint(\"Bar\") # Also a short comment\n\n# You can have comments span multiple\n# lines just by adding more '#' at the \n# start of the line.\n\n# You can keep code from running\n# print(\"Baz\")"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-bak.html#comments-should-follow-indentation",
    "href": "lectures/2.3-Python_the_Basics-bak.html#comments-should-follow-indentation",
    "title": "Python, the Basics",
    "section": "Comments should follow indentation",
    "text": "Comments should follow indentation\n# Function for processing occupational data\n# from the 2001 and 2011 Censuses.\ndef occ_data(df):\n    #  Columns of interest in data set\n    occ = ['Managerial','Professional','Technical',\n           'Administrative','Skilled','Personal Service',\n           'Customer Service','Operators','Elementary']\n    \n    # Integrate results into single dataset -- \n    # right now we don't replicate Jordan's approach of\n    # grouping them into 'knowledge worker' and 'other'. \n    occ_data = pd.DataFrame()\n    ..."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-bak.html#easier-multi-line-comments",
    "href": "lectures/2.3-Python_the_Basics-bak.html#easier-multi-line-comments",
    "title": "Python, the Basics",
    "section": "Easier Multi-Line Comments",
    "text": "Easier Multi-Line Comments\nThe below are not real comments, but they can help when you have a really long comment that you want to make. They are also used to help explain what a function does (called a docstring).\n\"\"\"\nSo I was thinking that what we need here is \na way to handle the case where the data is\nincomplete or contains an observation that we\nweren't expecting (e.g. \"N/A\" instead of \"0\").\n\"\"\""
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-bak.html#commenting-tips",
    "href": "lectures/2.3-Python_the_Basics-bak.html#commenting-tips",
    "title": "Python, the Basics",
    "section": "Commenting Tips",
    "text": "Commenting Tips\nSome useful tips for commenting your code: - Include general information at the top of your programming file. - Assume the person reading the code is a coder themselves. - Good commenting is sparse in the sense that it is used judiciously, and concise without being gnomic. - Use comments to track the logic of your code (especially in conditionals and loops)"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-bak.html#more-resources",
    "href": "lectures/2.3-Python_the_Basics-bak.html#more-resources",
    "title": "Python, the Basics",
    "section": "More Resources",
    "text": "More Resources\nHere are some links to videos on LinkedIn Learning that might help, and YouTube will undoubtedly have lots more options and styles of learning… - Types of Data - Variables and expressions - Strings - The string type - Common string methods - Formatting strings - Splitting and joining - Numeric types - The bool type - Storing Data in Variables - Conditional structures - If Statements - If-Else Statements - If-Elif - Whitespace and comments - Using print() - Conditional syntax - Conditional operators - Conditional assignment"
  },
  {
    "objectID": "lectures/2.4-Lists.html#whats-in-a-list-part-2",
    "href": "lectures/2.4-Lists.html#whats-in-a-list-part-2",
    "title": "Lists",
    "section": "What’s in a List? (Part 2)",
    "text": "What’s in a List? (Part 2)\nIn fact, when I say lists can hold many types of data, I should have said that they can hold any type of data:\nx = 3\ny = \"Foo\"\nz = [\"A\", \"list\", 42]\n\na = [x, y, z]\nThe output of a is:\n[3, 'Foo', ['A', 'list', 42]]\n\nWe’re going to come back to this a lot later, but for now notice that a list can hold lists!"
  },
  {
    "objectID": "lectures/2.4-Lists.html#accessing-lists",
    "href": "lectures/2.4-Lists.html#accessing-lists",
    "title": "Lists",
    "section": "Accessing Lists",
    "text": "Accessing Lists\n\nWhy is a Python list like a lift in Europe, and not like an elevator in America?"
  },
  {
    "objectID": "lectures/2.4-Lists.html#using-list-indexes",
    "href": "lectures/2.4-Lists.html#using-list-indexes",
    "title": "Lists",
    "section": "Using List Indexes",
    "text": "Using List Indexes\nWorked example:\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\nLists are ‘indexed’ numerically from the zero-th element:\n\n\n\nMassey ^1\nHarvey ^2\nRose ^3\n\n\n\n\n0\n1\n2\n\n\n\n>>> print(geographers[1])\nHarvey\n>>> print(geographers[2])\nRose\n>>> print(geographers[3])\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nIndexError: list index out of range\n\nAnd notice this error: Python tells you waht the problem is. The issue is understanding what the message means if you don’t know the vocabulary."
  },
  {
    "objectID": "lectures/2.4-Lists.html#interpolation",
    "href": "lectures/2.4-Lists.html#interpolation",
    "title": "Lists",
    "section": "Interpolation",
    "text": "Interpolation\nWe can also use variables as list indexes:\n\n\n\nMassey\nHarvey\nRose\n\n\n\n\n0\n1\n2\n\n\n\n>>> i = 0\n>>> print(geographers[i])\nMassey\nAnything that evaluates (i.e. resolves) to a number can be used as an index:\n>>> i = 1\n>>> print(geographers[i+1])\nRose\n>>> print(geographers[ (i-2+1)*2 ])\nMassey"
  },
  {
    "objectID": "lectures/2.4-Lists.html#countdown",
    "href": "lectures/2.4-Lists.html#countdown",
    "title": "Lists",
    "section": "Countdown!",
    "text": "Countdown!\nWe can ‘count’ backwards from the end of the list using negative numbers:\n>>> print( geographers[-1] )\nRose\n>>> print( geographers[-2] )\nHarvey"
  },
  {
    "objectID": "lectures/2.4-Lists.html#does-not-compute",
    "href": "lectures/2.4-Lists.html#does-not-compute",
    "title": "Lists",
    "section": "Does Not Compute!",
    "text": "Does Not Compute!\nErrors can be scary… but informative\n>>> print( geographers[5] )\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nIndexError: list index out of range\n\n>>> print( geographers[1.25] )\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: list indices must be integers or slices, not float\nAlthough there is quite a lot of output for such a simple mistake, notice that Python is giving us important hints about the source of the problem:\n\nIndexError: list index out of range\nTypeError: list indices must be integers or slices, not float"
  },
  {
    "objectID": "lectures/2.4-Lists.html#slicing-lists",
    "href": "lectures/2.4-Lists.html#slicing-lists",
    "title": "Lists",
    "section": "Slicing Lists",
    "text": "Slicing Lists\nYou can access more than one element at a time using a slice:\n>>> print( geographers[0:2] )\n['Massey','Harvey']\n>>> print( geographers[1:2] )\n['Harvey']\n>>> print( geographers[1:] )\n['Harvey', 'Rose']\n>>> print( geographers[-2:] )\n['Harvey', 'Rose']\nThe syntax for a slice is: list[ <start_idx>, <end_idx> ] What can be confusing is that the end index (end_idx) is not included in the slice. This is why [1:2] only returns a slice with one element.\n\nIt’s really subtle, but notice that a slice always returns a list, even if it’s just a list containing one thing. So geographers[1]=='Harvey' but geographers[1:2]==['Harvey']. Not the same thing!"
  },
  {
    "objectID": "lectures/2.4-Lists.html#test-yourself",
    "href": "lectures/2.4-Lists.html#test-yourself",
    "title": "Lists",
    "section": "Test Yourself",
    "text": "Test Yourself\nWhat do you think this will produce?\n>>> i = 2\n>>> print( geographers[ (i-3)**2-4:-1 ] )\nSee if you can work out in your head before typing it!"
  },
  {
    "objectID": "lectures/2.4-Lists.html#finding-things-in-lists",
    "href": "lectures/2.4-Lists.html#finding-things-in-lists",
    "title": "Lists",
    "section": "Finding Things in Lists",
    "text": "Finding Things in Lists"
  },
  {
    "objectID": "lectures/2.4-Lists.html#where-is-it",
    "href": "lectures/2.4-Lists.html#where-is-it",
    "title": "Lists",
    "section": "Where is It?",
    "text": "Where is It?\nlist.index(...) tells you where something can be found in a list:\n>>> geographers.index(\"Harvey\")\n1\n>>> geographers.index(\"Massey\")\n0\nCombining ideas that will become very useful later:\n>>> print(geographers[ geographers.index(\"Massey\") ])\nWhat do you think this prints? Why does it work at all?\n\nThis last example looks a little strange, but what if I had a separate list with first names, or Wikipedia links, or other information about these geographers? Because list.index(x) returns an integer we can use it as an index for accessing another list."
  },
  {
    "objectID": "lectures/2.4-Lists.html#is-it-there-at-all",
    "href": "lectures/2.4-Lists.html#is-it-there-at-all",
    "title": "Lists",
    "section": "Is it There at All?",
    "text": "Is it There at All?\nlist.index(...) has one flaw:\n>>> geographers.index('Batty')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nValueError: 'Batty' is not in list\nThere are ways to handle this, but often ‘throwing an error’ is overkill; here’s another way:\nif 'Batty' in geographers:\n    print(\"Found Mike!\")\nelse:\n    print(\"Not a geographer!\")"
  },
  {
    "objectID": "lectures/2.4-Lists.html#sorting",
    "href": "lectures/2.4-Lists.html#sorting",
    "title": "Lists",
    "section": "Sorting",
    "text": "Sorting\nWe can sort lists in alpha-numerical order:\n>>> geographers.sort()\n>>> print(geographers)\n['Harvey', 'Massey', 'Rose']\nAnd we can reverse-sort them too:\n>>> geographers.sort(reverse=True)\n>>> print(geographers)\n['Rose', 'Massey', 'Harvey']"
  },
  {
    "objectID": "lectures/2.4-Lists.html#changing-lists",
    "href": "lectures/2.4-Lists.html#changing-lists",
    "title": "Lists",
    "section": "Changing Lists",
    "text": "Changing Lists"
  },
  {
    "objectID": "lectures/2.4-Lists.html#lists-are-mutable",
    "href": "lectures/2.4-Lists.html#lists-are-mutable",
    "title": "Lists",
    "section": "Lists are Mutable",
    "text": "Lists are Mutable\nMutable == “liable or subject to change or alteration”\n>>> geographers = [\"Massey\",\"Harvey\",\"Rose\"]\n>>> geographers[2] = \"Jefferson\"\n>>> print(geographers)\n['Massey','Harvey','Jefferson']\nMore about Louise E. Jefferson"
  },
  {
    "objectID": "lectures/2.4-Lists.html#addingremoving-items-in-lists",
    "href": "lectures/2.4-Lists.html#addingremoving-items-in-lists",
    "title": "Lists",
    "section": "Adding/Removing Items in Lists",
    "text": "Adding/Removing Items in Lists\nWhen we insert() items into, or pop() items out of, a list we normally need to specify the index.\n>>> geographers.insert(0,\"von Humboldt\")\n>>> print(geographers)\n['von Humboldt', 'Massey', 'Harvey', 'Jefferson']\n>>> geographers.insert(3,\"von Humboldt\")\n>>> print(geographers)\n['von Humboldt', 'Massey', 'Harvey', 'von Humboldt', 'Jefferson']\nAnd in ‘reverse’:\n>>> geographers.pop(3)\n'von Humboldt'\n>>> print(geographers)\n['von Humboldt', 'Massey', 'Harvey', 'Jefferson']"
  },
  {
    "objectID": "lectures/2.4-Lists.html#test-yourself-1",
    "href": "lectures/2.4-Lists.html#test-yourself-1",
    "title": "Lists",
    "section": "Test Yourself",
    "text": "Test Yourself\nThere are two ways to remove David Harvey from the list of geographers without writing this:\n>>> geographers.pop(2) # This is not the answer!\n\nYou can adapt an example we saw earlier in ‘Finding Things’.\nYou can use Google.\n\n\nHints: remove and del are both options here."
  },
  {
    "objectID": "lectures/2.4-Lists.html#combining-lists",
    "href": "lectures/2.4-Lists.html#combining-lists",
    "title": "Lists",
    "section": "Combining Lists",
    "text": "Combining Lists"
  },
  {
    "objectID": "lectures/2.4-Lists.html#concatenation",
    "href": "lectures/2.4-Lists.html#concatenation",
    "title": "Lists",
    "section": "Concatenation",
    "text": "Concatenation\nWe can combine lists using addition:\n>>> female_geographers = ['Rose','Valentine','Massey','Jefferson']\n>>> male_geographers = ['Von Humboldt','Harvey','Hägerstrand']\n>>> all_geographers = female_geographers + male_geographers\n>>> print(all_geographers)\n['Rose', 'Valentine', 'Massey', 'Jefferson', 'Von Humboldt', 'Harvey', 'Hägerstrand']\n>>> print(all_geographers[0])\nRose"
  },
  {
    "objectID": "lectures/2.4-Lists.html#appending",
    "href": "lectures/2.4-Lists.html#appending",
    "title": "Lists",
    "section": "Appending",
    "text": "Appending\nAppending does something a bit different:\n>>> female_geographers = ['Rose','Valentine','Massey','Jefferson']\n>>> male_geographers = ['Von Humboldt','Harvey','Hägerstrand']\n>>> all_geographers = []\n>>> all_geographers.append(female_geographers)\n>>> all_geographers.append(male_geographers)\n>>> print(all_geographers)\n[['Rose', 'Valentine', 'Massey', 'Jefferson'], ['Von Humboldt', 'Harvey', 'Hägerstrand']]\n>>> print(all_geographers[0])\n['Rose', 'Valentine', 'Massey', 'Jefferson']\nWhat do you think has happened here?"
  },
  {
    "objectID": "lectures/2.4-Lists.html#test-yourself-2",
    "href": "lectures/2.4-Lists.html#test-yourself-2",
    "title": "Lists",
    "section": "Test Yourself",
    "text": "Test Yourself\n>>> male_geographers = ['Von Humboldt','Harvey','Hägerstrand']\n>>> male_geographers.append('Batty')\nWhat do you think print(male_geographers) will produce? And why do you think that append appears to do something different in these two examples?"
  },
  {
    "objectID": "lectures/2.4-Lists.html#finally",
    "href": "lectures/2.4-Lists.html#finally",
    "title": "Lists",
    "section": "Finally…",
    "text": "Finally…"
  },
  {
    "objectID": "lectures/2.4-Lists.html#how-many-geographers-do-i-know",
    "href": "lectures/2.4-Lists.html#how-many-geographers-do-i-know",
    "title": "Lists",
    "section": "How many geographers do I know?",
    "text": "How many geographers do I know?\nlen(...) gives you the length of ‘countable’ things:\n>>> geographers = [\"Massey\",\"Harvey\",\"Rose\"]\n>>> len(geographers)\n3\nBut…\n>>> female_geographers = ['Rose','Valentine','Massey','Jefferson']\n>>> male_geographers = ['Von Humboldt','Harvey','Hägerstrand']\n>>> all_geographers = []\n>>> all_geographers.append(female_geographers)\n>>> all_geographers.append(male_geographers)\n>>> print( len(all_geographers) )\n2"
  },
  {
    "objectID": "lectures/2.4-Lists.html#remember-whos-in-the-list",
    "href": "lectures/2.4-Lists.html#remember-whos-in-the-list",
    "title": "Lists",
    "section": "Remember: Who’s in the List?",
    "text": "Remember: Who’s in the List?\n>>> geographers = [\"Massey\",\"Harvey\",\"Rose\"]\n>>> print(\"Massey\" in geographers)\nTrue\n>>> print(\"Batty\" in geographers)\nFalse\nBut…\n>>> geographers = [\"Massey\",\"Harvey\",\"Rose\"]\n>>> geographers.index('Massey')\n0\n>>> geographers.index('Batty')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nValueError: 'Batty' is not in list\nThinking logically why might you choose one of these over the other?"
  },
  {
    "objectID": "lectures/2.4-Lists.html#test-yourself-3",
    "href": "lectures/2.4-Lists.html#test-yourself-3",
    "title": "Lists",
    "section": "Test Yourself",
    "text": "Test Yourself\nHow would you change this code:\n>>> geographers = [\"Massey\",\"Harvey\",\"Rose\"]\n>>> print(\"Massey\" in geographers)\nTrue\n>>> print(\"Batty\" in geographers)\nFalse\nSo that it prints:\nFalse\nTrue\nYou will have seen the answer to this in Code Camp, but you can also Google the it!"
  },
  {
    "objectID": "lectures/2.4-Lists.html#a-special-kind-of-list",
    "href": "lectures/2.4-Lists.html#a-special-kind-of-list",
    "title": "Lists",
    "section": "A Special Kind of ‘List’",
    "text": "A Special Kind of ‘List’\nBecause they come up a lot in geo-data analysis, it’s worth knowing about tuples. The easiest way to think about tuples is as an immutable list.\n>>> t = (52.124021, -0.0012012)\n>>> type(t)\n<class 'tuple'>\n>>> t\n(52.124021, -0.0012012)\n>>> t[0]\n52.124021\n>>> t[0] = 25.1203210\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: 'tuple' object does not support item assignment"
  },
  {
    "objectID": "lectures/2.4-Lists.html#more-resources",
    "href": "lectures/2.4-Lists.html#more-resources",
    "title": "Lists",
    "section": "More Resources",
    "text": "More Resources\n\nLists in Python\nTuples in Python\nRange and lists\nSequence types"
  },
  {
    "objectID": "lectures/2.5-Iteration.html#iteration",
    "href": "lectures/2.5-Iteration.html#iteration",
    "title": "Iteration",
    "section": "it·er·a·tion",
    "text": "it·er·a·tion\n/itə’rāSHən/\nNoun\nThe repetition of a process or utterance.\n\nrepetition of a mathematical or computational procedure applied to the result of a previous application, typically as a means of obtaining successively closer approximations to the solution of a problem.\na new version of a piece of computer hardware or software.plural noun: iterations\n\n\nMost programmers also call these loops."
  },
  {
    "objectID": "lectures/2.5-Iteration.html#making-the-difference-memorable",
    "href": "lectures/2.5-Iteration.html#making-the-difference-memorable",
    "title": "Iteration",
    "section": "Making the Difference Memorable",
    "text": "Making the Difference Memorable\n\nautoplay"
  },
  {
    "objectID": "lectures/2.5-Iteration.html#for",
    "href": "lectures/2.5-Iteration.html#for",
    "title": "Iteration",
    "section": "For",
    "text": "For\nThis ‘simple’ loop allows us to print out every element of the list in turn:\ngeographers = ['Rose','Massey','Jefferson','Von Humboldt','Harvey']\nfor g in geographers:\n  print(g)\n> Rose\n> Massey\n> Jefferson\n...\nNotice the format:\nfor x in list:\n  ...do something...\n\nNotice that this is the same in that we saw with if 'Batty' in geographers!"
  },
  {
    "objectID": "lectures/2.5-Iteration.html#while",
    "href": "lectures/2.5-Iteration.html#while",
    "title": "Iteration",
    "section": "While",
    "text": "While\nThis while loop does the same thing, but it does it differently:\ngeographers = ['Rose','Massey','Jefferson','Von Humboldt','Harvey']\ng = 0\nwhile g < len(geographers):\n  print( geographers[g] )\n  g += 1\nNotice the format:\nwhile <some condition is true>:\n  ...do something..."
  },
  {
    "objectID": "lectures/2.5-Iteration.html#nesting-loops",
    "href": "lectures/2.5-Iteration.html#nesting-loops",
    "title": "Iteration",
    "section": "Nesting Loops",
    "text": "Nesting Loops\nWe can use one loop ‘inside’ another loop! What do you think this might print?\nfor g in geographers:\n  for h in g:\n    print(h)\nLet’s puzzle it out…"
  },
  {
    "objectID": "lectures/2.5-Iteration.html#recap",
    "href": "lectures/2.5-Iteration.html#recap",
    "title": "Iteration",
    "section": "Recap",
    "text": "Recap\nfor iterates once over a collection items (e.g. a list).\nwhile keeps going until a condition is False."
  },
  {
    "objectID": "lectures/2.5-Iteration.html#test-yourself",
    "href": "lectures/2.5-Iteration.html#test-yourself",
    "title": "Iteration",
    "section": "Test Yourself",
    "text": "Test Yourself\nWhat will this code print? [I’d suggest that you don’t run it!]\ngeographers = ['Rose','Massey','Jefferson','Von Humboldt','Harvey']\ng = 0\nwhile g < len(geographers):\n  print( geographers[g] )"
  },
  {
    "objectID": "lectures/2.5-Iteration.html#test-yourself-tricksy-version",
    "href": "lectures/2.5-Iteration.html#test-yourself-tricksy-version",
    "title": "Iteration",
    "section": "Test Yourself (Tricksy Version)",
    "text": "Test Yourself (Tricksy Version)\nHere’s a really tricky one! The following two blocks of code produce the same output, how are they different?\ngeographers = ['Rose','Massey','Jefferson','Von Humboldt','Harvey']\ngeographers.reverse()\nfor g in geographers:\n  print(g)\nAnd:\ngeographers = ['Rose','Massey','Jefferson','Von Humboldt','Harvey']\ng = len(geographers)-1\nwhile g >= 0:\n  print( geographers[g] )\n  g -= 1"
  },
  {
    "objectID": "lectures/2.5-Iteration.html#one-more-thing",
    "href": "lectures/2.5-Iteration.html#one-more-thing",
    "title": "Iteration",
    "section": "One More Thing…",
    "text": "One More Thing…\nLet’s go back to the Lists examples for a second:\nfemale_geographers = ['Rose','Valentine','Massey','Jefferson']\nmale_geographers = ['Von Humboldt','Harvey','Hägerstrand']\nall_geographers = []\nall_geographers.append(female_geographers)\nall_geographers.append(male_geographers)\nHave a think about how this code works:\nfor ag in all_geographers:\n  for g in ag:\n    print(g)"
  },
  {
    "objectID": "lectures/2.5-Iteration.html#resources",
    "href": "lectures/2.5-Iteration.html#resources",
    "title": "Iteration",
    "section": "Resources",
    "text": "Resources\n\nWhat is Iteration?\nLoops\nFor Loop\nWhile Loop\n\nWe don’t cover the concept of recursion, but it’s quite a powerful idea and links nicely with Iteration: - What is a recursive function? - Define recursive functions"
  },
  {
    "objectID": "lectures/2.6-The_Command_Line.html#the-answer",
    "href": "lectures/2.6-The_Command_Line.html#the-answer",
    "title": "The Command Line",
    "section": "The Answer?",
    "text": "The Answer?\nNo matter how long you try to avoid it, eventually you’ll find things that can only be solved (or can be much more quickly solved) using the Command Line.\nThings like:\n\nInteracting with git is actually easier on the Command Line.\nInstalling and/or running developer-oriented tools (e.g. docker, GDAL, proj4/6).\nBulk operations, peeking and poking at (large) files, navigating the File System…\nCommand-chaining/scripting: automate things that would be hard/annoying to do manually…\n\nA lot of this ties back to data and servers.\n\nTrue story: 25 years ago I used to process more than 40GB of compressed plain-text data every day from my Titanium PowerBook. But that’s because it was all running on a server in New Jersey while I was in Manhattan. Everything was done using the Command Line and SSH (secure shell)."
  },
  {
    "objectID": "lectures/2.6-The_Command_Line.html#core-commands",
    "href": "lectures/2.6-The_Command_Line.html#core-commands",
    "title": "The Command Line",
    "section": "Core Commands",
    "text": "Core Commands"
  },
  {
    "objectID": "lectures/2.6-The_Command_Line.html#interacting-with-files",
    "href": "lectures/2.6-The_Command_Line.html#interacting-with-files",
    "title": "The Command Line",
    "section": "Interacting with Files",
    "text": "Interacting with Files\n\n\n\nCommand\nDoes\nExample\n\n\n\n\nls\nList\nls .\n\n\ncd\nChange Directory\ncd ~\n\n\npwd\nPrint Working Directory\npwd\n\n\nmv\nMove\nmv a.txt b.txt\n\n\n\nYou’ll also notice some ‘shortcuts’ here:\n\n\n\n\n\n\n\nShortcut\nMeans\n\n\n\n\n.\nThe current directory where commands will be executed.\n\n\n..\nThe directory above the current one (or ‘containing’, if you prefer).\n\n\n~\nThe current user’s home directory\n\n\n\n\nNotice that most commands on the Command Line involve typing mnemonics (the shortest possible combination of letters that is unique memorable)."
  },
  {
    "objectID": "lectures/2.6-The_Command_Line.html#a-simulated-perambulation-across-my-laptop",
    "href": "lectures/2.6-The_Command_Line.html#a-simulated-perambulation-across-my-laptop",
    "title": "The Command Line",
    "section": "A Simulated Perambulation Across My Laptop",
    "text": "A Simulated Perambulation Across My Laptop\ncd ~\npwd\n> /Users/casa\nls\n> Applications  Desktop  Dropbox  ...\ncd Dropbox\npwd\n> /Users/casa/Dropbox\nls\n> CASA  Lectures  Practicals ...\ncd /\npwd\n> /\nls\n> Applications  Library  System  Users Volumes ..."
  },
  {
    "objectID": "lectures/2.6-The_Command_Line.html#finding-things-in-files",
    "href": "lectures/2.6-The_Command_Line.html#finding-things-in-files",
    "title": "The Command Line",
    "section": "Finding Things in Files",
    "text": "Finding Things in Files\n\n\n\n\n\n\n\n\nCommand\nDoes\nExample\n\n\n\n\nless\nPeek at contents of a text file\nless file.txt\n\n\ngrep\nFind lines ‘matching’ pattern in a text file\ngrep 'pattern' file.txt\n\n\nhead\nPeek at first x rows of a text file\nhead -n 10 file.txt\n\n\ntail\nPeek at last x rows of a text file\ntail -n 10 file.txt\n\n\nwc\nCount things (rows, words, etc.\nwc -l file.txt"
  },
  {
    "objectID": "lectures/2.6-The_Command_Line.html#time-to-escape",
    "href": "lectures/2.6-The_Command_Line.html#time-to-escape",
    "title": "The Command Line",
    "section": "Time to Escape!",
    "text": "Time to Escape!\nSome characters are ‘special’ and need to be escaped. You’ll encounter these both in the shell (a.k.a. command line) and in Python:\n\n\n\n\n\n\n\n\nEscape\nDoes\nExample\n\n\n\n\n\\\nAllows spaces in file names\nless My\\ File\\ with\\ Spaces.txt\n\n\n\\t\nCreates/matches a tab character\n\\tThe start of a paragraph...\n\n\n\\n\nCreates/matches a newline character\nThe end of a row/para...\\n\n\n\n\\r\nCreates/matches a carriange return\nThe end of a row/para...\\r\\n\n\n\n\\$\nLiteral dollar sign (since $ often marks a variable)\nIt costs \\$1,000,000\n\n\n\\!\nLiteral exclamation mark (since ! can mean a number of things)\nDon't forget me\\!\n\n\n\nThis also becomes relevant when you’re dealing with quotes:\n\"\"This is a problem,\" she said.\"\nvs. \n\"\\\"This is a problem,\\\" she said.\"\n\nThe carriage return is only ever encountered on files that have been opened on Windows machines."
  },
  {
    "objectID": "lectures/2.6-The_Command_Line.html#compressingdecompressing-files",
    "href": "lectures/2.6-The_Command_Line.html#compressingdecompressing-files",
    "title": "The Command Line",
    "section": "Compressing/Decompressing Files",
    "text": "Compressing/Decompressing Files\n\n\n\nCommand\nDoes\nExample\n\n\n\n\ngzip\nCompress/Decompress files\ngzip file.txt\n\n\ngunzip\nDecompress files\ngunzip file.txt.gz[ ^1 ]\n\n\n\nBut we can make this more useful by chaining commands together using the ‘pipe’ (|):\ngzip -cd very_lg_file.txt.gz | head -n 500 | grep \"pattern\"\nThis will give you an ‘answer’ much, much, much faster than trying to open the whole file in, say, Excel, Numbers, or even Python.\n[ ^1 ]: This can also be done using ‘switches’ passed to gzip: gzip -cd (where -d means ‘decompress’)."
  },
  {
    "objectID": "lectures/2.6-The_Command_Line.html#redirecting-output",
    "href": "lectures/2.6-The_Command_Line.html#redirecting-output",
    "title": "The Command Line",
    "section": "Redirecting Output",
    "text": "Redirecting Output\nWe can redirect outputs in to new files with >, and inputs out of existing files using <:\ngzip -cd very_lg_file.txt.gz | head -n 500 | grep \"pattern\" > matches.txt\nThis directs all of the output from the previous commands into matches.txt as plain-text. The reverse < is only used in very special circumstances so you probably won’t encounter it very often."
  },
  {
    "objectID": "lectures/2.6-The_Command_Line.html#a-complex-example",
    "href": "lectures/2.6-The_Command_Line.html#a-complex-example",
    "title": "The Command Line",
    "section": "A (Complex) Example",
    "text": "A (Complex) Example\nI do not expect you to understand this, but I do want you to understand why this is important:\ndocker run -v conda:/home/jovyan/work --rm ${DOCKER_NM} start.sh \\\n   conda env export -n ${ENV_NM} | sed '1d;$d' | sed '$d' \\\n   | perl -p -e 's/^([^=]+=)([^=]+)=.+$/$1$2/m' \\\n   | grep -Ev '\\- _|cpp|backports|\\- lib|\\- tk|\\- xorg' > conda/environment_py.yml\n\nThis is how I generated the YAML file used by Anaconda Python installers: it is running a command on a virtual machine, collecting the output, filtering out lines by both row number and textual pattern, and directing this all in the environment_py.yml file. This can be run as part of my ‘build’ of the programming environment. It’s all automated!"
  },
  {
    "objectID": "lectures/2.6-The_Command_Line.html#getting-help",
    "href": "lectures/2.6-The_Command_Line.html#getting-help",
    "title": "The Command Line",
    "section": "Getting Help",
    "text": "Getting Help\nThe Software Carpentry people have a whole set of lessons around working with ‘the shell’ (a.k.a. Command Line) that might help you.\nSee The UNIX Shell."
  },
  {
    "objectID": "lectures/2.6-The_Command_Line.html#useful-videos",
    "href": "lectures/2.6-The_Command_Line.html#useful-videos",
    "title": "The Command Line",
    "section": "Useful Videos",
    "text": "Useful Videos\nThe Shell/Terminal in general: - Absolute BEGINNER Guide to the Mac OS Terminal - Linux Bash Shell for Beginners: Tutorial 1 - Beginner’s Guide to the Bash Terminal - Shell Novice - How to use the Command Line\nSome specific commands: - Cat - Gzip/Tar (also a good point about spaces in a file name!) - Grep - Find\nSome useful videos: - Using file system and shell commands"
  },
  {
    "objectID": "lectures/2.7-Git.html#however",
    "href": "lectures/2.7-Git.html#however",
    "title": "Getting to Grips with Git",
    "section": "However",
    "text": "However\nAlthough we naturally connect version control to coding, you can use it anywhere that you have files that are changing (hint: essays, notes, projects, assessments, code…) and need/want to track them.\nBonus: you also gain free backups if some part of your version control system is on a different computer!"
  },
  {
    "objectID": "lectures/2.7-Git.html#how-it-works",
    "href": "lectures/2.7-Git.html#how-it-works",
    "title": "Getting to Grips with Git",
    "section": "How It Works",
    "text": "How It Works\nThe natural way to think about managing versions of a document is to have a master copy somewhere. Everyone asks the server for the master copy, makes some changes, and then checks those changes back in.\nThis is not how Git works."
  },
  {
    "objectID": "lectures/2.7-Git.html#how-git-works",
    "href": "lectures/2.7-Git.html#how-git-works",
    "title": "Getting to Grips with Git",
    "section": "How Git Works",
    "text": "How Git Works\nGit is distributed. You could call it peer-to-peer, meaning that every computer where git is installed is its own server and has its own master copy.\nSo every computer has a full history of a file or project that been added to Git.\nIn order to make this useful, you need ways to synchronise changes between computers that all think they’re right."
  },
  {
    "objectID": "lectures/2.7-Git.html#github",
    "href": "lectures/2.7-Git.html#github",
    "title": "Getting to Grips with Git",
    "section": "GitHub",
    "text": "GitHub\nGitHub is nothing special to Git, just another server with which to negotiate changes. Do not think of GitHub as the ‘master’ copy. There isn’t one."
  },
  {
    "objectID": "lectures/2.7-Git.html#using-git",
    "href": "lectures/2.7-Git.html#using-git",
    "title": "Getting to Grips with Git",
    "section": "Using Git",
    "text": "Using Git"
  },
  {
    "objectID": "lectures/2.7-Git.html#getting-started",
    "href": "lectures/2.7-Git.html#getting-started",
    "title": "Getting to Grips with Git",
    "section": "Getting Started",
    "text": "Getting Started\n\n\n\nTerm\nMeans\n\n\n\n\nRepository (Repo)\nA project or achive stored in Git.\n\n\ninit\nTo create a new repo.\n\n\nclone\nTo make a full copy of a repo somewhere else.\n\n\n\nFor example:\nmkdir i2p\ncd i2p\ngit init\nOr: git clone https://github.com/jreades/i2p.git"
  },
  {
    "objectID": "lectures/2.7-Git.html#working-on-a-file",
    "href": "lectures/2.7-Git.html#working-on-a-file",
    "title": "Getting to Grips with Git",
    "section": "Working on a File",
    "text": "Working on a File\n\n\n\nTerm\nMeans\n\n\n\n\nadd\nAdd a file to a repo.\n\n\nMove (mv)\nMove/Rename a file in a repo.\n\n\nRemove (rm)\nRemove a file from a repo.\n\n\n\nFor example:\ngit add README.md\ngit mv README.md fileA.md\ngit rm fileA.md\nBut no one else knows about these changes yet!"
  },
  {
    "objectID": "lectures/2.7-Git.html#looking-at-the-history",
    "href": "lectures/2.7-Git.html#looking-at-the-history",
    "title": "Getting to Grips with Git",
    "section": "Looking at the History",
    "text": "Looking at the History\n\n\n\nTerm\nMeans\n\n\n\n\ndiff\nShow changes between commits.\n\n\nstatus\nShow status of files in repo.\n\n\nlog\nShow history of commits.\n\n\n\nFor example:\ngit status"
  },
  {
    "objectID": "lectures/2.7-Git.html#working-on-a-project-or-file",
    "href": "lectures/2.7-Git.html#working-on-a-project-or-file",
    "title": "Getting to Grips with Git",
    "section": "Working on a Project or File",
    "text": "Working on a Project or File\n\n\n\nTerm\nMeans\n\n\n\n\ncommit\nTo record changes to the repo.\n\n\nbranch\nCreate or delete branches.\n\n\ncheckout\nJump to a different branch.\n\n\n\nFor example:\ngit commit -m \"Moved and then deleted README file.\""
  },
  {
    "objectID": "lectures/2.7-Git.html#collaborating-on-a-project",
    "href": "lectures/2.7-Git.html#collaborating-on-a-project",
    "title": "Getting to Grips with Git",
    "section": "Collaborating on a Project",
    "text": "Collaborating on a Project\n\n\n\nTerm\nMeans\n\n\n\n\npull\nTo request changes to a repo from another computer.\n\n\npush\nTo send your changes to a repo to another computer.\n\n\n\nFor example:\ngit push"
  },
  {
    "objectID": "lectures/2.7-Git.html#consequences",
    "href": "lectures/2.7-Git.html#consequences",
    "title": "Getting to Grips with Git",
    "section": "Consequences",
    "text": "Consequences\nAll changes are local until you push them.\nIf you forget to push your changes (e.g. to GitHub) then you are not backed up if your computer dies."
  },
  {
    "objectID": "lectures/2.7-Git.html#this-is-not-easy",
    "href": "lectures/2.7-Git.html#this-is-not-easy",
    "title": "Getting to Grips with Git",
    "section": "This is not easy",
    "text": "This is not easy\n\nXKCD Comic About GitSource"
  },
  {
    "objectID": "lectures/2.7-Git.html#a-dropbox-analogy",
    "href": "lectures/2.7-Git.html#a-dropbox-analogy",
    "title": "Getting to Grips with Git",
    "section": "A Dropbox Analogy",
    "text": "A Dropbox Analogy\n\nThink of the programming environment (Vagrant+JupyterLab) as being an application (like Word or Excel) that allows you to read/write/edit notebook files.\nThink of GitHub as being like Dropbox somewhere in the cloud that files on your home machine can be backed up.\nSo you should not need to do anything in JupyterLab to move files to, or see changes on, GitHub.\nGit gives you much finer-grained control over files and syncrhonisation than Dropbox.\n\n\nLike Dropbox, GitHub offers a lot of ‘value added’ featuers (like simple text editing) on top of the basic service of ‘storing files’.\nDropbox will automatically back up anything that you put in your special Dropbox folder.\nGit will only back up the things that you tell it to back up, even if they are in the Git folder!"
  },
  {
    "objectID": "lectures/2.7-Git.html#a-note-on-workflow",
    "href": "lectures/2.7-Git.html#a-note-on-workflow",
    "title": "Getting to Grips with Git",
    "section": "A Note on Workflow",
    "text": "A Note on Workflow\nSo your workflow should be:\n\nSave edits to Jupyter notebook.\nRun git add <filename.ipynb> to record changes to the notebook (obviously replace <filename.ipynb> completely with the notebook filename.\nRun git commit -m \"Adding notes based on lecture\" (or whatever message is appropriate: -m means ‘message’).\nThen run git push to push the changes to GitHub.\n\nIf any of those commands indicate that there are no changes being recorded/pushed then it might be that you’re not editing the file that you think you are (this happens to me!).\nOn the GitHub web site you may need to force reload the view of the repository: Shift + Reload button usually does it in most browsers. You may also need to wait 5 to 10 seconds for the changes to become ‘visible’ before reloading. It’s not quite instantaeous."
  },
  {
    "objectID": "lectures/2.7-Git.html#resources",
    "href": "lectures/2.7-Git.html#resources",
    "title": "Getting to Grips with Git",
    "section": "Resources",
    "text": "Resources\n\nUnderstanding Git (Part 1) – Explain it Like I’m Five\nTrying Git\nVisualising Git\nGit Novice\nAndy’s R-focussed Tutorial\n\n\nI now have everything in Git repos: articles, research, presentations, modules… the uses are basically endless once you start using Markdown heavily (even if you don’t do much coding)."
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#how-does-it-work",
    "href": "lectures/3.1-Dictionaries.html#how-does-it-work",
    "title": "Dictionaries",
    "section": "How Does it Work?",
    "text": "How Does it Work?\nThe key can be almost anything—an integer, a string, a variable…—that is unique in the dictionary and immutable.\nA list is not immutable, so you can’t use a list as a dictionary key.\n\nAgain, just like a real dictionary: you don’t have multiple entries for ‘dog’, otherwise the dictionary wouldn’t work. You might have multiple definitions: which is to say, the key might return multiple values."
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#manipulating-dictionaries",
    "href": "lectures/3.1-Dictionaries.html#manipulating-dictionaries",
    "title": "Dictionaries",
    "section": "Manipulating Dictionaries",
    "text": "Manipulating Dictionaries\ncities = {\n  'San Francisco': 837442,\n  'London': 8673713,\n  'Paris': 2229621, \n  'Beijing': 21700000\n}\nc = 'Beijing'\nprint(f\"The population of {c} is {cities['Beijing']}\")"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#setting-values",
    "href": "lectures/3.1-Dictionaries.html#setting-values",
    "title": "Dictionaries",
    "section": "Setting Values",
    "text": "Setting Values\nIt’s the same process to update an existing value or create a new one:\ncities['Beijing'] = 21716620\ncities['Toronto'] = 2930000\n\nprint(cities['Beijing'])\nprint(cities['Toronto'])\ndel cities['Toronto'] # Key is deleted immediately\ncities.pop('Toronto',None) # Does not return error if key doesn't exist; returns None"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#getting-values",
    "href": "lectures/3.1-Dictionaries.html#getting-values",
    "title": "Dictionaries",
    "section": "Getting Values",
    "text": "Getting Values\nThere are two ways to retrieve values from a dictionary: 1. cities['Beijing'] 2. cities.get('Beijing')\nWhy have two? Consider:\nprint(cities['Sao Paulo'])\n> Traceback (most recent call last):\n>  File \"<stdin>\", line 1, in <module>\n> KeyError: 'Sao Paulo'\nprint(cities.get('Sao Paulo'))\n> None\n\nThe first triggers an error, the second returns None. Errors can be unfriendly: do you want your entire Python program to fail because a single city is missing, or would you rather than it did something a little more sensible such as… skipping the row?"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#getting-values-contd",
    "href": "lectures/3.1-Dictionaries.html#getting-values-contd",
    "title": "Dictionaries",
    "section": "Getting Values (cont’d)",
    "text": "Getting Values (cont’d)\nprint(cities['Sao Paulo'])\n> KeyError: ‘Sao Paulo’\n\nc = cities.get('Sao Paulo')\nif not c:\n  print(\"Sorry, no city by that name.\")\n> \"Sorry, no city by that name.\""
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#iterating",
    "href": "lectures/3.1-Dictionaries.html#iterating",
    "title": "Dictionaries",
    "section": "Iterating",
    "text": "Iterating\nSimilar to iterating over lists but…\nfor c in cities:\n  print(c)\nPrints:\n'San Francisco'\n'London'\n'Paris'\n'Beijing'\n'Toronto'\n\nHow would we print out the populations?"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#danger-will-robinson",
    "href": "lectures/3.1-Dictionaries.html#danger-will-robinson",
    "title": "Dictionaries",
    "section": "Danger, Will Robinson!",
    "text": "Danger, Will Robinson!\n\nDictionaries are unordered key/value pairs. There is no guarantee that things come out in the same order they went in! They complement ordered lists, they don’t replace them!"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#a-final-note",
    "href": "lectures/3.1-Dictionaries.html#a-final-note",
    "title": "Dictionaries",
    "section": "A Final Note!",
    "text": "A Final Note!\n\nKeys can be almost anything, and so can values!\n\ncities = {\n  'San Francisco': [37.77, -122.43, 'SFO']\n}\n\nWhat is this starting to look like?"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#and-also",
    "href": "lectures/3.1-Dictionaries.html#and-also",
    "title": "Dictionaries",
    "section": "And Also…",
    "text": "And Also…\ncities = {\n  'San Francisco': {\n    'lat': 37.77,\n    'lon': -122.43,\n    'airport':'SFO'}\n}\nprint(cities['San Francisco']['lat'])\nThis is basically what JSON is."
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#resources",
    "href": "lectures/3.1-Dictionaries.html#resources",
    "title": "Dictionaries",
    "section": "Resources",
    "text": "Resources\n\nDictionaries and sets\nComprehensions"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#making-sense-of-this",
    "href": "lectures/3.2-LOLs.html#making-sense-of-this",
    "title": "LoLs",
    "section": "Making Sense of This",
    "text": "Making Sense of This\nWe can ‘unpack’ my_list in stages in order to make sense of it:\nmy_list = [\n  [1, 2, 3], \n  [4, 5, 6], \n  [7, 8, 9]\n]\n\nfor i in my_list:\n  print(i)\nWhat do you think this will print?"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#debugging-our-thinking",
    "href": "lectures/3.2-LOLs.html#debugging-our-thinking",
    "title": "LoLs",
    "section": "Debugging Our Thinking",
    "text": "Debugging Our Thinking\nLet’s make it a little more obvious:\na = [1, 2, 3]\nb = [4, 5, 6]\nc = [7, 8, 9]\nmy_list = [\n  a, \n  b, \n  c]\nfor i in my_list:\n  print(i)"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#the-next-step",
    "href": "lectures/3.2-LOLs.html#the-next-step",
    "title": "LoLs",
    "section": "The Next Step",
    "text": "The Next Step\nWe could then try this:\nfor i in my_list:\n  print(f\" >> {i}\")\n  for j in i: # Remember that i is a list!\n    print(j)\nThis produces:\n >> [1, 2, 3]\n1\n2\n3\n >> [4, 5, 6]\n4\n..."
  },
  {
    "objectID": "lectures/3.2-LOLs.html#putting-it-together",
    "href": "lectures/3.2-LOLs.html#putting-it-together",
    "title": "LoLs",
    "section": "Putting It Together",
    "text": "Putting It Together\nmy_list = [\n  [1, 2, 3], \n  [4, 5, 6], \n  [7, 8, 9]\n]\nWe can access each list (i) by iterating over my_list. And we can access each element (j) of i using i[j].\nCan we… my_list[i][j]?"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#lets-try-it",
    "href": "lectures/3.2-LOLs.html#lets-try-it",
    "title": "LoLs",
    "section": "Let’s Try It!",
    "text": "Let’s Try It!\nprint(my_list[0][0])\n> 1\nprint(my_list[2][2])\n> 9\nSo my_list[0] grabs the first list ([1,2,3]) and then my_list[0][0] tells Python to get the first item in that first list (i.e. 1).\nSimilarly, my_list[2] grabs the third list ([7,8,9]) and then my_list[2][2] tells Python to get the third item in that third list (i.e. 9).\n How you print the number 5 from this list-of-lists?"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#making-this-useful",
    "href": "lectures/3.2-LOLs.html#making-this-useful",
    "title": "LoLs",
    "section": "Making This Useful",
    "text": "Making This Useful\nIf I rewrite my list this way perhaps it looks a little more useful?\nmy_cities = [\n  ['London', 51.5072, 0.1275, +0], \n  ['New York', 40.7127, 74.0059, -5], \n  ['Tokyo', 35.6833, 139.6833, +8]\n]\nNow we have something that is starting to look like data!"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#things-are-about-to-get-weird.",
    "href": "lectures/3.2-LOLs.html#things-are-about-to-get-weird.",
    "title": "LoLs",
    "section": "Things are about to get… weird.",
    "text": "Things are about to get… weird."
  },
  {
    "objectID": "lectures/3.2-LOLs.html#down-the-rabbit-hole",
    "href": "lectures/3.2-LOLs.html#down-the-rabbit-hole",
    "title": "LoLs",
    "section": "Down the Rabbit Hole",
    "text": "Down the Rabbit Hole\n\nAlice"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#lols-of-lols",
    "href": "lectures/3.2-LOLs.html#lols-of-lols",
    "title": "LoLs",
    "section": "LOLs of LOLs",
    "text": "LOLs of LOLs\nThis is also a legitimate list in Python.\nmy_cities = [\n  ['London', [51.5072, 0.1275], +0], \n  ['New York', [40.7127, 74.0059], -5], \n  ['Tokyo', [35.6833, 139.6833], +8]\n]\nprint(my_cities[0][0])\n> London\nprint(my_cities[0][1][0])\n> 51.5072\n Why might it be a better choice of data structure than the earlier version?"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#how-deep-can-you-go",
    "href": "lectures/3.2-LOLs.html#how-deep-can-you-go",
    "title": "LoLs",
    "section": "How Deep Can You Go?",
    "text": "How Deep Can You Go?\n\nThere is no real limit to how many lists you can nest inside of other lists. The only problem is accessing them!\n\nIt’s hard to make sense of: my_cities[0][1][4][1][8]!\n\nThat’s why most sane people rarely use more then three levels of lists. You will see developers iterate over them using i, j, and k."
  },
  {
    "objectID": "lectures/3.2-LOLs.html#its-a-deep-rabbit-hole",
    "href": "lectures/3.2-LOLs.html#its-a-deep-rabbit-hole",
    "title": "LoLs",
    "section": "It’s a Deep Rabbit Hole",
    "text": "It’s a Deep Rabbit Hole\n\nlHave fun!\nmy_dict = {\n  'London': [[51.5072, 0.1275], +0], \n  'New York': [[40.7127, 74.0059], -5], \n  'Tokyo': [[35.6833, 139.6833], +8]\n}"
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#compare",
    "href": "lectures/3.3-DOLs_to_Data.html#compare",
    "title": "Data Structures",
    "section": "Compare…",
    "text": "Compare…\nmy_cities = [\n  {name: 'London', loc: [51.5072, 0.1275], tz: +0}, \n  {name: 'New York', loc: [40.7127, 74.0059], tz: -5}, \n  {name: 'Tokyo', loc: [35.6833, 139.6833], tz: +8}\n]\nmy_cities = {\n  'London': {'loc': [51.5072, 0.1275], 'tz': +0}, \n  'New York': {'loc': [40.7127, 74.0059], 'tz': -5}, \n  'Tokyo': {'loc': [35.6833, 139.6833], 'tz': +8}\n}\n Is either of these the right way to store data?"
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#implications",
    "href": "lectures/3.3-DOLs_to_Data.html#implications",
    "title": "Data Structures",
    "section": "Implications",
    "text": "Implications\n\nSo we can mix and match dictionaries and lists in whatever way we need to store… ‘data’. The question is then: what’s the right way to store our data?\n\n\nAnswer: the way that makes the most sense to a human while also being the most robust for coding."
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#one-more-thing",
    "href": "lectures/3.3-DOLs_to_Data.html#one-more-thing",
    "title": "Data Structures",
    "section": "One more thing…",
    "text": "One more thing…"
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#thinking-it-through",
    "href": "lectures/3.3-DOLs_to_Data.html#thinking-it-through",
    "title": "Data Structures",
    "section": "Thinking it Through",
    "text": "Thinking it Through\nWhy does this work for both computers and people?\nds2 = {\n  'lat':[51.51,40.71,35.69],\n  'lon':[0.13,74.01,139.68],\n  'tz':[+0,-5,+8],\n  'name':['London','New York','Tokyo']\n}\n\nThis has advantages for the computer because everything of type ‘lat’ is a float, everything of type ‘tz’ is an integer, and everything of type ‘name’ is a string.\nMore subtly: we are doing away with the idea that the order of columns matters (humans don’t care that a city’s name is in the first column, and a city’s latitude in the second). We just want to find the column. But because we have a dictionary-of-lists we can ensure that the row order is preserved. Let’s see this in action."
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#examples",
    "href": "lectures/3.3-DOLs_to_Data.html#examples",
    "title": "Data Structures",
    "section": "Examples",
    "text": "Examples\nprint(ds2['name'][0])\n> London\nprint(ds2['lat'][0])\n> 51.51\nprint(ds2['tz'][0])\n> 0\nSo we know that everything in ds2[<dictionary>][n] is related to the same n\\[^{th}\\] city,."
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#how-is-that-easier",
    "href": "lectures/3.3-DOLs_to_Data.html#how-is-that-easier",
    "title": "Data Structures",
    "section": "How is that easier???",
    "text": "How is that easier???\nRemember that we can use any immutable ‘thing’ as a key. This means…\ncity = 'Tokyo'\ncity_idx = ds2['name'].index(city)\nprint(city_idx) # Prints 2\nprint(\"The time zone difference to \" + city + \" is \" + str(\n  ds2['tz'][city_idx]\n)) # Prints 8\nWe can re-write this into a single line as:\ncity = 'London'\nprint(\"The time zone different to \" + city + \" is \" + \n  str(ds2['tz'][ ds2['name'].index(city)] ))\n\nThis achieves several useful things:\n\nIt is fast: faster than iterating over a list-of-lists or dictionary-of-dictionaries.\nAll data in a list is of the same type so we can easily add checks to make sure that it’s valid.\nWe can add more columns and the process of finding something is just as fast as it is now. And adding more rows doesn’t make it much slower either!\nAlso, notice how in these two examples we don’t try to write the second example in one go: first, we work it out as a set of steps: how do we figure out what ‘row’ (position in the list) Tokyo is in? Now that we’ve got that, how do we retrieve the time zone value for Tokyo? We know that code works, now let’s do variable substitution, as we would if we were doing maths: we can replace the city_idx in the time zone lookup with ds2['name'].index('Tokyo')."
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#this-is-critical",
    "href": "lectures/3.3-DOLs_to_Data.html#this-is-critical",
    "title": "Data Structures",
    "section": "This is critical!",
    "text": "This is critical!\nIf you can get your head around it, then \nbecause you are dealing with a data structure.\n\nI have many years of experience and it took me several hours to get my head around why this approach is better as a way of working with data."
  },
  {
    "objectID": "lectures/3.4-Functions.html#what-does-a-function-look-like",
    "href": "lectures/3.4-Functions.html#what-does-a-function-look-like",
    "title": "Functions",
    "section": "What Does a Function Look Like?",
    "text": "What Does a Function Look Like?\n\nlen(my_list) is a function.\n\nSo len(...) encapsulates the process of figuring out how long something with ‘countable units’ actually is, whether it’s a string or a list.\n\nlen(123) is a Type Error.\nlen(‘123’) is not.\n Can you think why?"
  },
  {
    "objectID": "lectures/3.4-Functions.html#so-what-does-a-function-look-like",
    "href": "lectures/3.4-Functions.html#so-what-does-a-function-look-like",
    "title": "Functions",
    "section": "So What Does a Function Look like?",
    "text": "So What Does a Function Look like?\nAll function ‘calls’ looking something like this:\nfunction_name(...)\nWhere the ‘...’ are the inputs to the function; it could be one variable, 25 variables, a list, even another function!\nAnd if the function ‘returns’ something it will look like this:\nreturn_data = function_name(...)"
  },
  {
    "objectID": "lectures/3.4-Functions.html#but-notice",
    "href": "lectures/3.4-Functions.html#but-notice",
    "title": "Functions",
    "section": "But Notice!",
    "text": "But Notice!\nprint(total)\n> Traceback (most recent call last):\n>   File \"<stdin>\", line 1, in <module>\n> NameError: name 'total' is not defined\n\nprint(numbers)\n> Traceback (most recent call last):\n>   File \"<stdin>\", line 1, in <module>\n> NameError: name 'numbers' is not defined"
  },
  {
    "objectID": "lectures/3.4-Functions.html#simple-function",
    "href": "lectures/3.4-Functions.html#simple-function",
    "title": "Functions",
    "section": "Simple Function",
    "text": "Simple Function\nBy ‘simple’ I don’t mean easy, I mean it does one thing only:\ndef hello():\n  print(\"Hello world!\")\nWe then run it with:\nhello()\nAnd that produces:\nHello world!"
  },
  {
    "objectID": "lectures/3.4-Functions.html#passing-in-information",
    "href": "lectures/3.4-Functions.html#passing-in-information",
    "title": "Functions",
    "section": "Passing in Information",
    "text": "Passing in Information\nWe can pass information to a function if we tell the function what to expect:\ndef hello(name: str):\n  print(f\"Hello {name}!\")\nNow we can do this:\nhello(\"new programmers\")\n> Hello new programmers!"
  },
  {
    "objectID": "lectures/3.4-Functions.html#getting-information-out",
    "href": "lectures/3.4-Functions.html#getting-information-out",
    "title": "Functions",
    "section": "Getting Information Out",
    "text": "Getting Information Out\nFunctions become more useful when we can also get information out of them!\ndef hello(name: str) -> str:\n  return \"Hello \" + name + \"!\"\nNow the function does this:\noutput = hello(\"new programmers\")\nprint(output)\n> Hello new programmers!"
  },
  {
    "objectID": "lectures/3.4-Functions.html#writing-a-function",
    "href": "lectures/3.4-Functions.html#writing-a-function",
    "title": "Functions",
    "section": "Writing a Function",
    "text": "Writing a Function\ndef <function name>(<var. name>: <var. type>) -> <var. type>:\n  ...\n  return <var>\nThis can also be written:\ndef <function name>(<var. name>):\n  ...\n  return <var>\nPython is ‘friendly’ in the sense that all of the <variable type> information is optional, but it will help you (and Python) to know what you were expecting to happen."
  },
  {
    "objectID": "lectures/3.4-Functions.html#complicating-things",
    "href": "lectures/3.4-Functions.html#complicating-things",
    "title": "Functions",
    "section": "Complicating Things…",
    "text": "Complicating Things…\nds2 = {\n  'lat':[51.51,40.71,35.69],\n  'lon':[0.13,74.01,139.68],\n  'tz': [+0,-5,+8],\n  'name':['London','New York','Tokyo']\n}\n\ndef get_city_info(data: dict, city: str, field: str, lookup: str='name') -> str:\n  return str(data[field][ data[lookup].index(city) ])\n\ncity = 'New York'\nprint(f\"The latitude of {city} is {get_city_info(ds2,'New York','lat')}\")\n> The latitude of New York is 40.71"
  },
  {
    "objectID": "lectures/3.5-Packages.html#importing-a-package",
    "href": "lectures/3.5-Packages.html#importing-a-package",
    "title": "Packages",
    "section": "Importing a Package",
    "text": "Importing a Package\nIf a package is installed, then it’s as simple as:\nimport packagename\nYou normally do this at the start of a program so that it’s easy to see what the program requires to run:\nimport os\nimport math\n\n... <rest of your code> ..."
  },
  {
    "objectID": "lectures/3.5-Packages.html#what-can-a-package-do",
    "href": "lectures/3.5-Packages.html#what-can-a-package-do",
    "title": "Packages",
    "section": "What Can a Package Do?",
    "text": "What Can a Package Do?\nThere are many ways to find this out: read the documentation, search Google, and search StackOverflow. There’s even a web site python.readthedocs.io.\nBut we can also ask the package:\nimport math\ndir(math)\n['__doc__', '__file__', '__name__', '__package__', ..., 'log', 'log10', 'log1p', 'modf', 'pi', 'pow', 'radians', 'sin', 'sinh', 'sqrt', 'tan', 'tanh', 'trunc']\nhelp(math.log10)\nprint(math.pi)"
  },
  {
    "objectID": "lectures/3.5-Packages.html#so",
    "href": "lectures/3.5-Packages.html#so",
    "title": "Packages",
    "section": "So…",
    "text": "So…\n\ndir(<package name>) lists all ‘things’ that <package> contains.\nBy convention, things that start with __ are ‘private’ (you shouldn’t change them) and things that start and end with __ are metadata.\nEverything else you can interrogate with help(<package name>.<thing in package>).\n\nWith help(math.log10) you get an answer like this:\nHelp on built-in function log10 in module math:\n\nlog10(x, /)\n    Return the base 10 logarithm of x.\nWith help(math.pi) you get an answer Help on float object…\n\nThis tells you that pi is a float, it doesn’t tell you what Pi is (an irrational number). So here’s another case where the computer gives you a technically correct but not always helpful answer. In the context of the math package, Pi is a float constant."
  },
  {
    "objectID": "lectures/3.5-Packages.html#more-laziness-aliases",
    "href": "lectures/3.5-Packages.html#more-laziness-aliases",
    "title": "Packages",
    "section": "More Laziness: Aliases",
    "text": "More Laziness: Aliases\nProgrammers hate typing more than they have to. So if you use the math library a lot then you might object to constantly writing:\narea = math.pi * r**2\nln = math.log(area)\n...\nSo we can use an alias instead:\nimport math as m\narea = m.pi * r**2\n...\nYou will see this used a lot with more complex libraries like Pandas (pd), Geopandas (gpd), and PySAL (ps)."
  },
  {
    "objectID": "lectures/3.5-Packages.html#importing-part-of-a-package",
    "href": "lectures/3.5-Packages.html#importing-part-of-a-package",
    "title": "Packages",
    "section": "Importing Part of a Package",
    "text": "Importing Part of a Package\nSometimes even that is too much typing… or sometimes we only really want one or two things from a much larger package. In that case we can select these specifically:\nfrom math import pi, log10\nprint(pi)\nhelp(log10)"
  },
  {
    "objectID": "lectures/3.5-Packages.html#packages-make-your-life-easier",
    "href": "lectures/3.5-Packages.html#packages-make-your-life-easier",
    "title": "Packages",
    "section": "Packages Make Your Life Easier",
    "text": "Packages Make Your Life Easier\n\nFlying with Python"
  },
  {
    "objectID": "lectures/3.5-Packages.html#resources",
    "href": "lectures/3.5-Packages.html#resources",
    "title": "Packages",
    "section": "Resources",
    "text": "Resources\nA bit of a mish-mash of different explanations:\n\nLearnPython.org\nRealPython: Namespaces and Scope\nReal Python: Modules and Packages\nProrgamiz\nPythonCourse.eu\nTutorialsTeacher.com\nTutorialsPoint.com\nHow to Make a Package in Python\nHow to Create Your First Python Package\nCreate Python Packages for Your Python Code [This is more for distributing]"
  },
  {
    "objectID": "lectures/4.1-Methods.html#well",
    "href": "lectures/4.1-Methods.html#well",
    "title": "Methods",
    "section": "Well…",
    "text": "Well…\nmy_list.append( <value> ) is a function.\nmy_list.append( <value> ) is a special type of function called a method."
  },
  {
    "objectID": "lectures/4.1-Methods.html#whats-a-method-then",
    "href": "lectures/4.1-Methods.html#whats-a-method-then",
    "title": "Methods",
    "section": "What’s a Method Then?",
    "text": "What’s a Method Then?\nIf a package groups useful constants and functions together in one place, then methods are just a way of binding useful constants and functions together with data.\nSo my_list.append(...) is called a list method: it only knows how to append things to lists. It is bound to variables of ‘type list’."
  },
  {
    "objectID": "lectures/4.1-Methods.html#proof",
    "href": "lectures/4.1-Methods.html#proof",
    "title": "Methods",
    "section": "Proof!",
    "text": "Proof!\nmy_list = []\nhelp(my_list)\nThis will give you:\nHelp on list object:\n\nclass list(object)\n |  list(iterable=(), /)\n |  Built-in mutable sequence.\n |  If no argument is given, the constructor creates a new empty list.\n |  Methods defined here:\n | ...\n |  append(self, object, /)\n |      Append object to the end of the list.\n |\n |  clear(self, /)\n |      Remove all items from list.\n |\n |  copy(self, /)\n |      Return a shallow copy of the list.\n | ..."
  },
  {
    "objectID": "lectures/4.1-Methods.html#its-all-methods",
    "href": "lectures/4.1-Methods.html#its-all-methods",
    "title": "Methods",
    "section": "It’s all Methods",
    "text": "It’s all Methods\nmsg = 'Hello World'\ndir(msg)\n['__add__', '__class__', ..., 'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', ... 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']\nAnd then we can inquire about these methods:\nhelp(msg.capitalize)\nHelp on built-in function capitalize:\n\ncapitalize() method of builtins.str instance\n    Return a capitalized version of the string.\n\n    More specifically, make the first character have upper case and the rest lower\n    case."
  },
  {
    "objectID": "lectures/4.1-Methods.html#recap",
    "href": "lectures/4.1-Methods.html#recap",
    "title": "Methods",
    "section": "Recap",
    "text": "Recap\n\nMethods are functions (or constants) that are bound to an object from a class – so lists can append, but strings cannot (they concatenate using ‘+’).\n\nEverything in Python is a class with method: my_list = [] is just an easy way to write: my_list = list(), and my_string = 'Foo' is just an easy way to write my_string = str('Foo'). Even trusty old print is a method!"
  },
  {
    "objectID": "lectures/4.1-Methods.html#the-last-few-bit-of-new-vocabulary",
    "href": "lectures/4.1-Methods.html#the-last-few-bit-of-new-vocabulary",
    "title": "Methods",
    "section": "The Last Few Bit of New Vocabulary",
    "text": "The Last Few Bit of New Vocabulary\nFrom here on out, nearly all of what you learn will be new applications, not new fundamental concepts and terminology.\n\n\n\n\n\n\n\nTerm\nMeans\n\n\n\n\nClass\nThe template for a ‘thing’.\n\n\nObject\nThe instantiated ‘thing’.\n\n\nMethod\nFunctions defined for the class that are available to the object.\n\n\nConstructor\nThe special method that builds new objects of that class.\n\n\nSelf\nA reference to the current object."
  },
  {
    "objectID": "lectures/4.3-Design.html#tree-of-life",
    "href": "lectures/4.3-Design.html#tree-of-life",
    "title": "Object-Oriented Design",
    "section": "Tree of Life",
    "text": "Tree of Life\n{{org-chart}} Life Bacteria Aquifex Thermotoga Bacteroides Cytophaga Planctomyces … Not Bacteria Archaea Pyrodicticum Thermoproteus … Eukaryota Entamoebae Slime Molds Animals Fungi Plants Ciliates …"
  },
  {
    "objectID": "lectures/4.3-Design.html#tree-of-vehicles",
    "href": "lectures/4.3-Design.html#tree-of-vehicles",
    "title": "Object-Oriented Design",
    "section": "Tree of Vehicles",
    "text": "Tree of Vehicles\nMost people would call this a class hierarchy or diagram.\n{{org-chart}} Vehicle Small Bicycle Motorcycle Medium Sedan Coupé Estate Large Bus Van Tractor\nThere is no natural order here: where do unicycles or rickshaws go?"
  },
  {
    "objectID": "lectures/4.3-Design.html#classes-vs-packages",
    "href": "lectures/4.3-Design.html#classes-vs-packages",
    "title": "Object-Oriented Design",
    "section": "Classes vs Packages",
    "text": "Classes vs Packages\n\nFunctionally, a class and a package are indistinguishable, but a class produces objects that use functions and constants, whereas a package is a group of functions and constants that may, or may not, include classes.\n\nUgh, now try to keep this straight in your head."
  },
  {
    "objectID": "lectures/4.3-Design.html#key-takeaways",
    "href": "lectures/4.3-Design.html#key-takeaways",
    "title": "Object-Oriented Design",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nYou’ve been using Classes and Methods since you started.\nYou can ‘package up’ useful code into functions, and useful functions into packages.\nTogether, packages and classes will turbo-charge your programming skills.\nYou can stand on the shoulders of giants!"
  },
  {
    "objectID": "lectures/4.3-Design.html#resources",
    "href": "lectures/4.3-Design.html#resources",
    "title": "Object-Oriented Design",
    "section": "Resources",
    "text": "Resources\n\nWhat is object-oriented programming?\nPython object-oriented programming\nObject-oriented programming refresher\nUnderstanding inheritance\nAbstract base classes\nPython - Object-Oriented # Thank you!\n\n\n\n\nObject-Oriented Design • Jon Reades"
  },
  {
    "objectID": "lectures/5.1-Logic.html#in-code-now",
    "href": "lectures/5.1-Logic.html#in-code-now",
    "title": "Logic",
    "section": "In Code Now",
    "text": "In Code Now\nUsing ‘Booleans’ let’s set x=True and y=False:\nif x:\n  print(\"x\")\nif x and y:\n  print(\"x and y\")\nif x or y:\n  print(\"x or y\")\nif x and not y:\n  print(\"x and not y\")\nif not(x and y):\n  print(\"not x and y\")\n\nif x is the simplest: if True then print \"x\". There is no logic, we just do it or we don’t depending on whether x is True.\nif x and y is the ‘park and ice cream question’: there’s no ice cream without going to the park. So both things have to be True for this to work out happily and print(\"x and y\").\nif x or y is the ‘cereal or toast for breakfast’ question: my daughter is having breakfast either way, so if she answers ‘Yes’ (True) to either then it doesn’t matter that the other one is False, she’s still had breakfast, or we’re still printing “x or y”. And note, if she was really hungry and said yes to both then that would also be breakfast. So if they are both True that’s also fine.\nif x and not y: is the negation of one term. So ‘if you behave and don’t touch that again then you will get a treat’ is what we’re looking for here. But you can also negate the entire thing: not(x and y) is also valid logic. So this turns a False on if x and y into a True."
  },
  {
    "objectID": "lectures/5.1-Logic.html#combining-logic-with-operators",
    "href": "lectures/5.1-Logic.html#combining-logic-with-operators",
    "title": "Logic",
    "section": "Combining Logic With Operators",
    "text": "Combining Logic With Operators\nRemember that operators like <= and == also produce True/False answers. So that means that:\nx = y = 5\nz = 3\nif x==y:\n  print(\"x==y\")\nif x==y and x==z:\n  print(\"x==y and x==z\")\nif x==y or x==z:\n  print(\"x==y or x==z\")\nif x==y and not x==z:\n  print(\"x==y and not x==z\")"
  },
  {
    "objectID": "lectures/5.1-Logic.html#a-special-case",
    "href": "lectures/5.1-Logic.html#a-special-case",
    "title": "Logic",
    "section": "A Special Case",
    "text": "A Special Case\nThere is a second set of logical operators that apply in very specific circumstances. These are called ‘bitwise’ operators and apply to data specified in bits.\n\n\n\nRegular Operator\nBitwise Equivalent\n\n\n\n\nand\n&\n\n\nor\n\\|\n\n\nnot\n~\n\n\n\nLet’s see (briefly) how these work…"
  },
  {
    "objectID": "lectures/5.1-Logic.html#working-with-bits",
    "href": "lectures/5.1-Logic.html#working-with-bits",
    "title": "Logic",
    "section": "Working With Bits",
    "text": "Working With Bits\nLet’s set x=38 and y=3:\nprint(f\"{x:b}\") # `:b` means byte-format\nprint(f\"{y:b}\")\nThis gives us that x is '100110' and y is '11', so now:\nprint(\"{0:b}\".format( x & y ))\n> 10\nprint(\"{0:b}\".format( x | y ))\n> '100111'\nprint(\"{0:b}\".format( x & ~y ))\n> '100100'"
  },
  {
    "objectID": "lectures/5.1-Logic.html#perhaps-easier-to-see-this-way",
    "href": "lectures/5.1-Logic.html#perhaps-easier-to-see-this-way",
    "title": "Logic",
    "section": "Perhaps Easier to See This Way?",
    "text": "Perhaps Easier to See This Way?\n\n\n\nOperator\n1\n2\n3\n4\n5\n6\n\n\n\n\nx\n1\n0\n0\n1\n1\n0\n\n\ny\n0\n0\n0\n0\n1\n1\n\n\nx & y\n0\n0\n0\n0\n1\n0\n\n\nx | y\n1\n0\n0\n1\n1\n1\n\n\n~y\n1\n1\n1\n1\n0\n0\n\n\nx & ~y\n1\n0\n0\n1\n0\n0\n\n\n\nBitwise operations are very, very fast and so are a good way to, say, find things in large data sets. You’ve been warned.\n\nThey are how pandas and numpy manage indexes and queries against data frames."
  },
  {
    "objectID": "lectures/5.1-Logic.html#nulls-none-vs.-nan",
    "href": "lectures/5.1-Logic.html#nulls-none-vs.-nan",
    "title": "Logic",
    "section": "Nulls: None vs. NaN",
    "text": "Nulls: None vs. NaN\nBeware of using logic with things that are not what they appear:\n\nNone is Python’s way of saying that something has no value at all (not 0 or \" \"… but None). It is a class.\nNaN (Not a Number) is a special numeric data type to deal with things like infinite values and other ‘things’ that are not numbers that can be represented with values. np.nan should be used whenever you are dealing with data in bulk (e.g. see Pandas!)."
  },
  {
    "objectID": "lectures/5.1-Logic.html#none-vs.-nan",
    "href": "lectures/5.1-Logic.html#none-vs.-nan",
    "title": "Logic",
    "section": "None vs. NaN",
    "text": "None vs. NaN\nimport numpy as np\nprint(type(np.nan))\n> float\nprint(type(None))\n> NoneType\nCritically:\nprint(np.nan==np.nan)\nprint(np.nan is np.nan)"
  },
  {
    "objectID": "lectures/5.1-Logic.html#membership",
    "href": "lectures/5.1-Logic.html#membership",
    "title": "Logic",
    "section": "Membership",
    "text": "Membership"
  },
  {
    "objectID": "lectures/5.1-Logic.html#in-not-in",
    "href": "lectures/5.1-Logic.html#in-not-in",
    "title": "Logic",
    "section": "In / Not In",
    "text": "In / Not In\nWe’ve touched on these before:\ng = ['Harvey','Rose','Batty','Jefferson']\nif 'Batty' in g:\n  print(\"In the group!\")\nif 'Marx' not in g:\n  print(\"Not in the group!\")\nThere is also a set data type that supports in, and not in.\n\nThis is a good place for a recap though."
  },
  {
    "objectID": "lectures/5.1-Logic.html#sets",
    "href": "lectures/5.1-Logic.html#sets",
    "title": "Logic",
    "section": "Sets",
    "text": "Sets\nYou can also do more complex membership ‘things’ with sets:\ns1 = {'cherry','orange','banana','tomato'} # Or s1(...)\ns2 = {'potato','celery','carrot','tomato'} # Or s2(...)\nprint('potato' in s1) # Same as for lists\n>>> False\nprint(s1.difference(s2))\n>>> {'banana', 'cherry', 'orange'}\nprint(s1.intersection(s2))\n>>> {'tomato'}\nprint(s1.union(s2))\n>>> {'orange', 'carrot', 'tomato', 'cherry', 'celery', 'potato', 'banana'}\n\nThese more advanced functions are only for sets, not lists or dicts."
  },
  {
    "objectID": "lectures/5.1-Logic.html#resources",
    "href": "lectures/5.1-Logic.html#resources",
    "title": "Logic",
    "section": "Resources",
    "text": "Resources\n\nLogical operators: And, or, not\nComparison operators\nBitwise operators\nComparison operators\nBoolean operators\nOperator precedence\nNaN and None in Python\nHandling Missing Data"
  },
  {
    "objectID": "lectures/5.2-Randomness.html#reproducibility-good-or-bad",
    "href": "lectures/5.2-Randomness.html#reproducibility-good-or-bad",
    "title": "Randomness",
    "section": "Reproducibility: Good or Bad?",
    "text": "Reproducibility: Good or Bad?\nDepends on the problem:\n\nBanking and Encryption: bad\nSampling and testing: could go either way\nReproducing research/documentation: good\n\n\nOK, technically, even encryption needs to be reproducible to allow for decryption, but you sure don’t want it to be easy."
  },
  {
    "objectID": "lectures/5.2-Randomness.html#not-very-good-encryption",
    "href": "lectures/5.2-Randomness.html#not-very-good-encryption",
    "title": "Randomness",
    "section": "Not Very Good Encryption",
    "text": "Not Very Good Encryption\n\n\n\nCyphertext\nOutput\n\n\n\n\nROT0\nTo be or not to be, That is the question\n\n\nROT1\nUp cf ps opu up cf, Uibu jt uif rvftujpo\n\n\nROT2\nVq dg qt pqv vq dg, Vjcv ku vjg swguvkqp\n\n\n…\n…\n\n\nROT9\nCx kn xa wxc cx kn, Cqjc rb cqn zdnbcrxw\n\n\n\nKnown as the Caesar Cypher, but since the transformation is simple (A..Z+=x) decryption is easy. How can we make this harder?"
  },
  {
    "objectID": "lectures/5.2-Randomness.html#python-is-random",
    "href": "lectures/5.2-Randomness.html#python-is-random",
    "title": "Randomness",
    "section": "Python is Random",
    "text": "Python is Random\nimport random\nrandom.randint(0,10)\n> 8\nrandom.randint(0,10)\n> 4\nrandom.randint(0,10)\n> 0\nrandom.randint(0,10)\n> 8\nSee also: random.randrange, random.choice, random.sample, random.random, random.gauss, etc."
  },
  {
    "objectID": "lectures/5.2-Randomness.html#and-repeat",
    "href": "lectures/5.2-Randomness.html#and-repeat",
    "title": "Randomness",
    "section": "And Repeat…",
    "text": "And Repeat…\nimport random\nsize = 10\ndata = [0] * size\n\ntests = 100000\nwhile tests > 0:\n    data[random.randint(0,len(data)-1)] += 1\n    tests -= 1\n\nprint(data)\n\nWhat will this return?\nWill it hold for more than 10 numbers?"
  },
  {
    "objectID": "lectures/5.2-Randomness.html#aaaaaaaaaaand-repeat",
    "href": "lectures/5.2-Randomness.html#aaaaaaaaaaand-repeat",
    "title": "Randomness",
    "section": "Aaaaaaaaaaand Repeat",
    "text": "Aaaaaaaaaaand Repeat\nimport random \nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nsize = 1000\ndata = [0] * size\n\ntests = 10000000\nwhile tests > 0:\n    data[random.randint(0,len(data)-1)] += 1\n    tests -= 1\n\nfig = plt.figure()\nplt.bar(np.arange(0,len(data)), data)\nfig.savefig('Random.png', dpi=150, transparent=True)"
  },
  {
    "objectID": "lectures/5.2-Randomness.html#aaaaaaaaaaand-repeat-1",
    "href": "lectures/5.2-Randomness.html#aaaaaaaaaaand-repeat-1",
    "title": "Randomness",
    "section": "Aaaaaaaaaaand Repeat",
    "text": "Aaaaaaaaaaand Repeat"
  },
  {
    "objectID": "lectures/5.2-Randomness.html#seeding-keys",
    "href": "lectures/5.2-Randomness.html#seeding-keys",
    "title": "Randomness",
    "section": "Seeding Keys",
    "text": "Seeding Keys\nTechnically, computers use pseudo-random number generators. These are initialised with a seed to generate sequences.\nThe same seed will yield the same sequence.\n\nYou might look at that uniform distribution and think ‘not bad’, but that’s not good enough for encryption and it’s also not good for reproducibility."
  },
  {
    "objectID": "lectures/5.2-Randomness.html#setting-a-seed",
    "href": "lectures/5.2-Randomness.html#setting-a-seed",
    "title": "Randomness",
    "section": "Setting a Seed",
    "text": "Setting a Seed\nTwo main libraries where seeds are set:\nimport random\nrandom.seed(42)\n\nimport numpy as np\nnp.random.seed(42)\n\nWhy do you often see 42 used as a seed?"
  },
  {
    "objectID": "lectures/5.2-Randomness.html#seeds-and-state",
    "href": "lectures/5.2-Randomness.html#seeds-and-state",
    "title": "Randomness",
    "section": "Seeds and State",
    "text": "Seeds and State\nimport random\nrandom.seed(42)\nst = random.getstate()\nfor r in range(0,3):\n    random.setstate(st)\n    print(f\"Repetition {r}:\")\n    ints = []\n    for i in range(0,10):\n        ints.append(random.randint(0,10))\n    print(f\"\\t{ints}\")"
  },
  {
    "objectID": "lectures/5.2-Randomness.html#question",
    "href": "lectures/5.2-Randomness.html#question",
    "title": "Randomness",
    "section": "Question!",
    "text": "Question!\n\nWhere would you use a mix of randomness and reproducbility as part of a data analysis process?\n\n\nHint: With a large data set it will often be useful to be able draw a random… what? But for reproducible analysis we will that random… what?… to be the same each time!"
  },
  {
    "objectID": "lectures/5.2-Randomness.html#other-applications",
    "href": "lectures/5.2-Randomness.html#other-applications",
    "title": "Randomness",
    "section": "Other Applications",
    "text": "Other Applications"
  },
  {
    "objectID": "lectures/5.2-Randomness.html#hashing",
    "href": "lectures/5.2-Randomness.html#hashing",
    "title": "Randomness",
    "section": "Hashing",
    "text": "Hashing\nChecking for changes (usally in a security context).\nimport hashlib\n\nr1 = hashlib.md5('CASA Intro to Programming'.encode())\nprint(\"The hex equivalent of r1 is: \", end=\"\")\nprint(r1.hexdigest())\n\nr2 = hashlib.md5('CASA Intro to Programming '.encode())\nprint(\"The hex equivalent of r2 is: \", end=\"\")\nprint(r2.hexdigest())\nOutputs:\n\"The hex equivalent of r1 is: acd601db5552408851070043947683ef\"\n\"The hex equivalent of r2 is: 4458e89e9eb806f1ac60acfdf45d85b6\""
  },
  {
    "objectID": "lectures/5.2-Randomness.html#and-note",
    "href": "lectures/5.2-Randomness.html#and-note",
    "title": "Randomness",
    "section": "And Note…",
    "text": "And Note…\nimport requests\nnight = requests.get(\"http://www.gutenberg.org/ebooks/1514.txt.utf-8\")\nprint(night.text[30:70])\nprint(len(night.text))\nr3 = hashlib.md5(night.text.encode())\nprint(\"The hex equivalent of r3 is: \", end=\"\") \nprint(r3.hexdigest())\nOutputs:\n112127\n\"A Midsummer Night's Dream by Shakespeare\"\n\"The hex equivalent of r3 is: acd601db5552408851070043947683ef\"\n\nCan be applied to anything: even one byte’s difference (e.g. in a application) can lead to a different hash output.\nBut notice too that hashes are always the same length."
  },
  {
    "objectID": "lectures/5.2-Randomness.html#jupyterlab-password",
    "href": "lectures/5.2-Randomness.html#jupyterlab-password",
    "title": "Randomness",
    "section": "JupyterLab Password",
    "text": "JupyterLab Password\nYou may have noticed this in Vagrantfile/Docker:\n'sha1:5b1c205a53e14e:0ce169b9834984347d62b20b9a82f6513355f72d'\nHow this was generated:\nimport uuid, hashlib\nsalt = uuid.uuid4().hex[:16] # Truncate salt\npassword = 'casa2021'\nhashed_password = hashlib.sha1(password.encode() + \n                  salt.encode()).hexdigest()\nprint(':'.join(['sha1',salt,hashed_password]))\nReplace the JUPYTER_PWD parameter in the start-up string for Docker or Vagrant.\nDon’t set your passwords this way."
  },
  {
    "objectID": "lectures/5.2-Randomness.html#encryption-security",
    "href": "lectures/5.2-Randomness.html#encryption-security",
    "title": "Randomness",
    "section": "Encryption & Security",
    "text": "Encryption & Security\nSimple hashing algorithms are not normally secure enough for full encryption. Genuine security training takes a whole degree + years of experience.\nAreas to look at if you get involved in applications:\n\nPublic and Private Key Encryption (esp. OpenSSL)\nPrivileges used by Applications (esp. Docker)\nRevocable Tokens (e.g. for APIs)\nInjection Attacks (esp. for SQL)"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#why-pandas",
    "href": "lectures/5.4-Pandas.html#why-pandas",
    "title": "Pandas",
    "section": "Why Pandas?",
    "text": "Why Pandas?\nPandas is probably (together with scipy and numpy) the main reason that Python has become popular for data science. According to ‘Learn Data Sci’ it accounts for 1% of all Stack Overflow question views!\nYou will want to bookmark these:\n\npandas.pydata.org\nPandas Docs\npandas tutorial for beginners"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#pandas-terminology-data-frame",
    "href": "lectures/5.4-Pandas.html#pandas-terminology-data-frame",
    "title": "Pandas",
    "section": "Pandas Terminology (Data Frame)",
    "text": "Pandas Terminology (Data Frame)"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#pandas-terminology-index",
    "href": "lectures/5.4-Pandas.html#pandas-terminology-index",
    "title": "Pandas",
    "section": "Pandas Terminology (Index)",
    "text": "Pandas Terminology (Index)"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#pandas-terminology-series",
    "href": "lectures/5.4-Pandas.html#pandas-terminology-series",
    "title": "Pandas",
    "section": "Pandas Terminology (Series)",
    "text": "Pandas Terminology (Series)"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#pandas-terminology-slice",
    "href": "lectures/5.4-Pandas.html#pandas-terminology-slice",
    "title": "Pandas",
    "section": "Pandas Terminology (Slice)",
    "text": "Pandas Terminology (Slice)"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#using-pandas",
    "href": "lectures/5.4-Pandas.html#using-pandas",
    "title": "Pandas",
    "section": "Using Pandas",
    "text": "Using Pandas\nSo here’s some code to read a CSV file:\nimport pandas as pd      # import package\n# Bitly link for: https://github.com/jreades/i2p/raw/master/data/src/2019-sample-Crime.csv\nurl='https://bit.ly/31NP4fx'\ndf = pd.read_csv(url)    # load a (remote) CSV\ntype(df)                 # not simple data type\nprint(df.columns.values) # column names\nOutput:\n<class 'pandas.core.frame.DataFrame'>\n['ID' 'Case Number' 'Date' 'Primary Type' 'Description'\n 'Location Description' 'Arrest' 'Domestic' 'Year' 'Latitude' 'Longitude']"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#summarise-a-data-frame",
    "href": "lectures/5.4-Pandas.html#summarise-a-data-frame",
    "title": "Pandas",
    "section": "Summarise a Data Frame",
    "text": "Summarise a Data Frame\ndf.describe() # Information about each Series\ndf.info()     # Information about each Series and the df\ndf.info is more about data types and memory usage. df.describe is for summarising information about the distribution of values in every series."
  },
  {
    "objectID": "lectures/5.4-Pandas.html#familiar",
    "href": "lectures/5.4-Pandas.html#familiar",
    "title": "Pandas",
    "section": "Familiar?",
    "text": "Familiar?\nThis should be looking eerily familiar:\nprint(type(df['Latitude']))          # type for column\nprint(type(df['Latitude'].values))   # type for values\nprint(df['Latitude'].values[:5])     # first five values\nprint(f\"1: {df['Latitude'].mean()}\") # summarise a series/column\nprint(f\"2: {df.Latitude.mean()}\")    # if no spaces in name\nProduces:\n<class 'pandas.core.series.Series'>\n<class 'numpy.ndarray'>\n[41.75130706 41.90399688 41.88032861 41.92438396 41.75579713]\n1: 41.84550008439\n2: 41.84550008439"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#jupyter-formatting",
    "href": "lectures/5.4-Pandas.html#jupyter-formatting",
    "title": "Pandas",
    "section": "Jupyter Formatting",
    "text": "Jupyter Formatting\nPandas is also ‘Jupyter-aware’, meaning that output can displayed directly in Jupyter in ‘fancy’ ways:"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#familiar-1",
    "href": "lectures/5.4-Pandas.html#familiar-1",
    "title": "Pandas",
    "section": "Familiar?",
    "text": "Familiar?\ndf.head(3)                   # all columns first 3 rows\ndf[['ID','Date','Year']].tail(3) # selected columns, last 3 rows\ndf.sample(frac=0.3)           # a random 30% sample\ndf.sample(3, random_state=42) # set state of random number generator\ndf.sample(3, random_state=42) # will yield same sample"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#data-frames-vs-series",
    "href": "lectures/5.4-Pandas.html#data-frames-vs-series",
    "title": "Pandas",
    "section": "Data Frames vs Series",
    "text": "Data Frames vs Series\nAny operation on a data frame returns a data frame.\nAny operation on a series returns a series.\nBy default, any operation returns a shallow copy and the original is unchanged unless you specify inplace=True (for methods that allow it).\nIf you need a full copy then use the copy() method (e.g. df.copy() or df.Series.copy())."
  },
  {
    "objectID": "lectures/5.4-Pandas.html#what-can-we-do",
    "href": "lectures/5.4-Pandas.html#what-can-we-do",
    "title": "Pandas",
    "section": "What Can We Do?",
    "text": "What Can We Do?"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#chaining",
    "href": "lectures/5.4-Pandas.html#chaining",
    "title": "Pandas",
    "section": "Chaining",
    "text": "Chaining\nOperations on a Data Frame return a DataFrame and operations on a Series return a Series, allowing us to ‘chain’ steps together:\ndf.sort_values(['Year','ID'], ascending=False).head(20).median()"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#selection",
    "href": "lectures/5.4-Pandas.html#selection",
    "title": "Pandas",
    "section": "Selection",
    "text": "Selection\nWe can subset the data using selection criteria\n# All rows where Primary Type is ASSAULT\ndf[\n  df['Primary Type']=='ASSAULT'\n]\n\n# Calculations on a slice (returns mean centroid!)\ndf[df['Primary Type']=='ASSAULT'][['Longitude','Latitude']].mean()\n\n# Two conditions with a bit-wise AND\ndf[\n  (df['Primary Type']=='ASSAULT') &\n  (df['Description']=='AGGRAVATED: HANDGUN')\n]\n\n# Two conditions with a bit-wise OR\ndf[\n  (df['Primary Type']=='ASSAULT') |\n  (df['Primary Type']=='THEFT')\n]"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#now-we-can-automate-data-anlysis",
    "href": "lectures/5.4-Pandas.html#now-we-can-automate-data-anlysis",
    "title": "Pandas",
    "section": "Now we can automate… data anlysis!",
    "text": "Now we can automate… data anlysis!"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#dealing-with-types",
    "href": "lectures/5.4-Pandas.html#dealing-with-types",
    "title": "Pandas",
    "section": "Dealing with Types",
    "text": "Dealing with Types\nA Data Series can only be of one type:\n\n\n\nPandas Dtype\nPython Type\nUsage\n\n\n\n\nobject\nstr or mixed\nText or mixed columns\n\n\nint64\nint\nInteger columns\n\n\nfloat64\nfloat\nFloating point columns\n\n\nbool\nbool\nTrue/False columns\n\n\ndatetime64\nN/A (datetime)\nDate and time columns\n\n\ntimedelta[ns]\nN/A (datetime)\nDatetime difference columns\n\n\ncategory\nN/A (set)\nCategorical columns"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#changing-the-type",
    "href": "lectures/5.4-Pandas.html#changing-the-type",
    "title": "Pandas",
    "section": "Changing the Type",
    "text": "Changing the Type\nprint(df['Primary Type'].unique()) # Find unique values\nprint(df['Primary Type'].dtype.name)\ndf['Primary Type'] = df['Primary Type'].astype('category')\nprint(df['Primary Type'].dtype.name)\nprint(df['Primary Type'].describe())\nOutputs:\n['BURGLARY' 'DECEPTIVE PRACTICE' 'BATTERY'...]\nobject   # < before `as type`\ncategory # < after `as type`\ncount       100\nunique       15\ntop       THEFT\nfreq         28\nName: Primary Type, dtype: object # category==special type of object"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#datetime-data",
    "href": "lectures/5.4-Pandas.html#datetime-data",
    "title": "Pandas",
    "section": "Datetime Data",
    "text": "Datetime Data\nWhat do we do here?\ndf.Date.to_list()[:3]\n>>> ['04/20/2019 11:00:00 PM', '12/02/2019 10:35:00 AM', '10/06/2019 04:50:00 PM']\nPandas handles date and times using a datetime type that also works as an index (more on these later):\ndf['dt'] = pd.to_datetime(df.Date.values, \n              format=\"%m/%d/%Y %H:%M:%S %p\")\nThese follow the formatting conventions of strftime (string format time) for conversion."
  },
  {
    "objectID": "lectures/5.4-Pandas.html#datetime-formats",
    "href": "lectures/5.4-Pandas.html#datetime-formats",
    "title": "Pandas",
    "section": "Datetime Formats",
    "text": "Datetime Formats\nExamples of strftime conventions include:\n\n\n\nFormat\nApplies To\n\n\n\n\n%d\n2-digit day\n\n\n%m\n2-digit month\n\n\n%y\n2-digit year\n\n\n%Y\n4-digit year\n\n\n%p\nAM/PM"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#tidying-up",
    "href": "lectures/5.4-Pandas.html#tidying-up",
    "title": "Pandas",
    "section": "Tidying Up",
    "text": "Tidying Up\nThis is one way, there are many subtleties…\n# Fix categories\nmapping = {}\n# df['Primary Type'].unique().to_list() also works\nfor x in df['Primary Type'].cat.categories.to_list():\n    mapping[x]=x.title()\ndf['Primary Type'] = df['Primary Type'].cat.\n                             rename_categories(mapping)\nThere are many other things that we can do, many of them accessible via Google! For instance, to deal with pricing information treated as a string:\ndf2['price'].str.replace('$','').astype(float)"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#other-types-of-tidying-up",
    "href": "lectures/5.4-Pandas.html#other-types-of-tidying-up",
    "title": "Pandas",
    "section": "Other Types of Tidying Up",
    "text": "Other Types of Tidying Up\nThe inplace=True option means that df itself is changed. Otherwise df is not touched and a copy is returned:\nprint(df.columns.values)\ndf.drop(columns=['Year'], inplace=True)\nprint(df.columns.values)\nThere is also df.dropna() which can apply to rows or columns with NULL values.\nColumns are dropped by df.drop(index=[...]); however, I often prefer: df = df[df.ID>11934410] (selection!).\n Why might you want the default to not be in_place?"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#accessing-data-by-location",
    "href": "lectures/5.4-Pandas.html#accessing-data-by-location",
    "title": "Pandas",
    "section": "Accessing Data by Location",
    "text": "Accessing Data by Location\n\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n\nID\nCase Number\nDate\nPrimary Type\n\n\n0\n11667185\nJC237601\n04/20/2020 11:00:00PM\nBURGLARY\n\n\n1\n11998178\nJC532226\n12/02/2020 10:35:00AM\nDECEPTIVE PRACTICE\n\n\n2\n11852571\nJC462365\n10/06/2020 04:50:00PM\nBATTERY\n\n\n\nWe can interact with rows and columns by position or name:\ndf.iloc[0:2,0:2] # List selection! (':' means 'all')\ndf.loc[0:2,['ID','Case Number']] # Dict selection\nThese actually return different results because of the index: 0:2 in df.loc returns the rows labeled 0, 1, and 2!"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#indexes",
    "href": "lectures/5.4-Pandas.html#indexes",
    "title": "Pandas",
    "section": "Indexes",
    "text": "Indexes\nSo by default, pandas creates a row index index whose values are 0..n and column index whose values are the column names. You will see this if you print out the head:\ndf.head(3)\nThe left-most column (without) a name is the index.\ndf.set_index('ID', inplace=True)\ndf.head(3)"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#indexes-contd",
    "href": "lectures/5.4-Pandas.html#indexes-contd",
    "title": "Pandas",
    "section": "Indexes (cont’d)",
    "text": "Indexes (cont’d)\n\n\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n\nID\nCase Number\nDate\nPrimary Type\n\n\n0\n11667185\nJC237601\n04/20/2020 11:00:00PM\nBURGLARY\n\n\n1\n11998178\nJC532226\n12/02/2020 10:35:00AM\nDECEPTIVE PRACTICE\n\n\n2\n11852571\nJC462365\n10/06/2020 04:50:00PM\nBATTERY\n\n\n\nAnd now:\nprint(df.loc[11667185,:])\nprint(df.loc[11667185:11852571,'Case Number':'Date'])\nNotice! We used iloc to select rows/cols based on integer location and we can use loc to select rows/cols based on name location.\nP.S. You can reset the index using df.reset_index(inplace=True)."
  },
  {
    "objectID": "lectures/5.4-Pandas.html#saving",
    "href": "lectures/5.4-Pandas.html#saving",
    "title": "Pandas",
    "section": "Saving",
    "text": "Saving\nPandas can write to a wide range of file types, here are some of the more popular ones:\n\n\n\nCommand\nSaved As…\n\n\n\n\ndf.to_csv(<path>)\nCSV file. But note the options to change sep (default is ',') and to suppress index output (index=False).\n\n\ndf.to_excel(<path>)\nXLSX file. But note the options to specify a sheet_name, na_rep, and so on, as well as to suppress the index (index=False).\n\n\ndf.to_feather(<path>)\nFeather file also directly usable by R. Requires pyarrow to be installed to access the options.\n\n\ndf.to_latex(<path>))\nWrite to LaTeX file to include into main body. Requires booktabs. Could to copy+pasted with print(df.to_latex()).\n\n\ndf.to_markdown(<path>)\nWrite to Markdown format. Requires tabulate. Could do copy+paste with print(df.to_markdown()).\n\n\n\nIn most cases compression is supported automatically (e.g. df.to_csv('file.csv.gz'))."
  },
  {
    "objectID": "lectures/5.4-Pandas.html#resources",
    "href": "lectures/5.4-Pandas.html#resources",
    "title": "Pandas",
    "section": "Resources",
    "text": "Resources\n\nData Cleaning with Numpy and Pandas\nPandas dtypes\nThe Index Explained\nUsing Pandas iloc\nA Clear Explanation of the Pandas Index\nUfuncs and Apply"
  },
  {
    "objectID": "lectures/6.1-Mapping.html#the-challenge",
    "href": "lectures/6.1-Mapping.html#the-challenge",
    "title": "Computational Mapping",
    "section": "The Challenge",
    "text": "The Challenge\nThe hardest part of purely computational approaches is the need to anticipate how maps will look according to variations in:\n\nThe density and type of data\nThe context of the data\nThe different scales involved\nThe number of maps involved\nThe need to annotate and label elements\n\nUltimately, the complexity of the choices here may require the use of a scriptable GIS over ggplot or matplotlib.\n\nDon’t forget that both QGIS and Arc offer a ‘Model Builder’ that is basically ‘visual programming’."
  },
  {
    "objectID": "lectures/6.1-Mapping.html#constituency-cards",
    "href": "lectures/6.1-Mapping.html#constituency-cards",
    "title": "Computational Mapping",
    "section": "Constituency Cards",
    "text": "Constituency Cards\nClone and reproduce: github.com/alasdairrae/wpc and explanation: cconstituency cards."
  },
  {
    "objectID": "lectures/6.1-Mapping.html#short-term-lets-in-scotland",
    "href": "lectures/6.1-Mapping.html#short-term-lets-in-scotland",
    "title": "Computational Mapping",
    "section": "Short-Term Lets in Scotland",
    "text": "Short-Term Lets in Scotland\nAnalysis of Airbnb and other short-term lets in Scotland feeding through into policy-making via Research into the impact of short-term lets on communities across Scotland"
  },
  {
    "objectID": "lectures/6.1-Mapping.html#every-building-in-america",
    "href": "lectures/6.1-Mapping.html#every-building-in-america",
    "title": "Computational Mapping",
    "section": "Every Building in America",
    "text": "Every Building in America\nBuilding footprints collected by Microsoft, but presentation by New York Times highlights society-nature interactions."
  },
  {
    "objectID": "lectures/6.1-Mapping.html#think-it-through",
    "href": "lectures/6.1-Mapping.html#think-it-through",
    "title": "Computational Mapping",
    "section": "Think it Through!",
    "text": "Think it Through!"
  },
  {
    "objectID": "lectures/6.1-Mapping.html#colouring-in",
    "href": "lectures/6.1-Mapping.html#colouring-in",
    "title": "Foundations of Spatial Data Science",
    "section": "Colouring in",
    "text": "Colouring in"
  },
  {
    "objectID": "lectures/6.1-Mapping.html#a-deceptively-simple-problem",
    "href": "lectures/6.1-Mapping.html#a-deceptively-simple-problem",
    "title": "Computational Mapping",
    "section": "A Deceptively Simple Problem",
    "text": "A Deceptively Simple Problem\n\nWe want to show data on a map in a way that is both accurate and informative.\n\nWhy might this not be possible?"
  },
  {
    "objectID": "lectures/6.1-Mapping.html#classification",
    "href": "lectures/6.1-Mapping.html#classification",
    "title": "Computational Mapping",
    "section": "Classification",
    "text": "Classification\nTrade-offs:\n\nThe greater the accuracy of a choropleth or other class-based map, the less it’s possible generalise from it.\nThere is no ‘right’ way to group data into an arbitrary number of discrete classes (a.k.a. to generalise).\n\nHumans can only take in so much data at once. Your choice of colour scheme, breaks, and classification can profoundly affect how people see the world."
  },
  {
    "objectID": "lectures/6.1-Mapping.html#six-views-of-employment",
    "href": "lectures/6.1-Mapping.html#six-views-of-employment",
    "title": "Computational Mapping",
    "section": "Six Views of Employment",
    "text": "Six Views of Employment"
  },
  {
    "objectID": "lectures/6.1-Mapping.html#six-views-of-employment-1",
    "href": "lectures/6.1-Mapping.html#six-views-of-employment-1",
    "title": "Computational Mapping",
    "section": "Six Views of Employment",
    "text": "Six Views of Employment"
  },
  {
    "objectID": "lectures/6.1-Mapping.html#six-views-of-employment-2",
    "href": "lectures/6.1-Mapping.html#six-views-of-employment-2",
    "title": "Computational Mapping",
    "section": "Six Views of Employment",
    "text": "Six Views of Employment"
  },
  {
    "objectID": "lectures/6.1-Mapping.html#consider",
    "href": "lectures/6.1-Mapping.html#consider",
    "title": "Computational Mapping",
    "section": "Consider",
    "text": "Consider\nWe want to: - Group features with similar values together. - Show these in a way that doesn’t mislead the viewer.\nBut we have the following problems: - Too many classes confuse the viewer. - Too few classes hides structure/pattern."
  },
  {
    "objectID": "lectures/6.1-Mapping.html#choices-choices",
    "href": "lectures/6.1-Mapping.html#choices-choices",
    "title": "Computational Mapping",
    "section": "Choices, Choices",
    "text": "Choices, Choices\nAt the very least we have the following options:\n\nAssign classes manually.\nSplit range evenly (i.e. equal intervals).\nSplit data evenly (i.e. quantiles).\nSplit data according to distribution (i.e. SD).\nSplit data so that members of each group are more similar to each other than to members of another group (i.e. natural breaks/Jencks)."
  },
  {
    "objectID": "lectures/6.1-Mapping.html#whats-best",
    "href": "lectures/6.1-Mapping.html#whats-best",
    "title": "Computational Mapping",
    "section": "What’s Best?",
    "text": "What’s Best?\nLook at the Data!"
  },
  {
    "objectID": "lectures/6.1-Mapping.html#takeaway-maps-have-a-rhetoric",
    "href": "lectures/6.1-Mapping.html#takeaway-maps-have-a-rhetoric",
    "title": "Computational Mapping",
    "section": "Takeaway: Maps have a ‘Rhetoric’",
    "text": "Takeaway: Maps have a ‘Rhetoric’"
  },
  {
    "objectID": "lectures/6.1-Mapping.html#resources",
    "href": "lectures/6.1-Mapping.html#resources",
    "title": "Computational Mapping",
    "section": "Resources",
    "text": "Resources\n\nQGIS Styles to Share\nQGIS and 3D Visualisation\nModelling your data processing flow in QGIS\nQGIS Documentation"
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#basic-functionality",
    "href": "lectures/6.2-Geopandas.html#basic-functionality",
    "title": "Geopandas",
    "section": "Basic Functionality",
    "text": "Basic Functionality"
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#reading-writing",
    "href": "lectures/6.2-Geopandas.html#reading-writing",
    "title": "Geopandas",
    "section": "Reading & Writing",
    "text": "Reading & Writing\nSupported file formats:\n\n\n\nType\nExtension\nNotes\n\n\n\n\nShape\n.shp (etc.)\nMaximum compatibility\n\n\nGeoPackage\n.gpkg\nGood default choice\n\n\nGeoJSON\n.geojson\nFor web mapping\n\n\nZip\n.zip\nFor use with Shapefiles\n\n\nWKT\n.txt\nPlain-text & SQL\n\n\n\nAdditionally, it is possible to read only subsets of the data using row, column, geometry, and bbox filters."
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#reading-remote-files",
    "href": "lectures/6.2-Geopandas.html#reading-remote-files",
    "title": "Geopandas",
    "section": "Reading (Remote Files)",
    "text": "Reading (Remote Files)\nAgain, depending on file size you may want to save these locally, but…\nimport geopandas as gpd\ngpkg_src = 'https://bit.ly/2K4JcsB'\nworld = gpd.read_file(gpkg_src)\nworld.plot(facecolor='white', edgecolor='darkblue')"
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#writing-local-files",
    "href": "lectures/6.2-Geopandas.html#writing-local-files",
    "title": "Geopandas",
    "section": "Writing (Local Files)",
    "text": "Writing (Local Files)\nCan apparently write any OGR-supported vector drivers.\nworld.to_file('world.gpkg', driver='GPKG')\nworld.to_file('world.shp') # driver='ESRI Shapefile'\nworld.to_file('world.geojson', driver='GeoJSON')\n\nIf you forget to specify the driver this writes shapefiles to a directory called world.gpkg!"
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#data-structures",
    "href": "lectures/6.2-Geopandas.html#data-structures",
    "title": "Geopandas",
    "section": "Data Structures",
    "text": "Data Structures\nGeoPandas does all this by adding two new classes:\n\nGeoDataFrame\nGeoSeries\n\nIn principle, a GeoSeries can contain multiple geo-data types, but in practice you’ll want to be one of the following shapely classes:\n\nPoints / Multi-Points\nLines / Multi-Lines\nPolygons / Multi-Polygons"
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#consider",
    "href": "lectures/6.2-Geopandas.html#consider",
    "title": "Geopandas",
    "section": "Consider",
    "text": "Consider\nRecall that we can ask if a particular object is an instance of any given class:\nprint(isinstance(world, str))\nprint(isinstance(world, gpd.GeoDataFrame))\nprint(isinstance(world, pd.DataFrame))\nPrints: False, True, True.\nprint(isinstance(world.geometry, str))\nprint(isinstance(world.geometry, pd.Series))\nprint(isinstance(world.geometry, gpd.GeoSeries))\nAlso prints: False, True, True.\n\nSo converting from Pandas to GeoPandas works well because GeoPandas knows all about Pandas.\nYou can use a GeoDataFrame anywhere you’d use a DataFrame with no loss of functionality! Same for a GeoSeries, though in this case a GeoSeries cannot perform the same statistical operations."
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#projections",
    "href": "lectures/6.2-Geopandas.html#projections",
    "title": "Geopandas",
    "section": "Projections",
    "text": "Projections\nDepending on your data source, you may or may not have projection information attached to your GeoDataFrame:\n>>> print(world.crs)\nepsg:4326\nBut:\n>>> world.crs\n<Geographic 2D CRS: EPSG:4326>\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich"
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#finding-projections",
    "href": "lectures/6.2-Geopandas.html#finding-projections",
    "title": "Geopandas",
    "section": "Finding Projections",
    "text": "Finding Projections\nYou can pretty much find any EPSG you might need at epsg.io. By far the most commonly-used here are:\n\nEPSG:4326 for the World Geodetic System 84 used in GPS.\nEPSG:27700 for OSGB 1936/British National Grid used in the UK.\n\nNote: large territories (such as Canada, China and Russia) may well have multiple projections at the state of provincial level."
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#reprojection",
    "href": "lectures/6.2-Geopandas.html#reprojection",
    "title": "Geopandas",
    "section": "Reprojection",
    "text": "Reprojection\nFor data sets without projection information (i.e. anything loaded from a shapefile) you must gdf.set_crs(<spec>). For all others you should gdf.to_crs(<spec>).\nworld2 = world.to_crs('ESRI:54030')\nworld2.plot()\n\n\nDifferent geometry columns can have different projections."
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#standards",
    "href": "lectures/6.2-Geopandas.html#standards",
    "title": "Geopandas",
    "section": "Standards?",
    "text": "Standards?\n\nCan you think why the European Petroleum Standards Group is one of the big drivers behind (and users of) high-quality projection data?\n\n\nHint: you really don’t want to miss what?"
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#the-spatial-index",
    "href": "lectures/6.2-Geopandas.html#the-spatial-index",
    "title": "Geopandas",
    "section": "The Spatial Index",
    "text": "The Spatial Index\nWe can use GeoSeries’ spatial index directly to perform simple spatial queries:\nwslice = world.cx[-50:50, -20:20]\nax = wslice.plot()"
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#attributes",
    "href": "lectures/6.2-Geopandas.html#attributes",
    "title": "Geopandas",
    "section": "Attributes",
    "text": "Attributes\nA GeoSeries has attributes like any other Series, but also includes some spatially-specifc ones:\n\narea — if a polygon\nbounds — for each feature\ntotal_bounds — for each GeoSeries\ngeom_type — if you don’t already know\nis_valid — if you need to test"
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#methods",
    "href": "lectures/6.2-Geopandas.html#methods",
    "title": "Geopandas",
    "section": "Methods",
    "text": "Methods\nAdditional GeoSeries methods icnlude:\n\ndistance() — returns Series measuring distances to some other feature (called as: <GeoSeries>.distance(<feature>))\ncentroid — returns GeoSeries of strict centroids (called as: <GeoSeries>.centroid)\nrepresentative_point() — returns GeoSeries of points within features\nto_crs() and plot(), which you’ve already seen.\n\n\nNote that centroid is not called with parentheses. Technically it’s more like an attribute than a method."
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#relationship-tests",
    "href": "lectures/6.2-Geopandas.html#relationship-tests",
    "title": "Geopandas",
    "section": "Relationship Tests",
    "text": "Relationship Tests\nSimple geographical tests:\n\ngeom_almost_equals() — tries to deal with rounding issues when comparing two features.\ncontains() — is shape contained within some other features.\nintersects() — does shape intersect some other features."
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#point-data",
    "href": "lectures/6.2-Geopandas.html#point-data",
    "title": "Geopandas",
    "section": "Point Data",
    "text": "Point Data"
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#converting-non-spatial-data-1",
    "href": "lectures/6.2-Geopandas.html#converting-non-spatial-data-1",
    "title": "Geopandas",
    "section": "Converting Non-Spatial Data 1",
    "text": "Converting Non-Spatial Data 1\nLat/Long and Northing/Easting benefit from a helper function:\nurl = 'https://bit.ly/3owocdI'\ndf  = pd.read_csv(url)\n\ngdf = gpd.GeoDataFrame(df, \n            geometry=gpd.points_from_xy(\n                        df['longitude'], \n                        df['latitude'], \n                        crs='epsg:4326'\n            )\n      )\ngdf.plot()\n\nYou can also use list comprehensions ([x for x in list]) and zip to combine two lists!"
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#csv-to-points-in-3-lines",
    "href": "lectures/6.2-Geopandas.html#csv-to-points-in-3-lines",
    "title": "Geopandas",
    "section": "CSV to Points in 3 Lines!",
    "text": "CSV to Points in 3 Lines!\n\n\nNotice that the default plot from a GeoDataFrame is… a map!"
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#converting-non-spatial-data-2",
    "href": "lectures/6.2-Geopandas.html#converting-non-spatial-data-2",
    "title": "Geopandas",
    "section": "Converting Non-Spatial Data 2",
    "text": "Converting Non-Spatial Data 2\nOther feature types need to be in some kind of regular format such as Well-Known Text (WKT), GeoJSON, or something readable as a Shapely geometry.\nbbox = 'POLYGON((5000000.0 2500000.0, 5000000.0 -2500000.0, -5000000.0 -2500000.0, -5000000.0 2500000.0, 5000000.0 2500000.0))'\n\nfrom shapely import wkt\nbgdf = gpd.GeoDataFrame(\n                {'id':[0], 'coordinates':bbox})\nbgdf['geometry'] = bgdf.coordinates.apply(wkt.loads)\nbgdf = bgdf.set_crs('ESRI:54030')\n\nThese are more rarely used for our purposes but knowing that they exist is useful."
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#from-text-to-bounding-box",
    "href": "lectures/6.2-Geopandas.html#from-text-to-bounding-box",
    "title": "Geopandas",
    "section": "From Text to Bounding Box",
    "text": "From Text to Bounding Box\nscale = int(float('1e7'))\nf,ax=plt.subplots(figsize=(8,4))\nworld2.plot(ax=ax)\nbgdf.plot(ax=ax, color='none', edgecolor='r')\nax.set_xlim([-0.75*scale, +0.75*scale])\nax.set_ylim([-3*scale/10, +3*scale/10])"
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#resources",
    "href": "lectures/6.2-Geopandas.html#resources",
    "title": "Geopandas",
    "section": "Resources",
    "text": "Resources\n\nGeoPandas on ReadTheDocs\nDani’s GDS Course\nDani’s Web Mapping Course\nDani’s GDS’19 Course"
  },
  {
    "objectID": "lectures/6.3-EDA.html#epicyclic-feedback",
    "href": "lectures/6.3-EDA.html#epicyclic-feedback",
    "title": "Exploratory Data Analysis",
    "section": "Epicyclic Feedback",
    "text": "Epicyclic Feedback\nPeng and Matsui, The Art of Data Science, p.8\n\n\n\n\n\n\n\n\n\n\nSet Expectations\nCollect Information\nRevise Expectations\n\n\n\n\nQuestion\nQuestion is of interest to audience\nLiterature search/experts\nSharpen question\n\n\nEDA\nData are appropriate for question\nMake exploratory plots\nRefine question or collect more data\n\n\nModelling\nPrimary model answers question\nFit secondary models / analysis\nRevise model to include more predictors\n\n\nInterpretation\nInterpretation provides specific and meaningful answer\nInterpret analyses with focus on effect and uncertainty\nRevise EDA and/or models to provide more specific answers\n\n\nCommunication\nProcess & results are complete and meaningful\nSeek feedback\nRevises anlyses or approach to presentation"
  },
  {
    "objectID": "lectures/6.3-EDA.html#approaching-eda",
    "href": "lectures/6.3-EDA.html#approaching-eda",
    "title": "Exploratory Data Analysis",
    "section": "Approaching EDA",
    "text": "Approaching EDA\nThere’s no hard and fast way of doing EDA, but as a general rule you’re looking to:\n\nClean\nCanonicalise\nClean More\nVisualise & Describe\nReview\nClean Some More\n…\n\n\nCleaning Part 1: testing validity of records (possibly while tracking rejected records for subsequent analysis)\nCanonicalisation: controling for variation (e.g. typos, capitalisation, formatting, leading/trailing whitespace, different types of NULL values, etc.) and in a spatial context deal with projection and geo-data issues.\nCleaning Part 2: further testing of records (e.g. deciding what to do with NaNs, missing values, outside of study area, etc.)\nVisualise & Describe: covered in QM but we’ll take a high-level look at this."
  },
  {
    "objectID": "lectures/6.3-EDA.html#a-related-take",
    "href": "lectures/6.3-EDA.html#a-related-take",
    "title": "Exploratory Data Analysis",
    "section": "A Related Take",
    "text": "A Related Take\nFrom: EDA—Don’t ask how, ask what:\n\nDescriptive Statistics: get a high-level understanding of your dataset\nMissing values: come to terms with how bad your dataset is\nDistributions and Outliers: and why countries that insist on using different units make our jobs so much harder\nCorrelations: and why sometimes even the most obvious patterns still require some investigating"
  },
  {
    "objectID": "lectures/6.3-EDA.html#another-take",
    "href": "lectures/6.3-EDA.html#another-take",
    "title": "Exploratory Data Analysis",
    "section": "Another Take",
    "text": "Another Take\nHere’s another view of how to do EDA:\n\nPreview data\nCheck total number of entries and column types\nCheck any null values\nCheck duplicate entries\nPlot distribution of numeric data (univariate and pairwise joint distribution)\nPlot count distribution of categorical data\nAnalyse time series of numeric data by daily, monthly and yearly frequencies"
  },
  {
    "objectID": "lectures/6.3-EDA.html#signal-noise",
    "href": "lectures/6.3-EDA.html#signal-noise",
    "title": "Exploratory Data Analysis",
    "section": "Signal & Noise",
    "text": "Signal & Noise"
  },
  {
    "objectID": "lectures/6.3-EDA.html#what-is-it",
    "href": "lectures/6.3-EDA.html#what-is-it",
    "title": "Exploratory Data Analysis",
    "section": "What is it?",
    "text": "What is it?"
  },
  {
    "objectID": "lectures/6.3-EDA.html#what-is-it-1",
    "href": "lectures/6.3-EDA.html#what-is-it-1",
    "title": "Exploratory Data Analysis",
    "section": "What is it?",
    "text": "What is it?"
  },
  {
    "objectID": "lectures/6.3-EDA.html#what-is-it-2",
    "href": "lectures/6.3-EDA.html#what-is-it-2",
    "title": "Exploratory Data Analysis",
    "section": "What is it?",
    "text": "What is it?"
  },
  {
    "objectID": "lectures/6.3-EDA.html#start-with-a-chart",
    "href": "lectures/6.3-EDA.html#start-with-a-chart",
    "title": "Exploratory Data Analysis",
    "section": "Start with a Chart",
    "text": "Start with a Chart\nThe problem of relying on statistics alone was amply illustrated by Anscombe’s Quartet (1973)…\n\nWe are not very good at looking at spreadsheets.\nWe are very good at spotting patterns visually.\n\nSometimes, we are too good; that’s where the stats comes in. Think of it as the ‘tiger in the jungle’ problem.."
  },
  {
    "objectID": "lectures/6.3-EDA.html#anscombes-quartet",
    "href": "lectures/6.3-EDA.html#anscombes-quartet",
    "title": "Exploratory Data Analysis",
    "section": "Anscombe’s Quartet",
    "text": "Anscombe’s Quartet\n\n\n\nX1\nY1\nX2\nY2\nX3\nY3\nX4\nY4\n\n\n\n\n10.0\n8.04\n10.0\n9.14\n10.0\n7.46\n10.0\n6.58\n\n\n8.0\n6.95\n8.0\n8.14\n8.0\n6.77\n8.0\n5.76\n\n\n13.0\n7.58\n13.0\n8.74\n13.0\n12.74\n13.0\n7.71\n\n\n9.0\n8.81\n9.0\n8.77\n9.0\n7.11\n9.0\n8.84\n\n\n11.0\n8.33\n11.0\n9.26\n11.0\n7.81\n11.0\n8.47\n\n\n14.0\n9.96\n14.0\n8.10\n14.0\n8.84\n14.0\n7.04\n\n\n6.0\n7.24\n6.0\n6.13\n6.0\n6.08\n6.0\n5.25\n\n\n4.0\n4.26\n4.0\n3.10\n4.0\n5.39\n4.0\n12.5\n\n\n12.0\n10.84\n12.0\n9.13\n12.0\n8.15\n12.0\n5.56\n\n\n7.0\n4.82\n7.0\n7.26\n7.0\n6.42\n7.0\n7.91\n\n\n5.0\n5.68\n5.0\n4.74\n5.0\n5.73\n5.0\n6.89"
  },
  {
    "objectID": "lectures/6.3-EDA.html#summary-statistics-for-the-quartet",
    "href": "lectures/6.3-EDA.html#summary-statistics-for-the-quartet",
    "title": "Exploratory Data Analysis",
    "section": "Summary Statistics for the Quartet",
    "text": "Summary Statistics for the Quartet\n\n\n\nProperty\nValue\n\n\n\n\nMean of x\n9.0\n\n\nVariance of x\n11.0\n\n\nMean of y\n7.5\n\n\nVariance of y\n4.12\n\n\nCorrelation between x and y\n0.816\n\n\nLinear Model\ny = 3 + 0.5x"
  },
  {
    "objectID": "lectures/6.3-EDA.html#but-what-do-they-look-like",
    "href": "lectures/6.3-EDA.html#but-what-do-they-look-like",
    "title": "Exploratory Data Analysis",
    "section": "But What do They Look Like?",
    "text": "But What do They Look Like?"
  },
  {
    "objectID": "lectures/6.3-EDA.html#the-tiger-that-isnt",
    "href": "lectures/6.3-EDA.html#the-tiger-that-isnt",
    "title": "Exploratory Data Analysis",
    "section": "The Tiger that Isn’t",
    "text": "The Tiger that Isn’t\n\nI would argue that the basic purpose of charts and of statistics as a whole is to help us untangle signal from noise. We are ‘programmed’ to see signals, so we need to set the standard for ‘it’s a tiger!’ quite high in research & in policy-making.\n\n\nOr, as Albert Einstein reportedly said: “If I can’t picture it, I can’t understand it.”"
  },
  {
    "objectID": "lectures/6.3-EDA.html#think-it-through",
    "href": "lectures/6.3-EDA.html#think-it-through",
    "title": "Exploratory Data Analysis",
    "section": "Think it Through",
    "text": "Think it Through\nYou can make a lot of progress in your research without any advanced statistics!\n\nA ‘picture’ isn’t just worth 1,000 words, it could be a whole dissertation!\nThe right chart makes your case eloquently and succinctly.\n\nAlways ask yourself: - What am I trying to say? - How can I say it most effectively? - Is there anything I’m overlooking in the data?\nA good chart is a good way to start!"
  },
  {
    "objectID": "lectures/6.3-EDA.html#what-makes-a-good-plot",
    "href": "lectures/6.3-EDA.html#what-makes-a-good-plot",
    "title": "Exploratory Data Analysis",
    "section": "What Makes a Good Plot?",
    "text": "What Makes a Good Plot?\nA good chart or table:\n\nServes a purpose — it is clear how it advances the argument in a way that could not be done in the text alone.\nContains only what is relevant — zeroes in on what the reader needs and is not needlessly cluttered.\nUses precision that is meaningful —\n\n\nFar too many charts or tables could be easily written up in a single sentence.\nFar too many charts or tables contain redundancy, clutter, and ‘flair’.\nDon’t report average height of your class to sub-millimeter level accuracy, or lat/long to sub-atomic scale."
  },
  {
    "objectID": "lectures/6.3-EDA.html#for-example",
    "href": "lectures/6.3-EDA.html#for-example",
    "title": "Exploratory Data Analysis",
    "section": "For Example…",
    "text": "For Example…\nHow much precision is necessary? Wikipedia has the answer at the equator!\n\n\n\nDecimal Places\nDegrees\nDistance\n\n\n\n\n0\n1\n111km\n\n\n1\n0.1\n11.1km\n\n\n2\n0.01\n1.11km\n\n\n3\n0.001\n111m\n\n\n4\n0.0001\n11.1m\n\n\n5\n0.00001\n1.11m\n\n\n6\n0.000001\n11.1cm\n\n\n7\n0.0000001\n1.11cm\n\n\n8\n0.00000001\n1.11mm"
  },
  {
    "objectID": "lectures/6.3-EDA.html#goals-by-world-cup-final",
    "href": "lectures/6.3-EDA.html#goals-by-world-cup-final",
    "title": "Exploratory Data Analysis",
    "section": "Goals by World Cup Final",
    "text": "Goals by World Cup Final"
  },
  {
    "objectID": "lectures/6.3-EDA.html#goals-by-world-cup-final-1",
    "href": "lectures/6.3-EDA.html#goals-by-world-cup-final-1",
    "title": "Exploratory Data Analysis",
    "section": "Goals by World Cup Final",
    "text": "Goals by World Cup Final"
  },
  {
    "objectID": "lectures/6.3-EDA.html#average-goals-by-world-cup-final",
    "href": "lectures/6.3-EDA.html#average-goals-by-world-cup-final",
    "title": "Exploratory Data Analysis",
    "section": "Average Goals by World Cup Final",
    "text": "Average Goals by World Cup Final\n\n\nIn 1982 the number of teams went from 16 to 24, and in 1998 it went from 24 to 32!"
  },
  {
    "objectID": "lectures/6.3-EDA.html#how-far-from-equality",
    "href": "lectures/6.3-EDA.html#how-far-from-equality",
    "title": "Exploratory Data Analysis",
    "section": "How far from Equality?",
    "text": "How far from Equality?"
  },
  {
    "objectID": "lectures/6.3-EDA.html#how-far-from-equality-1",
    "href": "lectures/6.3-EDA.html#how-far-from-equality-1",
    "title": "Exploratory Data Analysis",
    "section": "How far from Equality?",
    "text": "How far from Equality?"
  },
  {
    "objectID": "lectures/6.3-EDA.html#the-purpose-of-a-chart",
    "href": "lectures/6.3-EDA.html#the-purpose-of-a-chart",
    "title": "Exploratory Data Analysis",
    "section": "The Purpose of a Chart",
    "text": "The Purpose of a Chart\nThe purpose of a graph is to show that there are relationships within the observations in a data set.\nChoose the chart to highlight relationships / lack thereof: - Think of a chart or table as part of your ‘argument’ – if you can’t tell me how a figure advances your argument (or if your explanation is more concise than the figure) then you probably don’t need it. - Identify & prioritise the relationships in the data. - Choose a chart type/chart symbology that gives emphasis to the most important relationships."
  },
  {
    "objectID": "lectures/6.3-EDA.html#beyond-the-chart",
    "href": "lectures/6.3-EDA.html#beyond-the-chart",
    "title": "Exploratory Data Analysis",
    "section": "Beyond the Chart",
    "text": "Beyond the Chart"
  },
  {
    "objectID": "lectures/6.3-EDA.html#real-numbers",
    "href": "lectures/6.3-EDA.html#real-numbers",
    "title": "Exploratory Data Analysis",
    "section": "Real Numbers",
    "text": "Real Numbers\nConsider the difference in emphasis between:\n\n11316149\n11,316,149\n11.3 million\n11 x 10\\[^{6}\\]\n22%\n22.2559%\n\nAlways keep in mind the purpose of the number."
  },
  {
    "objectID": "lectures/6.3-EDA.html#theres-still-a-role-for-tables",
    "href": "lectures/6.3-EDA.html#theres-still-a-role-for-tables",
    "title": "Exploratory Data Analysis",
    "section": "There’s Still a Role for Tables",
    "text": "There’s Still a Role for Tables\nWhy a table is sometimes better than a chart:\n\nYou need to present data values with greater detail\nYou need to enable readers to draw comparisons between data values\nYou need to present the same data in multiple ways (e.g. raw number and percentage)\nYou want to show many dimensions for a small number of observations\n\n\ne.g. percentage of people falling into each ethnic or income category for a small number of wards or boroughs."
  },
  {
    "objectID": "lectures/6.3-EDA.html#undergraduate-tables-failing-grade",
    "href": "lectures/6.3-EDA.html#undergraduate-tables-failing-grade",
    "title": "Exploratory Data Analysis",
    "section": "Undergraduate Tables (Failing Grade)",
    "text": "Undergraduate Tables (Failing Grade)"
  },
  {
    "objectID": "lectures/6.3-EDA.html#undergraduate-tables-passing-grade",
    "href": "lectures/6.3-EDA.html#undergraduate-tables-passing-grade",
    "title": "Exploratory Data Analysis",
    "section": "Undergraduate Tables (Passing Grade)",
    "text": "Undergraduate Tables (Passing Grade)"
  },
  {
    "objectID": "lectures/6.3-EDA.html#postgraduate-tables-failing-grade",
    "href": "lectures/6.3-EDA.html#postgraduate-tables-failing-grade",
    "title": "Exploratory Data Analysis",
    "section": "Postgraduate Tables (Failing Grade)",
    "text": "Postgraduate Tables (Failing Grade)"
  },
  {
    "objectID": "lectures/6.3-EDA.html#postgraduate-tables-failing-grade-1",
    "href": "lectures/6.3-EDA.html#postgraduate-tables-failing-grade-1",
    "title": "Exploratory Data Analysis",
    "section": "Postgraduate Tables (Failing Grade)",
    "text": "Postgraduate Tables (Failing Grade)"
  },
  {
    "objectID": "lectures/6.3-EDA.html#design-for-tables",
    "href": "lectures/6.3-EDA.html#design-for-tables",
    "title": "Exploratory Data Analysis",
    "section": "Design for Tables",
    "text": "Design for Tables\nPrinciples:\n\nReduce the number of lines to a minimum (and you should almost never need vertical lines).\nUse ‘white-space’ to create visual space between groups of unrelated (or less related) elements.\nRemove redundancy (if you find yourself typing ‘millions’ or ‘GBP’ or ‘Male’ repeatedly then you’ve got redundancy).\nEnsure that meta-data is clearly separate from, but attached to, the graph (i.e. source, title, etc.)."
  },
  {
    "objectID": "lectures/6.3-EDA.html#deploying-python",
    "href": "lectures/6.3-EDA.html#deploying-python",
    "title": "Exploratory Data Analysis",
    "section": "Deploying Python",
    "text": "Deploying Python"
  },
  {
    "objectID": "lectures/6.3-EDA.html#handy-recall",
    "href": "lectures/6.3-EDA.html#handy-recall",
    "title": "Exploratory Data Analysis",
    "section": "Handy Recall",
    "text": "Handy Recall\nIf you are trying to follow along by writing code…\nimport pandas as pd\nimport geopandas as gpd\nurl='https://bit.ly/3owocdI'\ndf = pd.read_csv(url) \ndf['price'] = df.price.str.replace('$','').astype('float')\nThis will load the sampled Airbnb data from GitHub. You can always download it and load it locally (maybe a good utilty function to package up?)"
  },
  {
    "objectID": "lectures/6.3-EDA.html#what-can-we-do-series",
    "href": "lectures/6.3-EDA.html#what-can-we-do-series",
    "title": "Exploratory Data Analysis",
    "section": "What Can We Do? (Series)",
    "text": "What Can We Do? (Series)\nThis is by no means all that we can do…\n\n\n\nCommand\nReturns\n\n\n\n\ns.mean()\nMean\n\n\ns.count()\nNumber of non-null values\n\n\ns.max()\nHighest value\n\n\ns.min()\nLowest value\n\n\ns.median()\nMedian\n\n\ns.std()\nStandard deviation\n\n\ns.quantile(q=x)\n\\[x^{th}\\] quantile"
  },
  {
    "objectID": "lectures/6.3-EDA.html#what-can-we-do-data-frame",
    "href": "lectures/6.3-EDA.html#what-can-we-do-data-frame",
    "title": "Exploratory Data Analysis",
    "section": "What Can We Do? (Data Frame)",
    "text": "What Can We Do? (Data Frame)\n\n\n\nCommand\nReturns\n\n\n\n\ndf.mean()\nMean of each column\n\n\ndf.count()\nNumber of non-null values in each column\n\n\ndf.max()\nHighest value in each column\n\n\n\\[\\vdots\\]\n\\[\\vdots\\]\n\n\ndf.corr()\nCorrelation between columns\n\n\ndf.describe()\nSummarise\n\n\n\n\nNotice how we have the same functionality, but it operates at the level of the data set itself now. We gain a few new functions as well that relate to interactions between columns (a.k.a. data series)."
  },
  {
    "objectID": "lectures/6.3-EDA.html#measures",
    "href": "lectures/6.3-EDA.html#measures",
    "title": "Exploratory Data Analysis",
    "section": "Measures",
    "text": "Measures\nSo pandas provides functions for commonly-used measures:\nprint(df.price.mean())\nprint(df.price.median())\nprint(df.price.quantile(0.25))\nOutput:\n118.4542\n80.5\n40.75"
  },
  {
    "objectID": "lectures/6.3-EDA.html#more-complex-measures",
    "href": "lectures/6.3-EDA.html#more-complex-measures",
    "title": "Exploratory Data Analysis",
    "section": "More Complex Measures",
    "text": "More Complex Measures\nBut Pandas also makes it easy to derive new variables:\ndf['zscore'] = (df.price - df.price.mean())/df.price.std()\ndf.zscore.describe()\ncount    1.000000e+02\nmean    -1.332268e-17\nstd      1.000000e+00\nmin     -8.351963e-01\n25%     -6.273139e-01\n50%     -3.064081e-01\n75%      2.546724e-01\nmax      5.486042e+00\nName: zscore, dtype: float64"
  },
  {
    "objectID": "lectures/6.3-EDA.html#and-even-more-complex",
    "href": "lectures/6.3-EDA.html#and-even-more-complex",
    "title": "Exploratory Data Analysis",
    "section": "And Even More Complex",
    "text": "And Even More Complex\ndf['iqr_std'] = (df.price - df.price.median())/ \\\n      (df.price.quantile(q=0.75)-df.price.quantile(q=0.25))\ndf.iqr_std.describe()\ncount    100.000000\nmean       0.347407\nstd        1.133804\nmin       -0.599542\n25%       -0.363844\n50%        0.000000\n75%        0.636156\nmax        6.567506\nName: iqr_std, dtype: float64"
  },
  {
    "objectID": "lectures/6.3-EDA.html#the-plot-thickens",
    "href": "lectures/6.3-EDA.html#the-plot-thickens",
    "title": "Exploratory Data Analysis",
    "section": "The Plot Thickens",
    "text": "The Plot Thickens\nWe’ll get to more complex plotting over the course of the term, but here’s a good start for exploring the data! All plotting depends on matplotlib which is the ogre in the attic to R’s ggplot.\nimport matplotlib.pyplot as plt\nThis will allow you to save and manipulate the figures created in Python. It is not the most intuitive approach (unless you’ve used MATLAB before) but it does work."
  },
  {
    "objectID": "lectures/6.3-EDA.html#boxplot",
    "href": "lectures/6.3-EDA.html#boxplot",
    "title": "Exploratory Data Analysis",
    "section": "Boxplot",
    "text": "Boxplot\ndf.price.plot.box()\nplt.savefig('pboxplot.png', dpi=150, transparent=True)"
  },
  {
    "objectID": "lectures/6.3-EDA.html#frequency",
    "href": "lectures/6.3-EDA.html#frequency",
    "title": "Exploratory Data Analysis",
    "section": "Frequency",
    "text": "Frequency\ndf.room_type.value_counts().plot.bar()\nplt.savefig('phistplot.png', dpi=150, transparent=True)"
  },
  {
    "objectID": "lectures/6.3-EDA.html#a-correlation-heatmap",
    "href": "lectures/6.3-EDA.html#a-correlation-heatmap",
    "title": "Exploratory Data Analysis",
    "section": "A Correlation Heatmap",
    "text": "A Correlation Heatmap\nWe’ll get to these in more detail in a couple of weeks, but here’s some useful output…"
  },
  {
    "objectID": "lectures/6.3-EDA.html#a-map",
    "href": "lectures/6.3-EDA.html#a-map",
    "title": "Exploratory Data Analysis",
    "section": "A ‘Map’",
    "text": "A ‘Map’\ndf.plot.scatter(x='longitude',y='latitude')\nplt.savefig('pscatterplot.png', dpi=150, transparent=True)"
  },
  {
    "objectID": "lectures/6.3-EDA.html#a-fancy-map",
    "href": "lectures/6.3-EDA.html#a-fancy-map",
    "title": "Exploratory Data Analysis",
    "section": "A Fancy ‘Map’",
    "text": "A Fancy ‘Map’\ndf.plot.scatter(x='longitude',y='latitude',\n                c='price',colormap='viridis',\n                figsize=(10,5),title='London',\n                grid=True,s=24,marker='x')\nplt.savefig('pscatterplot.png', dpi=150, transparent=True)"
  },
  {
    "objectID": "lectures/6.3-EDA.html#an-actual-map",
    "href": "lectures/6.3-EDA.html#an-actual-map",
    "title": "Exploratory Data Analysis",
    "section": "An Actual ‘Map’",
    "text": "An Actual ‘Map’\ngdf = gpd.GeoDataFrame(df, \n      geometry=gpd.points_from_xy(df['longitude'], df['latitude'], crs='epsg:4326'))\ngdf.plot(column='price', cmap='viridis', scheme='quantiles', markersize=8, legend=True)"
  },
  {
    "objectID": "lectures/6.3-EDA.html#resources",
    "href": "lectures/6.3-EDA.html#resources",
    "title": "Exploratory Data Analysis",
    "section": "Resources",
    "text": "Resources\nThere’s so much more to find, but:\n\nPandas Reference\nA Guide to EDA in Python (Looks very promising)\nEDA with Pandas on Kaggle\nEDA Visualisation using Pandas\nPython EDA Analysis Tutorial\nBetter EDA with Pandas Profiling [Requires module installation]\nEDA: DataPrep.eda vs Pandas-Profiling [Requires module installation]\nA Data Science Project for Beginners (EDA)\nEDA: A Pracitcal Guide and Template for Structured Data\nEDA—Don’t ask how, ask what (Part 1)\nPreparing your Dataset for Modeling – Quickly and Easily (Part 2)\nHandling Missing Data\nIntroduction to Exploratory Data Analysis (EDA)"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#getting-spatial-with-boroughs",
    "href": "lectures/6.4-ESDA.html#getting-spatial-with-boroughs",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Getting Spatial (with Boroughs)",
    "text": "Getting Spatial (with Boroughs)\nimport geopandas as gpd\nurl = 'https://bit.ly/3neINBV'\nboros = gpd.read_file(url, driver='GPKG')\nboros.plot(color='none',edgecolor='red')"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#handy-recall",
    "href": "lectures/6.4-ESDA.html#handy-recall",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Handy Recall",
    "text": "Handy Recall\nAnd if you are trying to follow along by writing code…\nimport pandas as pd\nurl ='https://bit.ly/3owocdI'\ndf  = pd.read_csv(url) \ndf['price'] = df.price.str.replace('$','').astype('float')\ngdf = gpd.GeoDataFrame(df, \n        geometry=gpd.points_from_xy(\n          df['longitude'], df['latitude'], crs='epsg:4326'\n        )\n      )\nThis will load the sampled Airbnb data from GitHub."
  },
  {
    "objectID": "lectures/6.4-ESDA.html#convex-hull",
    "href": "lectures/6.4-ESDA.html#convex-hull",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Convex Hull",
    "text": "Convex Hull\nboros['hulls'] = boros.geometry.convex_hull\nboros = boros.set_geometry('hulls')\nboros.plot(column='NAME', categorical=True, alpha=0.5)"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#dissolve",
    "href": "lectures/6.4-ESDA.html#dissolve",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Dissolve",
    "text": "Dissolve\nboros['region'] = 'London'\nldn = boros.dissolve(by='region')\n\nf,ax = plt.subplots(figsize=(10,8))\nldn.plot(ax=ax)\ngdf.plot(ax=ax, column='price', scheme='HeadTailBreaks', cmap='inferno')"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#simplify",
    "href": "lectures/6.4-ESDA.html#simplify",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Simplify",
    "text": "Simplify\nldn.simplify(250).plot()"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#buffer",
    "href": "lectures/6.4-ESDA.html#buffer",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Buffer",
    "text": "Buffer\nldn.buffer(500).plot()"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#buffer-simplify",
    "href": "lectures/6.4-ESDA.html#buffer-simplify",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Buffer & Simplify",
    "text": "Buffer & Simplify\nldn.buffer(1000).simplify(1000).plot()"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#difference",
    "href": "lectures/6.4-ESDA.html#difference",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Difference",
    "text": "Difference\nAnd some nice chaining…\nldn.buffer(3000).simplify(2500).difference(ldn.geometry).plot()"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#legendgrams",
    "href": "lectures/6.4-ESDA.html#legendgrams",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Legendgrams",
    "text": "Legendgrams"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#implementing-legendgrams",
    "href": "lectures/6.4-ESDA.html#implementing-legendgrams",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Implementing Legendgrams",
    "text": "Implementing Legendgrams\nimport pysal as ps\n# https://github.com/pysal/mapclassify\nimport mapclassify as mc\n# https://jiffyclub.github.io/palettable/\nimport palettable.matplotlib as palmpl\nfrom legendgram import legendgram\n\nf,ax = plt.subplots(figsize=(10,8))\ngdf.plot(column='price', scheme='Quantiles', cmap='magma', k=5, ax=ax)\nq = mc.Quantiles(gdf.price.values, k=5)\n\n# https://github.com/pysal/legendgram/blob/master/legendgram/legendgram.py\nlegendgram(f, ax, \n               gdf.price, q.bins, pal=palmpl.Magma_5,\n               legend_size=(.4,.2), # legend size in fractions of the axis\n               loc = 'upper left', # mpl-style legend loc\n               clip = (0,500), # clip range of the histogram\n               frameon=True)\n\nNote that the number of colours need to match k, which is 5 in this case.\nIt should be possible to set up the colormap and bins such that they can be passed to both GeoPandas and Legendgram."
  },
  {
    "objectID": "lectures/6.4-ESDA.html#knn-weights",
    "href": "lectures/6.4-ESDA.html#knn-weights",
    "title": "Exploratory Spatial Data Analysis",
    "section": "KNN Weights",
    "text": "KNN Weights"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#spatial-lag-of-distance-band",
    "href": "lectures/6.4-ESDA.html#spatial-lag-of-distance-band",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Spatial Lag of Distance Band",
    "text": "Spatial Lag of Distance Band"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#implementing-db",
    "href": "lectures/6.4-ESDA.html#implementing-db",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Implementing DB",
    "text": "Implementing DB\nw2 = weights.DistanceBand.from_dataframe(gdf, threshold=2000, alpha=-0.25)\ngdf['price_std'] = (gdf.price - gdf.price.mean()) / gdf.price.std()\ngdf['w_price_std'] = weights.lag_spatial(w2, gdf.price_std)\ngdf[['name','price_std','w_price_std']].sample(5, random_state=42)\n\n\n\n\nname\nprice_std\nw_price_std\n\n\n\n\n83\nSouthfields Home\n-0.27\n0.00\n\n\n53\nFlat in Islington, Central London\n-0.51\n-0.58\n\n\n70\n3bedroom Family Home minutes from Kensington Tube\n0.83\n0.46\n\n\n453\nBed, 20 min to Liverpool st, EAST LONDON\n-0.07\n-0.82\n\n\n44\nAvni Kensington Hotel\n2.52\n3.25"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#morans-i",
    "href": "lectures/6.4-ESDA.html#morans-i",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Moran’s I",
    "text": "Moran’s I\nmi = esda.Moran(gdf['price'], w)\nprint(f\"{mi.I:0.4f}\")\nprint(f\"{mi.p_sim:0.4f}\")\nmoran_scatterplot(mi)"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#local-morans-i",
    "href": "lectures/6.4-ESDA.html#local-morans-i",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Local Moran’s I",
    "text": "Local Moran’s I"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#implementing-local-morans-i",
    "href": "lectures/6.4-ESDA.html#implementing-local-morans-i",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Implementing Local Moran’s I",
    "text": "Implementing Local Moran’s I\nlisa = esda.Moran_Local(gdf.price, w)\n# Break observations into significant or not\ngdf['sig'] = lisa.p_sim < 0.05\n# Store the quadrant they belong to\ngdf['quad'] = lisa.q\ngdf[['name','price','sig','quad']].sample(5, random_state=42)\n\n\n\n\n\n\n\n\n\n\n\nname\nprice\nsig\nquad\n\n\n\n\n83\nSouthfields Home\n85.0\nFalse\n3\n\n\n53\nFlat in Islington, Central London\n55.0\nFalse\n3\n\n\n70\n3bedroom Family Home minutes from Kensington Tube\n221.0\nFalse\n1\n\n\n453\nBed, 20 min to Liverpool st, EAST LONDON\n110.0\nFalse\n3\n\n\n44\nAvni Kensington Hotel\n430.0\nFalse\n1"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#full-lisa",
    "href": "lectures/6.4-ESDA.html#full-lisa",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Full LISA",
    "text": "Full LISA\nplot_local_autocorrelation(lisa, gdf, 'price')"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#resources",
    "href": "lectures/6.4-ESDA.html#resources",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Resources",
    "text": "Resources\nThere’s so much more to find, but:\n\nPandas Reference\nEDA with Pandas on Kaggle\nEDA Visualisation using Pandas\nPython EDA Analysis Tutorial\nBetter EDA with Pandas Profiling [Requires module installation]\nVisualising Missing Data\nChoosing Map Colours"
  },
  {
    "objectID": "lectures/7.1-Notebooks_as_Documents.html#recall-tangled-workflows",
    "href": "lectures/7.1-Notebooks_as_Documents.html#recall-tangled-workflows",
    "title": "Note(book)s to Documents",
    "section": "Recall: Tangled Workflows",
    "text": "Recall: Tangled Workflows\nIt’s not just about mixing code and comment, we also want:\n\nTo separate content from presentation\nTo define mappings between presentation styles\nTo produce the best-quality output for the format chosen\n\n\n\nExamples of this include CSS for web sites, LaTeX templates, and Markdown styles.\nMVC approach to software design."
  },
  {
    "objectID": "lectures/7.1-Notebooks_as_Documents.html#pandoc",
    "href": "lectures/7.1-Notebooks_as_Documents.html#pandoc",
    "title": "Note(book)s to Documents",
    "section": "Pandoc",
    "text": "Pandoc\nTool for converting documents between formats, including:\n\nPlain Text/YAML\nMarkdown\nLaTeX/PDF\nHTML/Reveal.js\nJupyter Notebook\nXML/ODT/DOCX\nEPUB/DocBook\nBibTeX/JSON"
  },
  {
    "objectID": "lectures/7.1-Notebooks_as_Documents.html#latex",
    "href": "lectures/7.1-Notebooks_as_Documents.html#latex",
    "title": "Note(book)s to Documents",
    "section": "LaTeX",
    "text": "LaTeX\nIntended for type-setting of scientific documents, but has been used for slides, posters, CVs, etc. It is not a word processor, it’s more like a compiler.\n\n \n\n\nThis format is based on Edward Tufte’s VSQD and can be found on GitHub."
  },
  {
    "objectID": "lectures/7.1-Notebooks_as_Documents.html#latex-in-practice",
    "href": "lectures/7.1-Notebooks_as_Documents.html#latex-in-practice",
    "title": "Note(book)s to Documents",
    "section": "LaTeX in Practice",
    "text": "LaTeX in Practice\nYou write LaTeX in any text editor, but specialist apps like Texpad or Overleaf make it easier.\n\\documentclass[11pt,article,oneside]{memoir}\n\\newcommand{\\bl}{\\textsc{bl}~\\/}\n\\usepackage{tabularx}\n\n\\begin{document}\n\\maketitle \n\nThis report provides an overview of activities ...\n\n\\section{Applications}\nA primary objective was the submission...\n\nUCL has an institutional license for Overleaf.\nThis document is then compiled (or ‘typeset’) with the commands provided by the preamble being interpreted and applied. Depending on the length of the document and sophistication of the styles it can take up to 3 or 4 minutes for a book-length document, but small documents should compile in a few seconds.\nCompilation allows us to do things like have Master Documents that actually work, include PDFs, make forwards and backwards references."
  },
  {
    "objectID": "lectures/7.1-Notebooks_as_Documents.html#bibtex",
    "href": "lectures/7.1-Notebooks_as_Documents.html#bibtex",
    "title": "Note(book)s to Documents",
    "section": "BibTeX",
    "text": "BibTeX\nProvides bilbiographic support for LaTeX but widely used by other utilities as is also plain-text.\n@article{Lavin:2019,\n        Author = {Lavin, Matthew J.},\n        Doi = {10.46430/phen0082},\n        Journal = {The Programming Historian},\n        Number = {8},\n        Title = {Analyzing Documents with TF-IDF},\n        Year = {2019},\n        Bdsk-Url-1 = {https://doi.org/10.46430/phen0082}}\n\n@incollection{Kitchin:2016,\n        Author = {Kitchin, R. and Lauriault, T.P. and McArdie, G.},\n        Booktitle = {Smart Urbanism},\n        Chapter = {2},\n        Editor = {Marvin, Luque-Ayala, McFarlane},\n        Title = {Smart Cities and the Politics of Urban Data},\n        Year = {2016}}"
  },
  {
    "objectID": "lectures/7.1-Notebooks_as_Documents.html#bibtex-in-practice",
    "href": "lectures/7.1-Notebooks_as_Documents.html#bibtex-in-practice",
    "title": "Note(book)s to Documents",
    "section": "BibTeX in Practice",
    "text": "BibTeX in Practice\nTo reference a document we then need to tell LaTeX or Pandoc where to look:\n\\bibliographystyle{apacite} # LaTeX\n\\bibliography{Spatial_Data_Chapter.bib} # LaTeX\nWith citations following formats like:\n\\citep[p.22]{Reades2018} # LaTeX\nOr:\n[@dignazio:2020, chap. 4] # Markdown"
  },
  {
    "objectID": "lectures/7.1-Notebooks_as_Documents.html#reveal.js",
    "href": "lectures/7.1-Notebooks_as_Documents.html#reveal.js",
    "title": "Note(book)s to Documents",
    "section": "Reveal.js",
    "text": "Reveal.js\nJavaScript-based presentation framework. Can use Markdown (using the separator --- to separate slides) to generate portable interactive slides including references/bibliographies.\nCompare:\n\nhttps://github.com/darribas/wmn/blob/master/src/slides/lecture_01.md\nhttps://github.com/darribas/wmn/blob/master/src/slidedecks/lecture_01.html\nhttp://darribas.org/wmn/slidedecks/lecture_01.html#/"
  },
  {
    "objectID": "lectures/7.1-Notebooks_as_Documents.html#recap-of-formatting",
    "href": "lectures/7.1-Notebooks_as_Documents.html#recap-of-formatting",
    "title": "Note(book)s to Documents",
    "section": "Recap of Formatting",
    "text": "Recap of Formatting"
  },
  {
    "objectID": "lectures/7.1-Notebooks_as_Documents.html#headings",
    "href": "lectures/7.1-Notebooks_as_Documents.html#headings",
    "title": "Note(book)s to Documents",
    "section": "Headings",
    "text": "Headings\n\n\n\nMarkdown\nLaTeX\n\n\n\n\n# Heading Level 1\n\\section{Heading Level 1}\n\n\n## Heading Level 2\n\\subsection{Heading Level 2}\n\n\n### Heading Level 3\n\\subsubsection{Heading Level 3}"
  },
  {
    "objectID": "lectures/7.1-Notebooks_as_Documents.html#inline-elements",
    "href": "lectures/7.1-Notebooks_as_Documents.html#inline-elements",
    "title": "Note(book)s to Documents",
    "section": "Inline Elements",
    "text": "Inline Elements\n\n\n\n\n\n\n\nMarkdown\nLaTeX\n\n\n\n\n1. Numbered item 1\n\\begin{enumerate} \\n \\item ... \\end{enumerate}\n\n\n- Bulleted list item 1\n\\begin{itemize} \\n \\item ... \\n \\end{itemize}\n\n\n_italics_ or *italics*\n\\emph{italics} or \\textit{italics}\n\n\n**bold**\n\\textbf{bold}\n\n\n> blockquote\n\\begin{quote} \\n blockquote \\end{quote}\n\n\nSome `code` is here\nSome \\texttt{code} is here\n\n\n[Link Text](URL)\n\\href{Link Text}{URL}\n\n\n![Alt Text](Image URL)\n\\begin{figure}\\n \\includegraphics[opts]{...}\\n \\end{figure}"
  },
  {
    "objectID": "lectures/7.1-Notebooks_as_Documents.html#mathematics",
    "href": "lectures/7.1-Notebooks_as_Documents.html#mathematics",
    "title": "Note(book)s to Documents",
    "section": "Mathematics",
    "text": "Mathematics\n\n\n\n\n\n\n\nMarkdown\nLaTeX\n\n\n\n\nSame as LaTex with 2 \\('s | `\\)x=5$| | Same as LaTex with 2 $'s |\\(\\pi\\)| | Same as LaTex with 2 $'s |\\(e = mc^{2}\\)`\n\n\n\n\nWe can show all this directly in the Notebook!\n\\[\\pi\\]; \\[e = mc^{2}\\]; \\[\\int_{0}^{\\inf} x^2 \\,dx\\]; \\[\\sum_{n=1}^{\\infty} 2^{-n} = 1\\]\nOverleaf has good documentation for most (basic) applications."
  },
  {
    "objectID": "lectures/7.1-Notebooks_as_Documents.html#recap",
    "href": "lectures/7.1-Notebooks_as_Documents.html#recap",
    "title": "Note(book)s to Documents",
    "section": "Recap",
    "text": "Recap\n\nYou will usually want to Google most things to do with laying out LaTeX code."
  },
  {
    "objectID": "lectures/7.1-Notebooks_as_Documents.html#resources",
    "href": "lectures/7.1-Notebooks_as_Documents.html#resources",
    "title": "Note(book)s to Documents",
    "section": "Resources",
    "text": "Resources\n\nJupyter Tips and Tricks\nPandoc Demos\nBeginner’s Guide to Jupyter Notebooks\n7 Essential Tips to Writing With Jupyter Notebooks\nVersion Control with Jupyter\nSustainable Publishing using Pandoc and Markdown"
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#describing-character-sequences",
    "href": "lectures/7.2-Patterns_in_Text.html#describing-character-sequences",
    "title": "Patterns in Text",
    "section": "Describing Character Sequences?",
    "text": "Describing Character Sequences?\nConsider the following character sequences:\n\nfoo@bar.com\nhttps://www.ucl.ac.uk/bartlett/casa/\n\n102-1111\n\nE17 5RS\nNow, fair Hippolyta, our nuptial hour / Draws on apace. Four happy days bring in / Another moon. But, oh, methinks how slow / This old moon wanes. She lingers my desires, / Like to a stepdame or a dowager / Long withering out a young man’s revenue. (I.i.)\n\n\nWe need ways to distinguish: Upper and Lower Case, Digits, Space Characters, Other Characters, Repetition, Type… these are things that Regular Expressions help us to achieve."
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#applications-of-regexes",
    "href": "lectures/7.2-Patterns_in_Text.html#applications-of-regexes",
    "title": "Patterns in Text",
    "section": "Applications of RegExes",
    "text": "Applications of RegExes\nIf our problem follows some set of articulable rules about permissible sequences of characters then we can probably validate it using a regex:\n\n\n\n\n\n\n\nExamples\nMore Examples\n\n\n\n\nEmail\nPassword\n\n\nPostcode\nPhone number\n\n\nDate\nCredit cards\n\n\nWeb scraping\nSyntax highlighting\n\n\nSentence structure\nData wrangling\n\n\nSearching for/withinfiles/content\nLexical analysis/Language detection\n\n\n\n\nThese are all good problems…"
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#but",
    "href": "lectures/7.2-Patterns_in_Text.html#but",
    "title": "Patterns in Text",
    "section": "But…",
    "text": "But…\nString methods are not enough:\n>>> '123foo456'.index('foo')\n2\n>>> '123foo456'.split('foo')\n['123', '456']\n>>> ' 123 foo 456 '.strip()\n'123 foo 456'\n>>> 'HOW NOW BROWN COW?'.lower()\n'how now brown cow?'\n>>> 'How now brown cow?'.replace('brown ','green-')\n'How now green-cow?'\nSee: dir(str) for full list of string methods."
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#regular-expressions",
    "href": "lectures/7.2-Patterns_in_Text.html#regular-expressions",
    "title": "Patterns in Text",
    "section": "Regular Expressions",
    "text": "Regular Expressions\nRegexes are a way for talking about patterns observed in text, although their origins are rooted in philosophy and linguistics.\nImplemented in Python as:\nimport re\n# re.search(<regex>, <str>)\ns = '123foo456'\nif re.search('123',s):\n  print(\"Found a match.\")\nelse:\n  print(\"No match.\")\nPrints 'Found a match.'"
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#capturing-matches",
    "href": "lectures/7.2-Patterns_in_Text.html#capturing-matches",
    "title": "Patterns in Text",
    "section": "Capturing Matches",
    "text": "Capturing Matches\nm = re.search('123',s)\nprint(m.start())\nprint(m.end())\nprint(m.span())\nprint(m.group())\nOutputs:\n0\n3\n(0,3)\n123\n\nSo, we have None if a search fails, but if it succeeds then we have attributes of the match objection like start, end, span, and group (this last is going to be particularly interesting since it tells us what matched)."
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#configuring-matches",
    "href": "lectures/7.2-Patterns_in_Text.html#configuring-matches",
    "title": "Patterns in Text",
    "section": "Configuring Matches",
    "text": "Configuring Matches\nm = re.search('FOO',s)\nprint(m)\nm = re.search('FOO',s,re.IGNORECASE)\nprint(m)\nOutputs:\nNone\n<re.Match object; span=(3, 6), match='foo'>\nThe third parameter allows us to: match newlines (re.DOTALL), ignore case (re.IGNORECASE), take language into account (re.LOCALE), match across lines (re.MULTILINE), and write patterns across multiple lines (re.VERBOSE). If you need multiple options it’s re.DOTALL | re.IGNORECASE. Bitwise again!"
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#more-than-one-match",
    "href": "lectures/7.2-Patterns_in_Text.html#more-than-one-match",
    "title": "Patterns in Text",
    "section": "More Than One Match",
    "text": "More Than One Match\ns = '123foo456foo789'\nlst = re.findall('foo',s)\nprint(lst)\nlst = re.finditer('foo',s)\n[x for x in lst]\nrs  = re.sub('foo',' ',s)\nprint(rs)\nrs  = re.split(' ',rs)\nprint(rs)\nOutputs:\n['foo','foo']\n[<re.Match object; span=(3, 6), match='foo'>, <re.Match object; span=(9, 12), match='foo'>]\n'123 456 789'\n['123', '456', '789']"
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#lets-get-meta",
    "href": "lectures/7.2-Patterns_in_Text.html#lets-get-meta",
    "title": "Patterns in Text",
    "section": "Let’s Get Meta",
    "text": "Let’s Get Meta"
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#regular-expressions-do-much-more",
    "href": "lectures/7.2-Patterns_in_Text.html#regular-expressions-do-much-more",
    "title": "Patterns in Text",
    "section": "Regular Expressions Do Much More",
    "text": "Regular Expressions Do Much More\n>>> import re\n>>> m = re.search('\\$((\\d+,){2,}\\d+)',\n        \"'That will be $1,000,000 he said...'\")\n>>> m.group(1)\n'1,000,000'\nThis is not even scratching the surface, but it allows to look for sequences of 1-or-more digits followed by a comma… and for those sequence to repeat two or more times, ending with a sequence of digits. The rest is ignored."
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#character-classes",
    "href": "lectures/7.2-Patterns_in_Text.html#character-classes",
    "title": "Patterns in Text",
    "section": "Character Classes",
    "text": "Character Classes\n\n\n\n\n\n\n\n\nCharacters\nRegex Meta Class Options\n‘Antonyms’\n\n\n\n\na…z\n[a-z], \\w (word-like characters)\n[^a-z], \\W\n\n\nA…Z\n[A-Z], \\w (word-like characters)\n[^A-Z], \\W\n\n\n0…9\n[0-9], \\d (digits)\n[^0-9], \\D\n\n\n' ', \\n, \\t, \\r, \\f, \\v\n\\s\n\\S\n\n\n., [, ], +, $, ^, \\|, {, }, *, (, ), ?\nFor safety always precede character with a \\.\nNone\n\n\n\nNote: \\w will include _. And \\ is, once again, important as it ‘escapes’ various characters, and options."
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#metacharacters",
    "href": "lectures/7.2-Patterns_in_Text.html#metacharacters",
    "title": "Patterns in Text",
    "section": "Metacharacters",
    "text": "Metacharacters\n\n\n\n\n\n\n\nMetacharacter\nMeaning\n\n\n\n\n.\nAny character at all\n\n\n^\nStart of a string/line\n\n\n$\nEnd of a string/line\n\n\n*\n0 or more of something\n\n\n+\n1 or more of something\n\n\n?\n0 or 1 of something; also lazy modifier\n\n\n{m,n}\nBetween m and n of something\n\n\n[ ]\nA set of character literals\n\n\n( )\nGroup/remember this sequence of characters\n\n\n|\nOr"
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#i-am-completely-lost",
    "href": "lectures/7.2-Patterns_in_Text.html#i-am-completely-lost",
    "title": "Patterns in Text",
    "section": "I am Completely Lost",
    "text": "I am Completely Lost\n\nLet’s take some real regexes and try to make sense of them."
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#building-blocks",
    "href": "lectures/7.2-Patterns_in_Text.html#building-blocks",
    "title": "Patterns in Text",
    "section": "Building Blocks",
    "text": "Building Blocks\n\n\n\n\n\n\n\nRegex\nInterpretation\n\n\n\n\nr'\\s*'\n0 or more spaces\n\n\nr'\\d+'\n1 or more digits\n\n\nr'[A-Fa-f0-7]{5}'\nExactly 5 hexadecimal ‘digits’\n\n\nr'\\w+\\.\\d{2,}'\n1 or more ‘wordish’ characters, followed by a full-stop, then 2 or more digits\n\n\nr'^[^@]+@\\w+'\nOne more non-@ characters at the start of a line, followed by a ‘@’ then 1 or more ‘wordish’ characters.\n\n\nr'(uk\\|eu\\|fr)$'\nThe characters ‘uk’ or ‘eu’ or ‘fr’ at the end of a line."
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#whats-this",
    "href": "lectures/7.2-Patterns_in_Text.html#whats-this",
    "title": "Patterns in Text",
    "section": "What’s This?",
    "text": "What’s This?\nre.match(r'^[^@]+@([a-z0-9\\-]+\\.){1,5}[a-z0-9\\-]+$', s)\n\ns should be replaced with any string you want to check."
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#whats-this-1",
    "href": "lectures/7.2-Patterns_in_Text.html#whats-this-1",
    "title": "Patterns in Text",
    "section": "What’s This?",
    "text": "What’s This?\nre.match(r'\\d{4}-\\d{2}-\\d{2}', s)"
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#whats-this-2",
    "href": "lectures/7.2-Patterns_in_Text.html#whats-this-2",
    "title": "Patterns in Text",
    "section": "What’s This?",
    "text": "What’s This?\nre.match(r'^\\s*$', s)"
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#whats-this-3",
    "href": "lectures/7.2-Patterns_in_Text.html#whats-this-3",
    "title": "Patterns in Text",
    "section": "What’s This?",
    "text": "What’s This?\nre.match(r'^(http|https|ftp):[\\/]{2}([a-zA-Z0-9\\-]+\\.){1,4}[a-zA-Z]{2,5}(:[0-9]+)?\\/?([a-zA-Z0-9\\-\\._\\?\\'\\/\\\\\\+\\&\\%\\$#\\=~]*)',s)"
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#whats-this-4",
    "href": "lectures/7.2-Patterns_in_Text.html#whats-this-4",
    "title": "Patterns in Text",
    "section": "What’s This?",
    "text": "What’s This?\nre.match(r'([Gg][Ii][Rr] 0[Aa]{2})|((([A-Za-z][0-9]{1,2})|(([A-Za-z][A-Ha-hJ-Yj-y][0-9]{1,2})|(([A-Za-z][0-9][A-Za-z])|([A-Za-z][A-Ha-hJ-Yj-y][0-9][A-Za-z]?))))\\s?[0-9][A-Za-z]{2})',s)"
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#to-help",
    "href": "lectures/7.2-Patterns_in_Text.html#to-help",
    "title": "Patterns in Text",
    "section": "To Help…",
    "text": "To Help…\nre.VERBOSE to the rescue:\nregex = r\"\"\"\n([GIR] 0[A]{2})|    # Girobank \n(\n  (\n    ([A-Z][0-9]{1,2})| # e.g A00...Z99\n      (\n        ([A-Z][A-HJ-Y][0-9]{1,2})|  # e.g. AB54...ZX11\n          (([A-Z][0-9][A-Z])|  # e.g. A0B...Z9Z \n          ([A-Z][A-HJ-Y][0-9][A-Z]?))  # e.g. WC1 or WC1H\n        )\n      )\n    \\s?[0-9][A-Z]{2} # e.g. 5RX\n  )\n\"\"\"\nre.match(regex,s,re.VERBOSE|re.IGNORECASE) # Can also use: re.X|re.I\n\nThis is the government’s own regex but is probably not 100% accurate."
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#resources",
    "href": "lectures/7.2-Patterns_in_Text.html#resources",
    "title": "Patterns in Text",
    "section": "Resources",
    "text": "Resources\n\nPython Documentation\nReal Python: Regular Expressions 1\nReal Python: Regular Expressions 2\nData Camp RegEx Tutorial\nIntroduction to Regex\nUnderstanding RegExes in Python\nDemystifying RegExes in Python\nPython RegExes\nMastering String Methods in Python\n\nThanks to Yogesh Chavan and Nicola Pietroluongo for examples."
  },
  {
    "objectID": "lectures/7.3-Cleaning_Text.html#say-what",
    "href": "lectures/7.3-Cleaning_Text.html#say-what",
    "title": "Cleaning Text",
    "section": "Say What?",
    "text": "Say What?\n\nBye, bye, for loop!\n\n\nAnother great example of something where what makes sense to a human is not always the fastest way to get things done."
  },
  {
    "objectID": "lectures/7.3-Cleaning_Text.html#do-more-with-each-tick-tock",
    "href": "lectures/7.3-Cleaning_Text.html#do-more-with-each-tick-tock",
    "title": "Cleaning Text",
    "section": "Do More with Each Tick-Tock",
    "text": "Do More with Each Tick-Tock\n\nLibrary/Package implements weak form of vectorisation or parallelisation, but some libraries do more.\nYou must request it because it requires hardware or other support and it is highly optimsed.\nMultiple separate machines acting as one.\nMultiple GPUs acting as one.\n\n\nConceptually, these get challenging if you can’t clearly separate/parallelise tasks."
  },
  {
    "objectID": "lectures/7.3-Cleaning_Text.html#pandas.apply-vs.-numpy",
    "href": "lectures/7.3-Cleaning_Text.html#pandas.apply-vs.-numpy",
    "title": "Cleaning Text",
    "section": "Pandas.apply() vs. Numpy",
    "text": "Pandas.apply() vs. Numpy\nNumpy is fully vectorised and will almost always out-perform operations like Pandas apply, but both are massive improvements on for loops:\n\nExecute row-wise and column-wise operations.\nApply any arbitrary function to individual elements or whole axes.\nCan make use of lambda functions too for ‘one off’ operations.\n\nimport numpy as np\ndf.apply(np.sqrt) # Square root of all values\ndf.apply(np.sum, axis=0) # Sum by row"
  },
  {
    "objectID": "lectures/7.3-Cleaning_Text.html#lambda-functions",
    "href": "lectures/7.3-Cleaning_Text.html#lambda-functions",
    "title": "Cleaning Text",
    "section": "Lambda Functions",
    "text": "Lambda Functions\nFunctional equivalent of list comprehensions: 1-line, anonymous functions.\nFor example:\n>>> x = lambda a : a + 10\n>>> print(x(5))\n15\nOr:\n>>> full_name = lambda first, last: f'Full name: {first.title()} {last.title()}'\n>>> full_name('guido', 'van rossum')\n'Guido Van Rossum'\nThese are very useful with pandas."
  },
  {
    "objectID": "lectures/7.3-Cleaning_Text.html#dealing-with-structured-text",
    "href": "lectures/7.3-Cleaning_Text.html#dealing-with-structured-text",
    "title": "Cleaning Text",
    "section": "Dealing with Structured Text",
    "text": "Dealing with Structured Text"
  },
  {
    "objectID": "lectures/7.3-Cleaning_Text.html#beautiful-soup-selenium",
    "href": "lectures/7.3-Cleaning_Text.html#beautiful-soup-selenium",
    "title": "Cleaning Text",
    "section": "Beautiful Soup & Selenium",
    "text": "Beautiful Soup & Selenium\nTwo stages to acquiring web-based documents:\n\nAccessing the document: urllib can deal with many issues (even authentication), but not with dynamic web pages (which are increasingly common); for that, you need Selenium (library + driver).\nProcessing the document: simple data can be extracted from web pages with RegularExpressions, but not with complex (esp. dynamic) content; for that, you need BeautifulSoup4.\n\nThese interact with wider issues of Fair Use (e.g. rate limits and licenses); processing pipelines (e.g. saving WARCs or just the text file, multiple stages, etc.); and other practical constraints."
  },
  {
    "objectID": "lectures/7.3-Cleaning_Text.html#regular-expressions-breaks",
    "href": "lectures/7.3-Cleaning_Text.html#regular-expressions-breaks",
    "title": "Cleaning Text",
    "section": "Regular Expressions / Breaks",
    "text": "Regular Expressions / Breaks\nNeed to look at how the data is organised:\n\nFor very large corpora, you might want one document at a time (batch).\nFor very large files, you might want one line at a time (streaming).\nFor large files in large corpora, you might want more than one machine.\n\n\nSee the OpenVirus Project."
  },
  {
    "objectID": "lectures/7.3-Cleaning_Text.html#managing-vocabularies",
    "href": "lectures/7.3-Cleaning_Text.html#managing-vocabularies",
    "title": "Cleaning Text",
    "section": "Managing Vocabularies",
    "text": "Managing Vocabularies\n\nA lot of what you’ll see here are, at best, heuristics with enormous variation in practice."
  },
  {
    "objectID": "lectures/7.3-Cleaning_Text.html#starting-points",
    "href": "lectures/7.3-Cleaning_Text.html#starting-points",
    "title": "Cleaning Text",
    "section": "Starting Points",
    "text": "Starting Points\nThese strategies can be sued singly or all-together:\n\nStopwords\nCase\nAccent-stripping\nPunctuation\nNumbers\n\nSample stopwords:\n{'further', 'her', 'their', 'we', 'just', 'why', 'or', 'each', 's', \"it's\", 'ma', 'below', 'am', 'more', \"couldn't\", \"should've\", 'was', \"mightn't\", 'weren', 'ourselves', 'have', 'if', 'then', 'from', ...}\nBut these are just a starting point!\n\nWhat’s the semantic difference between 1,000,000 and 999,999?"
  },
  {
    "objectID": "lectures/7.3-Cleaning_Text.html#distributional-pruning",
    "href": "lectures/7.3-Cleaning_Text.html#distributional-pruning",
    "title": "Cleaning Text",
    "section": "Distributional Pruning",
    "text": "Distributional Pruning\nWe can prune from both ends of the distribution:\n\nOverly rare words: what does a word used in one document help us to do?\nOverly common ones: what does a word used in every document help us to do?\n\n\nAgain, no hard-and-fast rules: can be done on raw counts, percentage of all documents, etc. Choices will, realistically, depend on the nature of the data."
  },
  {
    "objectID": "lectures/7.3-Cleaning_Text.html#stemming-lemmatisation",
    "href": "lectures/7.3-Cleaning_Text.html#stemming-lemmatisation",
    "title": "Cleaning Text",
    "section": "Stemming & Lemmatisation",
    "text": "Stemming & Lemmatisation"
  },
  {
    "objectID": "lectures/7.3-Cleaning_Text.html#why-stem-or-lemmatise",
    "href": "lectures/7.3-Cleaning_Text.html#why-stem-or-lemmatise",
    "title": "Cleaning Text",
    "section": "Why Stem or Lemmatise?",
    "text": "Why Stem or Lemmatise?\nReduce the breadth of human expression:\n\nPorter & Snowball Stemming: rules-based truncation to a stem (can be augmented by language awareness).\nLemmatisation: dictionary-based ‘deduplication’ to a lemma (can be augmented by POS-tagging).\n\nCompare:\n\n\n\nSource\nPorter\nSnowball\nLemmatisation\n\n\n\n\nmonkeys\nmonkey\nmonkey\nmonkey\n\n\ncities\nciti\nciti\ncity\n\n\ncomplexity\ncomplex\ncomplex\ncomplexity\n\n\nReades\nread\nread\nReades"
  },
  {
    "objectID": "lectures/7.3-Cleaning_Text.html#resources",
    "href": "lectures/7.3-Cleaning_Text.html#resources",
    "title": "Cleaning Text",
    "section": "Resources",
    "text": "Resources\n\nVectorisation in Python\nLambda Functions\nReal Python Lambda Functions\nStemming words with NLTK\nStemming and Lemmatisation in Python\nKD Nuggets: A Practitioner’s Guide to NLP\nKD Nuggets: Linguistic Fundamentals for Natural Language Processing: 100 Essentials from Semantics and Pragmatics\nRoadmap to Natural Language Processing (NLP)"
  },
  {
    "objectID": "lectures/7.4-Analysing_Text.html#the-bag-of-words",
    "href": "lectures/7.4-Analysing_Text.html#the-bag-of-words",
    "title": "Analysing Text",
    "section": "The ‘Bag of Words’",
    "text": "The ‘Bag of Words’\nCould simply be seen as an extension of binarised approach on preceding slide:\n\n\n\nDocument\nUK\nTop\nPop\nCoronavirus\n\n\n\n\nNews\n4\n2\n0\n6\n\n\nCulture\n0\n4\n7\n0\n\n\nPolitics\n3\n0\n0\n3\n\n\nEntertainment\n3\n4\n8\n1"
  },
  {
    "objectID": "lectures/7.4-Analysing_Text.html#bow-in-practice",
    "href": "lectures/7.4-Analysing_Text.html#bow-in-practice",
    "title": "Analysing Text",
    "section": "BoW in Practice",
    "text": "BoW in Practice\nEnter, stage left, scikit-learn:\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\n\nvectorizer.fit(texts)\nvectors = vectorizer.transform(texts)\n\n# Same thing:\n# vectors = vectorizer.fit_transform(texts)\n\nprint(f'Vocabulary: {vectorizer.vocabulary_}')\nprint(f'Full vector: {vectors.toarray()}')"
  },
  {
    "objectID": "lectures/7.4-Analysing_Text.html#tfidf",
    "href": "lectures/7.4-Analysing_Text.html#tfidf",
    "title": "Analysing Text",
    "section": "TF/IDF",
    "text": "TF/IDF\nBuilds on Count Vectorisation by normalising the document frequency measure by the overall corpus frequency. Common words receive a large penalty:\n\\[\nW(t,d) = TF(t,d) / log(N/DF_{t})\n\\]\nFor example: if the term ‘cat’ appears 3 times in a document of 100 words then \\(TF(t,d)=3/100\\). If there are 10,000 documents and cat appears in 1,000 documents then \\(N/DF_{t}=10000/1000\\) and \\(log(10)=1\\), so IDF=1 and TF/IDF=0.03."
  },
  {
    "objectID": "lectures/7.4-Analysing_Text.html#tfidf-in-practice",
    "href": "lectures/7.4-Analysing_Text.html#tfidf-in-practice",
    "title": "Analysing Text",
    "section": "TF/IDF in Practice",
    "text": "TF/IDF in Practice\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\n\nvectorizer.fit(texts)\nvectors = vectorizer.transform(texts)\n\n# Same thing:\n# vectors=vectorizer.fit_transform(texts)\n\nprint(f'Vocabulary: {vectorizer.vocabulary_}')\nprint(f'Full vector: {vectors.toarray()}')\n\nWhat do you notice about how this code differs from the CountVectorizer?"
  },
  {
    "objectID": "lectures/7.4-Analysing_Text.html#term-co-occurence-matrix-tcm",
    "href": "lectures/7.4-Analysing_Text.html#term-co-occurence-matrix-tcm",
    "title": "Analysing Text",
    "section": "Term Co-Occurence Matrix (TCM)",
    "text": "Term Co-Occurence Matrix (TCM)\nThree input texts:\n\nthe cat sat on the mat\nthe cat sat on the fluffy mat\nthe fluffy ginger cat sat on the mat\n\n\n\n\n\nfluffy\nmat\nginger\nsat\non\ncat\nthe\n\n\n\n\nfluffy\n\n1\n1\n\n0.5\n0.5\n2.0\n\n\nmat\n\n\n\n\n0.5\n\n1.5\n\n\nginger\n\n\n\n0.5\n0.5\n1.0\n1.5\n\n\nsat\n\n\n\n\n3.0\n3.0\n2.5\n\n\non\n\n\n\n\n\n1.5\n3.0\n\n\ncat\n\n\n\n\n\n\n2.0\n\n\nthe"
  },
  {
    "objectID": "lectures/7.4-Analysing_Text.html#text2vec",
    "href": "lectures/7.4-Analysing_Text.html#text2vec",
    "title": "Analysing Text",
    "section": "Text2Vec",
    "text": "Text2Vec\nTypically some kind of 2 or 3-layer neural network that ‘learns’ (using as big a training data set as possible) how to embed the TCM into a lower-dimension representation.\nConceptual similarities to PCA in terms of what we’re trying to achieve, but the process is utterly different.\nMany different approaches, but GloVe (Stanford), word2vec (Google), fastText (Facebook), and ELMo (Allen) or BERT (Google) are probably the best-known."
  },
  {
    "objectID": "lectures/7.4-Analysing_Text.html#sentiment-analysis",
    "href": "lectures/7.4-Analysing_Text.html#sentiment-analysis",
    "title": "Analysing Text",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nRequires us to deal in great detail with bi- and tri-grams because negation and sarcasm are hard. Also tends to require training/labelled data."
  },
  {
    "objectID": "lectures/7.4-Analysing_Text.html#clustering",
    "href": "lectures/7.4-Analysing_Text.html#clustering",
    "title": "Analysing Text",
    "section": "Clustering",
    "text": "Clustering\n\n\n\n\n\n\n\n\n\n\n\nCluster\nGeography\nEarth Science\nHistory\nComputer Science\nTotal\n\n\n\n\n1\n126\n310\n104\n11,018\n11,558\n\n\n2\n252\n10,673\n528\n126\n11,579\n\n\n3\n803\n485\n6,730\n135\n8,153\n\n\n4\n100\n109\n6,389\n28\n6,626\n\n\nTotal\n1,281\n11,577\n13,751\n11,307\n37,916"
  },
  {
    "objectID": "lectures/7.4-Analysing_Text.html#topic-modelling",
    "href": "lectures/7.4-Analysing_Text.html#topic-modelling",
    "title": "Analysing Text",
    "section": "Topic Modelling",
    "text": "Topic Modelling\nLearning associations of words (or images or many other things) to hidden ‘topics’ that generate them:"
  },
  {
    "objectID": "lectures/7.4-Analysing_Text.html#word-clouds",
    "href": "lectures/7.4-Analysing_Text.html#word-clouds",
    "title": "Analysing Text",
    "section": "Word Clouds",
    "text": "Word Clouds"
  },
  {
    "objectID": "lectures/7.4-Analysing_Text.html#resources",
    "href": "lectures/7.4-Analysing_Text.html#resources",
    "title": "Analysing Text",
    "section": "Resources",
    "text": "Resources\n\nOne-Hot vs Dummy Encoding\nCategorical encoding using Label-Encoding and One-Hot-Encoder\nCount Vectorization with scikit-learn\nTFIDF.com\nThe TF*IDF Algorithm Explained\nHow to Use TfidfTransformer and TfidfVectorizer\nSciKit Learn Feature Extraction\nYour Guide to LDA\nMachine Learning — Latent Dirichlet Allocation LDA\nA Beginner’s Guide to Latent Dirichlet Allocation(LDA)\nAnalyzing Documents with TF-IDF\n\nBasically any of the lessons on The Programming Historian."
  },
  {
    "objectID": "lectures/7.4-Analysing_Text.html#more-resources",
    "href": "lectures/7.4-Analysing_Text.html#more-resources",
    "title": "Analysing Text",
    "section": "More Resources",
    "text": "More Resources\n\nIntroduction to Word Embeddings\nThe Current Best of Universal Word Embeddings and Sentence Embeddings\nUsing GloVe Embeddings\nWorking with Facebook’s FastText Library\nWord2Vec and FastText Word Embedding with Gensim\nSentence Embeddings. Fast, please!\nPlasticityAI Embedding Models\nClustering text documents using k-means\nTopic extraction with Non-negative Matrix Factorization and LDA"
  },
  {
    "objectID": "lectures/8.1-Linking_Data.html#non-spatial-linkages",
    "href": "lectures/8.1-Linking_Data.html#non-spatial-linkages",
    "title": "Linking Data",
    "section": "Non-Spatial Linkages",
    "text": "Non-Spatial Linkages"
  },
  {
    "objectID": "lectures/8.1-Linking_Data.html#general-join-syntax",
    "href": "lectures/8.1-Linking_Data.html#general-join-syntax",
    "title": "Linking Data",
    "section": "General Join Syntax",
    "text": "General Join Syntax\nA join refers to the merging of two (or more) data tables using one (or more) matching columns:\nnew_df = pd.merge(df1, df2, on='SensorID')\n\nNote that if you want to use the index column which isn’t, technically, a column then you need to use left_index=True and right_index=True — where left is the first data set in the join.\nNote that the default behaviour is an inner join (i.e. defaults to how='inner')"
  },
  {
    "objectID": "lectures/8.1-Linking_Data.html#inner-join",
    "href": "lectures/8.1-Linking_Data.html#inner-join",
    "title": "Linking Data",
    "section": "Inner Join",
    "text": "Inner Join\nData Set 1\n\n\n\nSensorID\nPlace\nOwner\n\n\n\n\n1 ⇒\nLHR\nBAA\n\n\n2 ✘\nLGA\nGIP\n\n\n3 ⇒\nSTA\nMAG\n\n\n4 ⇒\nLUT\nLuton LA\n\n\n5 ✘\nSEN\nStobart"
  },
  {
    "objectID": "lectures/8.1-Linking_Data.html#section",
    "href": "lectures/8.1-Linking_Data.html#section",
    "title": "Linking Data",
    "section": " ",
    "text": "Data Set 2\n\n\n\nSensorID\nParameter\nValue\n\n\n\n\n1 ⇐\nTemperature\n5ºC\n\n\n1 ⇐\nHumidity\n15%\n\n\n3 ⇐\nTemperature\n7ºC\n\n\n4 ⇐\nTemperature\n7ºC\n\n\n6 ✘\nHumidity\n18%"
  },
  {
    "objectID": "lectures/8.1-Linking_Data.html#inner-join-result",
    "href": "lectures/8.1-Linking_Data.html#inner-join-result",
    "title": "Linking Data",
    "section": "Inner Join Result",
    "text": "Inner Join Result\nOn an Inner Join all non-matching rows are dropped:\nnew_df = pd.merge(df1, df2, \n                    how = 'inner',\n                    on  = 'SensorID')\n\n\n\nSensorID\nPlace\nOwner\nParameter\nValue\n\n\n\n\n1\nLHR\nBAA\nTemperature\n5ºC\n\n\n1\nLHR\nBAA\nHumidity\n15%\n\n\n3\nSTA\nMAG\nTemperature\n7ºC\n\n\n4\nLUT\nLuton LA\nTemperature\n7ºC"
  },
  {
    "objectID": "lectures/8.1-Linking_Data.html#but-what-if",
    "href": "lectures/8.1-Linking_Data.html#but-what-if",
    "title": "Linking Data",
    "section": "But What If…",
    "text": "But What If…\nIf Data Set 2 had a SensorKey instead of a SensorID then the command changes to:\nnew_df = pd.merge(df1, df2, \n                  how      = 'inner',\n                  left_on  = 'SensorID',\n                  right_on = 'SensorKey')\nThis will also impact the result:\n\n\n\nSensorID\nPlace\nOwner\nSensorKey\nParameter\nValue\n\n\n\n\n1\nLHR\nBAA\n1\nTemperature\n5ºC\n\n\n1\nLHR\nBAA\n1\nHumidity\n15%\n\n\n3\nSTA\nMAG\n3\nTemperature\n7ºC\n\n\n4\nLUT\nLuton LA\n4\nTemperature\n7ºC"
  },
  {
    "objectID": "lectures/8.1-Linking_Data.html#outer-join",
    "href": "lectures/8.1-Linking_Data.html#outer-join",
    "title": "Linking Data",
    "section": "Outer Join",
    "text": "Outer Join\nOn an Outer Join all rows are retained, including ones with no match:\nnew_df = pd.merge(df1, df2,\n                  how = 'outer',\n                  on  = 'SensorID')\n\n\n\nSensorID\nPlace\nOwner\nParameter\nValue\n\n\n\n\n1\nLHR\nBAA\nTemp.\n5℃\n\n\n1\nLHR\nBAA\nHum.\n15%\n\n\n2\nLGA\nGIP\nNaN\nNaN\n\n\n3\nSTA\nMAG\nTemp.\n7℃\n\n\n4\nLUT\nLuton Borough\nNaN\nNaN\n\n\n5\nSEN\nStobart\nNaN\nNaN\n\n\n6\nNaN\nNaN\nHum.\n20%"
  },
  {
    "objectID": "lectures/8.1-Linking_Data.html#left-join",
    "href": "lectures/8.1-Linking_Data.html#left-join",
    "title": "Linking Data",
    "section": "Left Join",
    "text": "Left Join\nOn a Left Join all rows on the left table are retained, including ones with no match, but unmatched right rows are dropped:\nnew_df = pd.merge(df1, df2, \n                 how = 'left',\n                 on  = 'SensorID')\n\n\n\nSensorID\nPlace\nOwner\nParameter\nValue\n\n\n\n\n1\nLHR\nBAA\nTemp.\n5℃\n\n\n1\nLHR\nBAA\nHum.\n15%\n\n\n2\nLGA\nGIP\nNaN\nNaN\n\n\n3\nSTA\nMAG\nTemp.\n7℃\n\n\n4\nLUT\nLuton Borough\nNaN\nNULL\n\n\n5\nSEN\nStobart\nNaN\nNaN"
  },
  {
    "objectID": "lectures/8.1-Linking_Data.html#any-guesses",
    "href": "lectures/8.1-Linking_Data.html#any-guesses",
    "title": "Linking Data",
    "section": "Any Guesses?",
    "text": "Any Guesses?\n\nAny guesses for the fourth type of join?"
  },
  {
    "objectID": "lectures/8.1-Linking_Data.html#append-concat",
    "href": "lectures/8.1-Linking_Data.html#append-concat",
    "title": "Linking Data",
    "section": "Append & Concat",
    "text": "Append & Concat\nPandas has two additional join-like functions:\n\nAppend: can be used to add a dict, Series, or DataFrame to the ‘bottom’ of an existing df. It’s not advisable to extend a df one row at a time (do bulk concatenations instead).\nConcat: can be used to concatenate two dfs together along either axis (rows or columns) “while performing optional set logic (union or intersection) of the indexes (if any) on the other axes.”"
  },
  {
    "objectID": "lectures/8.1-Linking_Data.html#concat",
    "href": "lectures/8.1-Linking_Data.html#concat",
    "title": "Linking Data",
    "section": "Concat",
    "text": "Concat\ndf3 = pd.DataFrame.from_dict({\n    'SensorID': [2,3,8,9,10],\n    'Place': ['STA','LUT','BHX','MAN','INV'],\n    'Owner': ['BAA','Luton LA','???','???','???']\n})\npd.concat([df1, df3], ignore_index=True)\nOutputs:\n\n\n\n\nSensorID\nPlace\nOwner\n\n\n\n\n0\n1\nLHR\nBAA\n\n\n\\[\\vdots\\]\n\\[\\vdots\\]\n\\[\\vdots\\]\n\\[\\vdots\\]\n\n\n4\n5\nSEN\nStobart\n\n\n5\n2\nSTA\nBAA\n\n\n6\n3\nLUT\nGIP\n\n\n7\n8\nBHX\n???\n\n\n8\n9\nMAN\n???\n\n\n9\n10\nINV\n???"
  },
  {
    "objectID": "lectures/8.1-Linking_Data.html#append",
    "href": "lectures/8.1-Linking_Data.html#append",
    "title": "Linking Data",
    "section": "Append",
    "text": "Append\nto_append = [\n    {'SensorID': 5, 'Parameter': 'Humidity', 'Value': 0.45},\n    {'SensorID': 5, 'Parameter': 'Humidity', 'Value': 0.31},\n    {'SensorID': 4, 'Parameter': 'Temperature', 'Value': 2},\n    {'SensorID': 3, 'Parameter': 'Temperature', 'Value': 3}]\ndf2.append(to_append)\nOutputs:\n\n\n\nSensorID\nParameter\nValue\n\n\n\n\n\n0\n1\nTemperature\n5.00\n\n\n\\[\\vdots\\]\n\\[\\vdots\\]\n\\[\\vdots\\]\n\\[\\vdots\\]\n\n\n4\n6\nHumidity\n0.18\n\n\n0\n5\nHumidity\n0.45\n\n\n1\n5\nHumidity\n0.31\n\n\n2\n4\nTemperature\n2.00\n\n\n3\n3\nTemperature\n3.00\n\n\n\n\nNote that a Dictionary-of-Lists would also work for an append and that appending a column that doesn’t exist (for vertical appends) will cause the column to be created while appending a row that doesn’t exist (for horizontal appends) with cause the row to be created."
  },
  {
    "objectID": "lectures/8.1-Linking_Data.html#merge-vs.-append-vs.-concat-vs.-join",
    "href": "lectures/8.1-Linking_Data.html#merge-vs.-append-vs.-concat-vs.-join",
    "title": "Linking Data",
    "section": "Merge vs. Append vs. Concat vs. Join?",
    "text": "Merge vs. Append vs. Concat vs. Join?\nAs usual, Stack Overflow to the rescue:\n\nA very high level difference is that merge() is used to combine two (or more) dataframes on the basis of values of common columns (indices can also be used, use left_index=True and/or right_index=True), and concat() is used to append one (or more) dataframes one below the other (or sideways, depending on whether the axis option is set to 0 or 1).\n\n\njoin() is used to merge 2 dataframes on the basis of the index; instead of using merge() with the option left_index=True we can use join().\n\nHint: axis=0 refers to the row index & axis=1 to the column index."
  },
  {
    "objectID": "lectures/8.1-Linking_Data.html#merge-vs.-append-vs.-concat-vs.-join-1",
    "href": "lectures/8.1-Linking_Data.html#merge-vs.-append-vs.-concat-vs.-join-1",
    "title": "Linking Data",
    "section": "Merge vs. Append vs. Concat vs. Join?",
    "text": "Merge vs. Append vs. Concat vs. Join?\nThese achieve the same thing, but they are not always equivalent:\npd.merge(df1, df2, left_index=True, right_index=True)\npd.concat([df1, df2], axis=1)\ndf1.join(df2)\nGenerally:\n\nConcat expects the number of columns in all data frames to match (if concatenating vertically) and the number of rows in all data frames to match (if concatenating horizontally). It does not deal well with linking.\nAppend assumes that either the columns or the rows will match.\nJoin is basically a functionality-restricted merge."
  },
  {
    "objectID": "lectures/8.1-Linking_Data.html#resources",
    "href": "lectures/8.1-Linking_Data.html#resources",
    "title": "Linking Data",
    "section": "Resources",
    "text": "Resources\n\nPandas Guide to Merges\nDatabase-style joining/merging\nPandas Concat\nPandas Append"
  },
  {
    "objectID": "lectures/8.2-Linking_Spatial_Data.html#non-spatial-linkages-can-be-spatial",
    "href": "lectures/8.2-Linking_Spatial_Data.html#non-spatial-linkages-can-be-spatial",
    "title": "Linking Spatial Data",
    "section": "Non-Spatial Linkages can be Spatial",
    "text": "Non-Spatial Linkages can be Spatial"
  },
  {
    "objectID": "lectures/8.2-Linking_Spatial_Data.html#sjoin-vs.-join",
    "href": "lectures/8.2-Linking_Spatial_Data.html#sjoin-vs.-join",
    "title": "Linking Spatial Data",
    "section": "Sjoin vs. Join",
    "text": "Sjoin vs. Join\nSjoin adds an operator (['intersects','contains','within']) and example code can be found on GitHub.\ngdf = gpd.GeoDataFrame(df, \n            geometry=gpd.points_from_xy(df.longitude, df.latitude,\n            crs='epsg:4326')).to_crs('epsg:27700')\nhackney = boros[boros.NAME=='Hackney']\nrs = gpd.sjoin(gdf, hackney, op='within')"
  },
  {
    "objectID": "lectures/8.2-Linking_Spatial_Data.html#combining-operators-how",
    "href": "lectures/8.2-Linking_Spatial_Data.html#combining-operators-how",
    "title": "Linking Spatial Data",
    "section": "Combining Operators & How",
    "text": "Combining Operators & How\nChanging how to left, right, or inner changes the join’s behaviour:\nrs = gpd.sjoin(gdf, hackney, how='left', op='within')\nrs.NAME.fillna('None', inplace=True)\nax = boros[boros.NAME=='Hackney'].plot(edgecolor='k', facecolor='none', figsize=(8,8))\nrs.plot(ax=ax, column='NAME', legend=True)"
  },
  {
    "objectID": "lectures/8.2-Linking_Spatial_Data.html#merging-data",
    "href": "lectures/8.2-Linking_Spatial_Data.html#merging-data",
    "title": "Linking Spatial Data",
    "section": "Merging Data",
    "text": "Merging Data\nThese merge operators apply where a is the left set of features (in a GeoSeries or GeoDataFrame) and b is the right set:\n\nContains: Returns True if no points of b lie outside of a and at least one point of b lies inside a.\nWithin: Returns True if a’s boundary and interior intersect only with the interior of b (not its boundary or exterior).\nIntersects: Returns True if the boundary or interior of a intersects in any way with those of b.\n\nAll binary predicates are supported by features of GeoPandas, though only these three options are available in sjoin directly.\n\nBehaviour of operaions may vary with how you set up left and right tables, but you can probably think your way through it by asking: “Which features of x fall within features of y?” or “Do features of x contain y?” You will probably get it wrong at least a few times. That’s ok."
  },
  {
    "objectID": "lectures/8.2-Linking_Spatial_Data.html#additional-spatial-operations",
    "href": "lectures/8.2-Linking_Spatial_Data.html#additional-spatial-operations",
    "title": "Linking Spatial Data",
    "section": "Additional Spatial Operations",
    "text": "Additional Spatial Operations\nThese operators apply to the GeoSeries where a is a GeoSeries and b is one or more spatial features:\n\nContains / Covers: Returns a Series of dtype('bool') with value True for each geometry in a that contains b. These are different.\nCrosses: An object is said to cross other if its interior intersects the interior of the other but does not contain it, and the dimension of the intersection is less than the dimension of the one or the other.\nTouches: Returns a Series indicating which elements of a touch a point on b.\nDistance: Returns a Series containing the distance from all a to some b.\nDisjoint: Returns a Series indicating which elements of a do not intersect with any b.\nGeom Equals / Geom Almost Equals: strict and loose tests of equality between a and b in terms of their geometry.\nBuffer, Simplify, Centroid, Representative Point: common transformations.\nRotate, Scale, Affine Transform, Skew, Translate: less common transformations.\nUnary Union: aggregation of all the geometries in a"
  },
  {
    "objectID": "lectures/8.2-Linking_Spatial_Data.html#rtm",
    "href": "lectures/8.2-Linking_Spatial_Data.html#rtm",
    "title": "Linking Spatial Data",
    "section": "RTM",
    "text": "RTM\n\nIn particular, “contains” (and its converse “within”) has an aspect of its definition which may produce unexpected behaviour. This quirk can be expressed as “Polygons do not contain their boundary”. More precisely, the definition of contains is: Geometry A contains Geometry B iff no points of B lie in the exterior of A, and at least one point of the interior of B lies in the interior of A That last clause causes the trap – because of it, a LineString which is completely contained in the boundary of a Polygon is not considered to be contained in that Polygon! This behaviour could easily trip up someone who is simply trying to find all LineStrings which have no points outside a given Polygon. In fact, this is probably the most common usage of contains. For this reason it’s useful to define another predicate called covers, which has the intuitively expected semantics: Geometry A covers Geometry B iff no points of B lie in the exterior of A"
  },
  {
    "objectID": "lectures/8.2-Linking_Spatial_Data.html#set-operations-with-overlay",
    "href": "lectures/8.2-Linking_Spatial_Data.html#set-operations-with-overlay",
    "title": "Linking Spatial Data",
    "section": "Set Operations with Overlay",
    "text": "Set Operations with Overlay\nIt is also possible to apply GIS-type ‘overlay’ operations:\n\nThese operations return indexes for gdf1 and gdf2 (either could be a NaN) together with a geometry and (usually?) columns from both data frames:\nrs_union = geopandas.overlay(gdf1, gdf2, how='union')\nThe set of operations includes: union, intersection, difference, symmetric_difference, and identity.\n\nA notebook example can be found here."
  },
  {
    "objectID": "lectures/8.2-Linking_Spatial_Data.html#making-a-plan",
    "href": "lectures/8.2-Linking_Spatial_Data.html#making-a-plan",
    "title": "Linking Spatial Data",
    "section": "Making a Plan…",
    "text": "Making a Plan…"
  },
  {
    "objectID": "lectures/8.2-Linking_Spatial_Data.html#think-it-through",
    "href": "lectures/8.2-Linking_Spatial_Data.html#think-it-through",
    "title": "Linking Spatial Data",
    "section": "Think it Through",
    "text": "Think it Through\nAs your data grows in volume, the consequences of choosing the ‘wrong’ approach become more severe. Making a plan of attack becomes essential and it boils down to the following:\n\nSpatial joins are hard\nNon-spatial joins are easy\nKey-/Index-based joins are easiest\nAddition conditions to joins makes them harder.\n\nSo, when you have multiple joins…\n\nDo the easy ones first (they will run quickly on large data sets).\nDo the hard ones last (they will benefit most from the filtering process)."
  },
  {
    "objectID": "lectures/8.2-Linking_Spatial_Data.html#whats-the-right-order",
    "href": "lectures/8.2-Linking_Spatial_Data.html#whats-the-right-order",
    "title": "Linking Spatial Data",
    "section": "What’s the ‘Right’ Order?",
    "text": "What’s the ‘Right’ Order?\nQ. Find me a nearby family-style Italian restaurant…\n{{Org-Chart}} - City = New York - Cuisine = Italian - Style = Family - Location = Within Neighbourhood"
  },
  {
    "objectID": "lectures/8.2-Linking_Spatial_Data.html#mis-matched-scales",
    "href": "lectures/8.2-Linking_Spatial_Data.html#mis-matched-scales",
    "title": "Linking Spatial Data",
    "section": "Mis-matched Scales",
    "text": "Mis-matched Scales\nKeep in mind:\n\nWith complex geometries and mis-matched scales, converting the smaller geometry to centroids or representative points can speed things up a lot (within, contains, etc. become much simpler).\n\nAnd that:\n\nWith large data sets, rasterising the smaller and more ‘abundant’ geometry can speed things up a lot."
  },
  {
    "objectID": "lectures/8.2-Linking_Spatial_Data.html#long-term-view",
    "href": "lectures/8.2-Linking_Spatial_Data.html#long-term-view",
    "title": "Linking Spatial Data",
    "section": "Long-Term View",
    "text": "Long-Term View\n\nUltimately, if you continue to work with large spatial data sets you’ll need to look into web services and spatial databases."
  },
  {
    "objectID": "lectures/8.2-Linking_Spatial_Data.html#web-services",
    "href": "lectures/8.2-Linking_Spatial_Data.html#web-services",
    "title": "Linking Spatial Data",
    "section": "Web Services",
    "text": "Web Services\n\n\n\n\n\n\n\n\nAcronym\nMeans\nDoes\n\n\n\n\nWMS\nWeb Map Service\nTransfers map images within an area specified by bounding box; vector formats possible, but rarely used.\n\n\nWFS\nWeb Feature Service\nAllows interaction with features; so not about rendering maps directly, more about manipulation.\n\n\nWCS\nWeb Coverage Service\nTransfers data about objects covering a geographical area.\n\n\nOWS\nOpen Web Services\nSeems to be used by QGIS to serve data from a PC or server."
  },
  {
    "objectID": "lectures/8.2-Linking_Spatial_Data.html#spatial-databases",
    "href": "lectures/8.2-Linking_Spatial_Data.html#spatial-databases",
    "title": "Linking Spatial Data",
    "section": "Spatial Databases",
    "text": "Spatial Databases\nThere are many flavours:\n\nOracle: good enterprise support; reasonably feature-rich.\nMySQL: free; was feature-poor (though this looks to have changed with MySQL8).\nPostreSQL: free; standards-setting/compliant; heavyweight (requires PostGIS).\nMicrosoft Access: um, no.\nSpatiaLite: free; standards-setting/compliant; lightweight\n\nGenerally:\n\nAd-hoc, modestly-sized, highly portable == SpatiaLite\nPermanent, large, weakly portable == Postgres+PostGIS"
  },
  {
    "objectID": "lectures/8.3-Grouping_Data.html#grouping-operations",
    "href": "lectures/8.3-Grouping_Data.html#grouping-operations",
    "title": "Grouping Data",
    "section": "Grouping Operations",
    "text": "Grouping Operations\nIn Pandas these follow a split / apply / combine approach:\n\n\nNote that, for simplicity, I’ve abbreviate the Local Authority names since this is just a simplified example: TH (Tower Hamlets), HAK (Hackney), W (Westminster)."
  },
  {
    "objectID": "lectures/8.3-Grouping_Data.html#in-practice",
    "href": "lectures/8.3-Grouping_Data.html#in-practice",
    "title": "Grouping Data",
    "section": "In Practice",
    "text": "In Practice\ngrouped_df = df.groupby(<fields>).<function>\nFor instance, if we had a Local Authority (LA) field:\ngrouped_df = df.groupby('LA').sum()\nUsing apply the function could be anything:\ndef norm_by_data(x): # x is a column from the grouped df\n    x['d1'] /= x['d2'].sum() \n    return x\n\ndf.groupby('LA').apply(norm_by_data)"
  },
  {
    "objectID": "lectures/8.3-Grouping_Data.html#grouping-by-arbitrary-mappings",
    "href": "lectures/8.3-Grouping_Data.html#grouping-by-arbitrary-mappings",
    "title": "Grouping Data",
    "section": "Grouping by Arbitrary Mappings",
    "text": "Grouping by Arbitrary Mappings\nmapping = {'HAK':'Inner', 'TH':'Outer', 'W':'Inner'}\ndf.set_index('LA', inplace=True)\ndf.groupby(mapping).sum()"
  },
  {
    "objectID": "lectures/8.3-Grouping_Data.html#pivot-tables",
    "href": "lectures/8.3-Grouping_Data.html#pivot-tables",
    "title": "Grouping Data",
    "section": "Pivot Tables",
    "text": "Pivot Tables\nA ‘special case’ of Group By features:\n\nCommonly-used in business to summarise data for reporting.\nGrouping (summarisation) happens along both axes (Group By operates only on one).\npandas.cut(<series>, <bins>) can be a useful feature here since it chops a continuous feature into bins suitable for grouping."
  },
  {
    "objectID": "lectures/8.3-Grouping_Data.html#in-practice-1",
    "href": "lectures/8.3-Grouping_Data.html#in-practice-1",
    "title": "Grouping Data",
    "section": "In Practice",
    "text": "In Practice\nage = pd.cut(titanic['age'], [0, 18, 80])\ntitanic.pivot_table('survived', ['sex', age], 'class')"
  },
  {
    "objectID": "lectures/8.3-Grouping_Data.html#deriving-measures-of-diversity",
    "href": "lectures/8.3-Grouping_Data.html#deriving-measures-of-diversity",
    "title": "Grouping Data",
    "section": "Deriving Measures of Diversity",
    "text": "Deriving Measures of Diversity\n\nOne of the benefits of grouping is that it enables us to derive measures of density and diversity; here are just a few… Location Quotient (LQ), Herfindah-Hirschman Index (HHI), Shanon Entropy.\n\n\nI like easy measures."
  },
  {
    "objectID": "lectures/8.3-Grouping_Data.html#location-quotient",
    "href": "lectures/8.3-Grouping_Data.html#location-quotient",
    "title": "Grouping Data",
    "section": "Location Quotient",
    "text": "Location Quotient\nThe LQ for industry i in zone z is the share of employment for i in z divided by the share of employment of i in the entire region R.\n\\[\nLQ_{zi} = \\dfrac{Emp_{zi}/Emp_{z}}{Emp_{Ri}/Emp_{R}}\n\\]\n\n\n\n \nHigh Local Share\nLow Local Share\n\n\n\n\nHigh Regional Share\n\\[\\approx 1\\]\n\\[< 1\\]\n\n\nLow Regional Share\n\\[> 1\\]\n\\[\\approx 1\\]\n\n\n\n\nIn other words, this is a type of standardisation that enables to compare the concentration of Investment Bankers with the concentration of Accountants, even if there are many more Accountants than Bankers! But this can also apply to the share of flats to whole-property lettings just as easily.\nNote that this is influenced by small sample sizes (e.g. the number of Fijians in Britain)."
  },
  {
    "objectID": "lectures/8.3-Grouping_Data.html#herfindahl-hirschman-index",
    "href": "lectures/8.3-Grouping_Data.html#herfindahl-hirschman-index",
    "title": "Grouping Data",
    "section": "Herfindahl-Hirschman index",
    "text": "Herfindahl-Hirschman index\nThe HHI for an industry i is the sum of squared market shares for each company in that industry:\n\\[\nH = \\sum_{i=1}^{N} s_{i}^{2}\n\\]\n\n\n\n\n\n\n\nConcentration Level\nHHI\n\n\n\n\nMonopolistic: one firm accounts for 100% of the market\n\\[1.0\\]\n\n\nOligopolistic: top five firms account for 60% of the market\n\\[\\approx 0.8\\]\n\n\nCompetitive: anything else?\n\\[< 0.5\\]?\n\n\n\n\nIf \\(s_{i} = 1\\) then \\(s_{i}^{2} = 1\\), while if \\(s_{i} = 0.5\\) then \\(s_{i}^{2} = 0.25\\) and \\(s_{i} = 0.1\\) then \\(s_{i}^{2} = 0.01\\).\nThis can be translated to compare, for instance, local and regional neighbourhood diversity: some cities are ethnically diverse in aggregate but highly segregated at the local level.\nNote that this is influenced by the number of ‘firms’ (or ethnicities or…)."
  },
  {
    "objectID": "lectures/8.3-Grouping_Data.html#shannon-entropy",
    "href": "lectures/8.3-Grouping_Data.html#shannon-entropy",
    "title": "Grouping Data",
    "section": "Shannon Entropy",
    "text": "Shannon Entropy\nShannon Entropy is an information-theoretic measure:\n\\[\nH(X) = - \\sum_{i=1}^{n} P(x_{i}) log P(x_{i})\n\\]\n\nI often think of this as ‘surprise’: a high entropy measure means that it’s hard to predict what will happen next. So randomness has high entropy. By extension, high concentration has low entropy (even if the result is surprising on the level of intuition: I wasn’t expecting to see that) because I can predict a 6 on the next roll of the dice fairly easy if all of my previous rolls were 6s."
  },
  {
    "objectID": "lectures/8.4-Visualising_Data.html#seaborn",
    "href": "lectures/8.4-Visualising_Data.html#seaborn",
    "title": "Visualising Data",
    "section": "Seaborn",
    "text": "Seaborn\nDesigned to provide ggplot-like quality output using matplotlib back-end:\n\nImprove on default colourmaps and colour defaults.\nIntegration with pandas data frames (Note: no geopandas!).\nOffer more plot types out of the box.\nStill offer access to matplotlib’s back-end."
  },
  {
    "objectID": "lectures/8.4-Visualising_Data.html#plot-types",
    "href": "lectures/8.4-Visualising_Data.html#plot-types",
    "title": "Visualising Data",
    "section": "Plot Types",
    "text": "Plot Types"
  },
  {
    "objectID": "lectures/8.4-Visualising_Data.html#in-practice",
    "href": "lectures/8.4-Visualising_Data.html#in-practice",
    "title": "Visualising Data",
    "section": "In Practice",
    "text": "In Practice\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\nfmri = sns.load_dataset(\"fmri\")\nsns.lineplot(x=\"timepoint\", y=\"signal\",\n             hue=\"region\", style=\"event\",\n             data=fmri)"
  },
  {
    "objectID": "lectures/8.4-Visualising_Data.html#in-practice-2",
    "href": "lectures/8.4-Visualising_Data.html#in-practice-2",
    "title": "Visualising Data",
    "section": "In Practice 2",
    "text": "In Practice 2\nsns.set_theme(style=\"whitegrid\", palette=\"muted\")\ndf = sns.load_dataset(\"penguins\")\n\nax = sns.swarmplot(data=df, x=\"body_mass_g\", y=\"sex\", hue=\"species\")\nax.set(ylabel=\"\")"
  },
  {
    "objectID": "lectures/8.4-Visualising_Data.html#configuring-seaborn",
    "href": "lectures/8.4-Visualising_Data.html#configuring-seaborn",
    "title": "Visualising Data",
    "section": "Configuring Seaborn",
    "text": "Configuring Seaborn\nSeaborn provides a number of useful themes that act as shortcuts for setting multiple matplotlib parameters:\n\n\n\nSeaborn Command\nAccomplishes\n\n\n\n\nset_theme(...)\nSet multiple theme parameters in one step.\n\n\naxes_style(...)\nReturn a parameter dict for the aesthetic style of the plots.\n\n\nset_style(...)\nSet the aesthetic style of the plots.\n\n\nplotting_context(...)\nReturn a parameter dict to scale elements of the figure.\n\n\nset_context(...)\nSet the plotting context parameters.\n\n\n\nYou can also access:\n\nPalettes: colormaps can be generated using sns.color_palette(...) and set using sns.set_palette(...).\nAxes Styles: includes ‘darkgrid’, ‘whitegrid’, ‘dark’, ‘white’, ‘ticks’."
  },
  {
    "objectID": "lectures/8.4-Visualising_Data.html#matplotlib",
    "href": "lectures/8.4-Visualising_Data.html#matplotlib",
    "title": "Visualising Data",
    "section": "Matplotlib",
    "text": "Matplotlib\n\nSource: Anatomy of a figure."
  },
  {
    "objectID": "lectures/8.4-Visualising_Data.html#writing-to-plots",
    "href": "lectures/8.4-Visualising_Data.html#writing-to-plots",
    "title": "Visualising Data",
    "section": "Writing to Plots",
    "text": "Writing to Plots\nConfusingly, there are multiple ways to access elements of the plot and write into them:\n\nFigure: high-level features (e.g. title, padding, etc.). Can be accessed via plt.gcf() (get current figure) or upon creation (e.g. f, ax = plt.subplots() or f = plt.figure()).\nAxes: axis-level features (e.g. labels, tics, spines, limits, etc.). Can be accessed via plt.gca() (get current axes) or upon creation (e.g. f, ax = plt.subplots() or ax = f.add_subplot(1,1,1)).\n\nAnnotations, artists, and other features are typically written into the axes using the coordinate space of the figure (e.g. decimal degrees for lat/long, metres for BNG, etc.)."
  },
  {
    "objectID": "lectures/8.4-Visualising_Data.html#adding-a-3rd-dimension",
    "href": "lectures/8.4-Visualising_Data.html#adding-a-3rd-dimension",
    "title": "Visualising Data",
    "section": "Adding a 3rd Dimension",
    "text": "Adding a 3rd Dimension\nThis ‘feature’ is less well-developed but does work:\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure()\nax  = plt.axes(projection='3d')\n# OR\nfig = plt.figure()\nax  = fig.add_subplot(111, projection='3d')\n# THEN\nax.contour3D(X, Y, Z, ...)\nax.plot_surface(x, y, z, ...)\nax.plot3D(xline, yline, zline, ...)\nax.scatter3D(x, y, z, ...)\n# ax.plot_surface and ax.plot_wire also give you 3D renderings\nYou can then set the elevation and azimuth using: ax.view_init(<elevation>, <azimuth>)."
  },
  {
    "objectID": "lectures/8.4-Visualising_Data.html#saving-outputs",
    "href": "lectures/8.4-Visualising_Data.html#saving-outputs",
    "title": "Visualising Data",
    "section": "Saving Outputs",
    "text": "Saving Outputs\nStraightforward via save figure function, but lots of options!\nplt.savefig(fname, dpi=None, facecolor='w', edgecolor='w',\n    orientation='portrait', papertype=None, format=None,\n    transparent=False, bbox_inches=None, pad_inches=0.1,\n    frameon=None, metadata=None)\nThe format can be largely determined by the file extension in the fname (file name) and the supported formats depends on what you’ve installed! You can find out waht’s available using: plt.gcf().canvas.get_supported_filetypes()."
  },
  {
    "objectID": "lectures/8.4-Visualising_Data.html#jupyter",
    "href": "lectures/8.4-Visualising_Data.html#jupyter",
    "title": "Visualising Data",
    "section": "Jupyter",
    "text": "Jupyter\nBy default, Jupyter’s output is static matplotlib, but we can extend this in three ways:\n\nMake the static plot zoomable and pannable using %matplotlib widget (declare this at the top of your notebook).\nMake the plot more directly interactive using ipywidgets (import interact and related libs as needed).\nUse a browser-based visualisation tool such as bokeh or d3 (format may be very, very different from what you are ‘used to’ in Python)."
  },
  {
    "objectID": "lectures/8.4-Visualising_Data.html#widgets",
    "href": "lectures/8.4-Visualising_Data.html#widgets",
    "title": "Visualising Data",
    "section": "Widgets",
    "text": "Widgets\n%matplotlib widget\nrs = gpd.sjoin(gdf, hackney, how='left', op='within')\nrs.NAME.fillna('None', inplace=True)\nax = hackney.plot(edgecolor='k', facecolor='none')\nrs.plot(ax=ax, column='NAME', legend=True)"
  },
  {
    "objectID": "lectures/8.4-Visualising_Data.html#interact",
    "href": "lectures/8.4-Visualising_Data.html#interact",
    "title": "Visualising Data",
    "section": "Interact()",
    "text": "Interact()\nTaking an example from Dani’s work:\nfrom ipywidgets import interact\n# Alternatives: interactive, fixed, interact_manual\ninteract(\n    <function>, # Function to make interactive\n    <param0>,   # e.g. Data to use\n    <param1>,   # e.g. Range start/end/step\n    <param2>    # e.g. Fixed value\n);"
  },
  {
    "objectID": "lectures/8.4-Visualising_Data.html#bokeh",
    "href": "lectures/8.4-Visualising_Data.html#bokeh",
    "title": "Visualising Data",
    "section": "Bokeh",
    "text": "Bokeh"
  },
  {
    "objectID": "lectures/8.4-Visualising_Data.html#automation",
    "href": "lectures/8.4-Visualising_Data.html#automation",
    "title": "Visualising Data",
    "section": "Automation",
    "text": "Automation\nBecause of the way that matplotlib and Jupyter work, all resources built on top of these ‘platforms’ can, to some extent, be automated using functions. For example to draw circles and place text:\ndef circle(x, y, radius=0.15):\n    from matplotlib.patches import Circle\n    from matplotlib.patheffects import withStroke\n    circle = Circle((x, y), radius, clip_on=False, zorder=10, \n                    linewidth=1, edgecolor='black', \n                    facecolor=(0, 0, 0, .0125),\n                    path_effects=[withStroke(linewidth=5, \n                                  foreground='w')])\n    ax.add_artist(circle)\n\ndef text(x, y, text):\n    ax.text(x, y, text, backgroundcolor=\"white\",\n         ha='center', va='top', weight='bold', color='blue')"
  },
  {
    "objectID": "lectures/8.4-Visualising_Data.html#resources",
    "href": "lectures/8.4-Visualising_Data.html#resources",
    "title": "Visualising Data",
    "section": "Resources",
    "text": "Resources\n\nIntroduction to PyPlot (includes lots of parameter information)\nVisualisation with Seaborn\nSeaborn Tutorial\nElite Data Science Seaborn Tutorial\nDatacamp Seaborn Tutorial\nThree-Dimensional Plotting in Matplotlib\nAn easy introduction to 3D plotting with Matplotlib\nBokeh Gallery\nBokeh User Guide\nProgramming Historian: Visualizing Data with Bokeh and Pandas\nReal Python: Data Viz with Bokeh\nTowards Data Science: Data Viz with Bokeh Part 1\nUsing Interact"
  },
  {
    "objectID": "lectures/9.1-Data_Space.html#the-data-generating-process",
    "href": "lectures/9.1-Data_Space.html#the-data-generating-process",
    "title": "The Data Space",
    "section": "The Data Generating Process",
    "text": "The Data Generating Process\n\n\nSource: Are Statisticians Cold-Blooded Bosses?"
  },
  {
    "objectID": "lectures/9.1-Data_Space.html#the-data-generating-process-1",
    "href": "lectures/9.1-Data_Space.html#the-data-generating-process-1",
    "title": "The Data Space",
    "section": "The Data Generating Process",
    "text": "The Data Generating Process\n\n\nSource: Are Statisticians Cold-Blooded Bosses?"
  },
  {
    "objectID": "lectures/9.1-Data_Space.html#cashier-income-as-dgp",
    "href": "lectures/9.1-Data_Space.html#cashier-income-as-dgp",
    "title": "The Data Space",
    "section": "Cashier Income as DGP",
    "text": "Cashier Income as DGP\nQuestion: Retail cashier annual salaries have a Normal distribution with a mean equal to $25,000 and a standard deviation equal to $2,000. What is the probability that a randomly selected retail cashier earns more than $27,000?\nAnswer: 15.87%\nResult: All models are wrong, but some are useful (George Box)"
  },
  {
    "objectID": "lectures/9.1-Data_Space.html#house-prices-as-dgp",
    "href": "lectures/9.1-Data_Space.html#house-prices-as-dgp",
    "title": "The Data Space",
    "section": "House Prices as DGP",
    "text": "House Prices as DGP"
  },
  {
    "objectID": "lectures/9.1-Data_Space.html#distance",
    "href": "lectures/9.1-Data_Space.html#distance",
    "title": "The Data Space",
    "section": "Distance…",
    "text": "Distance…"
  },
  {
    "objectID": "lectures/9.1-Data_Space.html#so",
    "href": "lectures/9.1-Data_Space.html#so",
    "title": "The Data Space",
    "section": "So…",
    "text": "So…\nInstinctively, we know that Bill Gates’ wealth is much further from ‘normal’ than is his height. But how?\n\nHow can we compare income and height if they share no common units?\nHow can we compare the biodiversity of sites in the tropics with those of sub-Arctic areas given that there are different numbers of species to begin with?\n\nWe need ways to make different dimensions comparable, and we need ways to remove unit effects from distance measures."
  },
  {
    "objectID": "lectures/9.1-Data_Space.html#distance-in-1d",
    "href": "lectures/9.1-Data_Space.html#distance-in-1d",
    "title": "The Data Space",
    "section": "Distance in 1D",
    "text": "Distance in 1D\n\\[\nd(i,j) = |(i_{1}-j_{1})|\n\\]"
  },
  {
    "objectID": "lectures/9.1-Data_Space.html#distance-in-2d",
    "href": "lectures/9.1-Data_Space.html#distance-in-2d",
    "title": "The Data Space",
    "section": "Distance in 2D",
    "text": "Distance in 2D\n\\[\nd(i,j) = \\sqrt{(i_{1}-j_{1})^{2}+(i_{2}-j_{2})^{2}}\n\\]"
  },
  {
    "objectID": "lectures/9.1-Data_Space.html#distance-in-3d-or-more",
    "href": "lectures/9.1-Data_Space.html#distance-in-3d-or-more",
    "title": "The Data Space",
    "section": "Distance in 3D… or More",
    "text": "Distance in 3D… or More\nWe can keep adding dimensions…\n\\[\nd(i,j) = \\sqrt{(i_{1}-j_{1})^{2}+(i_{2}-j_{2})^{2}+(i_{3}-j_{3})^{2}}\n\\]\nYou continue adding dimensions indefinitely, but from here on out you are dealing with hyperspaces!"
  },
  {
    "objectID": "lectures/9.1-Data_Space.html#thinking-in-data-space",
    "href": "lectures/9.1-Data_Space.html#thinking-in-data-space",
    "title": "The Data Space",
    "section": "Thinking in Data Space",
    "text": "Thinking in Data Space\nWe can write the coordinates of an observation with 3 attributes (e.g. height, weight, income) as: \\[\nx_{i} = \\\\\\{ {x_{i1}, x_{i2}, x_{i3} } \\\\\\}\n\\]\nAnd we can think of something with 8 attributes (e.g. height, weight, income, age, year of birth, …) as occupying an 8-dimensional space (which, for obvious reasons, we can’t plot in one go)."
  },
  {
    "objectID": "lectures/9.1-Data_Space.html#two-propositions",
    "href": "lectures/9.1-Data_Space.html#two-propositions",
    "title": "The Data Space",
    "section": "Two Propositions",
    "text": "Two Propositions\n\nThat geographical space is no different from any other dimension in a data set.\nThat geographical space is still special when it comes to thinking about relationships."
  },
  {
    "objectID": "lectures/9.1-Data_Space.html#resources",
    "href": "lectures/9.1-Data_Space.html#resources",
    "title": "The Data Space",
    "section": "Resources",
    "text": "Resources"
  },
  {
    "objectID": "lectures/9.2-Transformation.html#transformation-in-1d",
    "href": "lectures/9.2-Transformation.html#transformation-in-1d",
    "title": "Transformation",
    "section": "Transformation in 1D",
    "text": "Transformation in 1D"
  },
  {
    "objectID": "lectures/9.2-Transformation.html#transformation-in-1d-1",
    "href": "lectures/9.2-Transformation.html#transformation-in-1d-1",
    "title": "Transformation",
    "section": "Transformation in 1D",
    "text": "Transformation in 1D\n\\[\nx-\\bar{x}\n\\]\n\n\nHow is this any different?"
  },
  {
    "objectID": "lectures/9.2-Transformation.html#so",
    "href": "lectures/9.2-Transformation.html#so",
    "title": "Transformation",
    "section": "So…",
    "text": "So…\n\nTransformations are mathematical operations applied to every observation in a data set that preserve some of the relationships between them."
  },
  {
    "objectID": "lectures/9.2-Transformation.html#for-example",
    "href": "lectures/9.2-Transformation.html#for-example",
    "title": "Transformation",
    "section": "For Example",
    "text": "For Example\nIf we subtract the mean from everyone’s height then we can immediately tell if someone is taller or shorter than we would expect.\nIf we subtract the mean from everyone’s income then we cannot immediately tell if someone is earning more or less that we would expect.\nSo what is a useful transformation in one context, may not be in another!"
  },
  {
    "objectID": "lectures/9.2-Transformation.html#fleshing-this-out",
    "href": "lectures/9.2-Transformation.html#fleshing-this-out",
    "title": "Transformation",
    "section": "Fleshing This Out",
    "text": "Fleshing This Out\nQuestion: How can you tell if you did better than everyone else on the Quiz or on the Data Set Biography?\nAnswer: Just subtracting the mean is not enough because the distributions are not the same. For that we also need to standardise the data in some way.\n\\[\nz = \\dfrac{x-\\bar{x}}{\\sigma}\n\\]\nDivide through by the distribution!"
  },
  {
    "objectID": "lectures/9.2-Transformation.html#z-score-standardisation",
    "href": "lectures/9.2-Transformation.html#z-score-standardisation",
    "title": "Transformation",
    "section": "Z-Score Standardisation",
    "text": "Z-Score Standardisation\n\\[\n\\dfrac{x-\\bar{x}}{\\sigma}\n\\]\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(data)\nprint(scaler.mean_)\nscaler.transform(data)\nscaler.transform(<new data>)\n\nThe important thing to note is that if transform data that has not been fit and you get values outside the range used for fitting then you can no longer assume a standard normal."
  },
  {
    "objectID": "lectures/9.2-Transformation.html#interquartile-standardisation",
    "href": "lectures/9.2-Transformation.html#interquartile-standardisation",
    "title": "Transformation",
    "section": "Interquartile Standardisation",
    "text": "Interquartile Standardisation\n\\[\n\\dfrac{x_{i}-x_{Q2}}{x_{Q3}-x_{Q1}}\n\\]\nfrom sklearn.preprocessing import RobustScaler\ntrf = RobustScaler(\n        quantile_range=(25.0,75.0)).fit(<data>)\ntrf.transform(<data>)"
  },
  {
    "objectID": "lectures/9.2-Transformation.html#interdecile-standardisation",
    "href": "lectures/9.2-Transformation.html#interdecile-standardisation",
    "title": "Transformation",
    "section": "Interdecile Standardisation",
    "text": "Interdecile Standardisation\n\\[\n\\dfrac{x_{i}-x_{50^{th}}}{x_{90^{th}}-x_{10^{th}}}\n\\]\nprint(\"You've got this...\")\n\nWhy standardise:\n\nWe understand the properties of normal-ish distributions, and can simulate them easily.\nMore ‘power’ in the statistical tools available.\nMany analyses assume that ‘error’ is random and symmetric (homoscedastic, not skewed)."
  },
  {
    "objectID": "lectures/9.2-Transformation.html#group-standardisation",
    "href": "lectures/9.2-Transformation.html#group-standardisation",
    "title": "Transformation",
    "section": "Group Standardisation",
    "text": "Group Standardisation\n\\[\nx'_{a,i} = \\dfrac{x_{ai}}{\\sum_{g} r_{N,g} P_{a,g}}\n\\]\n\n\\[x_{a,i}\\] = Value of attribute i in area a.\n\\[P_{a,g}\\] = Population of group g in area a.\n\\[r_{N,g}\\] = National ration N of group g\n\\[\\sum\\] = Sum for all groups.\n\\[x'_{a,i}\\] = Standardised value of i in area a."
  },
  {
    "objectID": "lectures/9.2-Transformation.html#normalisation",
    "href": "lectures/9.2-Transformation.html#normalisation",
    "title": "Transformation",
    "section": "Normalisation",
    "text": "Normalisation\n\n(Typically) About rescaling data to a range of [0,1]."
  },
  {
    "objectID": "lectures/9.2-Transformation.html#proportional-normalisation",
    "href": "lectures/9.2-Transformation.html#proportional-normalisation",
    "title": "Transformation",
    "section": "Proportional Normalisation",
    "text": "Proportional Normalisation\n$$\n$$\nimport numpy as np\ndata = <list>\ndata/np.sum(data)"
  },
  {
    "objectID": "lectures/9.2-Transformation.html#range-normalisation",
    "href": "lectures/9.2-Transformation.html#range-normalisation",
    "title": "Transformation",
    "section": "Range Normalisation",
    "text": "Range Normalisation\n$$\n$$\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(data)\nprint(scaler.data_max_)\nscaler.transform(data)\nscaler.transform(<new data>)\n\nNormalisation helps in several ways:\n\nScaling is important for comparability\nClustering is particularly sensitive to scale"
  },
  {
    "objectID": "lectures/9.2-Transformation.html#non-linear-transformations",
    "href": "lectures/9.2-Transformation.html#non-linear-transformations",
    "title": "Transformation",
    "section": "Non-Linear Transformations",
    "text": "Non-Linear Transformations"
  },
  {
    "objectID": "lectures/9.2-Transformation.html#log-transformation",
    "href": "lectures/9.2-Transformation.html#log-transformation",
    "title": "Transformation",
    "section": "Log Transformation",
    "text": "Log Transformation\nRecall: logs are the inverse of exponentiation!\n\nSo if \\[10^{3} = 1,000\\] then \\[log_{10}(1,000) = 3\\].\nAnd if \\[10^{0} = 1\\] then \\[log_{10}(1) = 0\\]\n\nThe Natural Log (\\[e\\]) has certain advantages over other logs.\n\nWhy is this so common? Esp. in social sciences?"
  },
  {
    "objectID": "lectures/9.2-Transformation.html#why-log-transform",
    "href": "lectures/9.2-Transformation.html#why-log-transform",
    "title": "Transformation",
    "section": "Why Log Transform?",
    "text": "Why Log Transform?\nConsider:\n\nThe formula for the mean is \\[\\frac{\\sum{x}}{n}\\].\nThe formula for variance is \\[\\frac{(x-\\bar{x})^{2}}{n}\\].\n\nConsider what happens if \\[(x-\\bar{x})\\] is 10, 100, 1,000, or 10,000?"
  },
  {
    "objectID": "lectures/9.2-Transformation.html#other-transforms",
    "href": "lectures/9.2-Transformation.html#other-transforms",
    "title": "Transformation",
    "section": "Other Transforms…",
    "text": "Other Transforms…\n\nQuantile (maps the PDF of each feature to a uniform distribution)\nSquare Root (often with count data)\nArcsine/Angular (with percentages, proportions, text)\nRank (with care on extreme distributions)\nBox-Cox and Yeo-Johnson (arbitrary power transformations)\n\n\n\nTo report measures of central tendency it’s usually helpful to convert back to the original units.\nThe more extreme the transformation the less meaningful measures of dispersion.\nCorrelation can be significantly affected in either direction.\nCount data can be tricky because you should not have negative values (especially \\[-\\infty\\])."
  },
  {
    "objectID": "lectures/9.2-Transformation.html#when-transforms-dont-help",
    "href": "lectures/9.2-Transformation.html#when-transforms-dont-help",
    "title": "Transformation",
    "section": "When Transforms Don’t Help",
    "text": "When Transforms Don’t Help\nArbitrarily transforming data isn’t a panacea. ‘Robust’ tests can be another approach when all else fails and two common approaches are:\n\nWorking with the ‘Trimmed Mean’: cutting off, say, the top and bottom 5% of scores would start to remove skew and offer a more useful view of the central tendency of the data.\nBootstrapping: by taking many sub-samples (usually of \\[n-1\\] data points or similar) we can build a picture of how certain metrics vary."
  },
  {
    "objectID": "lectures/9.2-Transformation.html#one-last-note",
    "href": "lectures/9.2-Transformation.html#one-last-note",
    "title": "Transformation",
    "section": "One Last Note",
    "text": "One Last Note\n\nThe term normalization is used in many contexts, with distinct, but related, meanings. Basically, normalizing means transforming so as to render normal. When data are seen as vectors, normalizing means transforming the vector so that it has unit norm. When data are though of as random variables, normalizing means transforming to normal distribution. When the data are hypothesized to be normal, normalizing means transforming to unit variance.\n\nSource: Stack Exchange"
  },
  {
    "objectID": "lectures/9.2-Transformation.html#transforming-spatial-data",
    "href": "lectures/9.2-Transformation.html#transforming-spatial-data",
    "title": "Transformation",
    "section": "Transforming Spatial Data",
    "text": "Transforming Spatial Data\n\nAll maps are inaccurate, the question is by how much?"
  },
  {
    "objectID": "lectures/9.2-Transformation.html#whats-wrong-with-this-map",
    "href": "lectures/9.2-Transformation.html#whats-wrong-with-this-map",
    "title": "Transformation",
    "section": "What’s Wrong with this Map?",
    "text": "What’s Wrong with this Map?"
  },
  {
    "objectID": "lectures/9.2-Transformation.html#thats-better",
    "href": "lectures/9.2-Transformation.html#thats-better",
    "title": "Transformation",
    "section": "That’s Better!",
    "text": "That’s Better!"
  },
  {
    "objectID": "lectures/9.2-Transformation.html#whats-a-projection",
    "href": "lectures/9.2-Transformation.html#whats-a-projection",
    "title": "Transformation",
    "section": "What’s a Projection?",
    "text": "What’s a Projection?\n\nSource"
  },
  {
    "objectID": "lectures/9.2-Transformation.html#resources",
    "href": "lectures/9.2-Transformation.html#resources",
    "title": "Transformation",
    "section": "Resources",
    "text": "Resources\n\nNormalisation vs Standardisation – Quantitative analysis\nTransforming Data with R\nData Transformation and Normality Testing\nIntroduction to Logarithms\nWhat is ‘e’ and where does it come from?\nLogarithms - What is e?\nsklearn API reference\nCompare effects of different scalers on data with outliers"
  },
  {
    "objectID": "lectures/9.3-Dimensionality.html#dimensionality-reduction",
    "href": "lectures/9.3-Dimensionality.html#dimensionality-reduction",
    "title": "Dimensionality",
    "section": "Dimensionality Reduction",
    "text": "Dimensionality Reduction"
  },
  {
    "objectID": "lectures/9.3-Dimensionality.html#pca",
    "href": "lectures/9.3-Dimensionality.html#pca",
    "title": "Dimensionality",
    "section": "PCA",
    "text": "PCA\nWorkhorse dimensionality reduction method: simple, fast, and effective. Can be thought of as freely rotating axes to align with directions of maximum variance. I like this summary:\n\nPCA (Principal Components Analysis) gives us our ideal set of features. It creates a set of principal components that are rank ordered by variance (the first component has higher variance than the second, the second has higher variance than the third, and so on), uncorrelated (all components are orthogonal), and low in number (we can throw away the lower ranked components as they contain little signal).\n\nBut I particularly liked this exposition in Towards Data Science."
  },
  {
    "objectID": "lectures/9.3-Dimensionality.html#in-practice",
    "href": "lectures/9.3-Dimensionality.html#in-practice",
    "title": "Dimensionality",
    "section": "In Practice…",
    "text": "In Practice…\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(data)\nprint(pca.explained_variance_ratio_)\nprint(pca.singular_values_)\npca.transform(data)\nSee also: Kernel PCA for non-linear problems."
  },
  {
    "objectID": "lectures/9.3-Dimensionality.html#rtfm",
    "href": "lectures/9.3-Dimensionality.html#rtfm",
    "title": "Dimensionality",
    "section": "RT(F)M",
    "text": "RT(F)M\nWhy was I banging on about transformations?\n\nLinear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.\n\nI found a nice explanation of PCA using dinner conversation over several bottles of wine as an example on Stats.StackExhcange.com. There are many good illustrations of this process on stats.stackexchange.com."
  },
  {
    "objectID": "lectures/9.3-Dimensionality.html#other-considerations",
    "href": "lectures/9.3-Dimensionality.html#other-considerations",
    "title": "Dimensionality",
    "section": "Other Considerations",
    "text": "Other Considerations\n\nPCA is a form of unsupervised learning that does not take output labels into account. Other approaches (such as Linear Discriminant Analysis [note: not Latent Dirichlet Allocation]) consider the output as part of the transformation. PCA is also deterministic.\n\nSee this discussion."
  },
  {
    "objectID": "lectures/9.3-Dimensionality.html#t-sne",
    "href": "lectures/9.3-Dimensionality.html#t-sne",
    "title": "Dimensionality",
    "section": "t-SNE",
    "text": "t-SNE\nt-Distributed Stochastic Neighbour Embedding is best understood as a visualisation technique, not an analytical one. This is because it is probabilistic and not deterministic."
  },
  {
    "objectID": "lectures/9.3-Dimensionality.html#in-practice-1",
    "href": "lectures/9.3-Dimensionality.html#in-practice-1",
    "title": "Dimensionality",
    "section": "In Practice…",
    "text": "In Practice…\nfrom sklearn.manifold import TSNE\nTSNE().fit_transform(<data>)\nHowever, the choice of perplexity and n_iter at instantiation matters. In practice you will need to experiment with both.\nt-SNE is also much harder computationally and it may be preferrable on very high-D data sets to apply PCA first and then t-SNE to the reduced data set! The output could then be fed to a clustering algorithm to make predictions about where new observations belong, but do not confuse that with meaning.\nNote: I have also installed the Multicore-TSNE module in the Docker/Vagrant image."
  },
  {
    "objectID": "lectures/9.3-Dimensionality.html#umap",
    "href": "lectures/9.3-Dimensionality.html#umap",
    "title": "Dimensionality",
    "section": "UMAP",
    "text": "UMAP\nPromising (non-deterministic) alternative to PCA that supports non-linear reduction while preserving both local and global structure. Not currently installed in Docker/Vagrant, but available under umap-learn (conda install -c conda-forge umap-learn). See examples on umap.scikit-tda.org."
  },
  {
    "objectID": "lectures/9.3-Dimensionality.html#gotcha",
    "href": "lectures/9.3-Dimensionality.html#gotcha",
    "title": "Dimensionality",
    "section": "Gotcha!",
    "text": "Gotcha!\nBoth t-SNE and UMAP require very careful handling:\n\nHyperparameters matter\nCluster size means nothing\nCluster distances mean nothing\nClusters may mean nothing (low neighbour count/perplexity)\nOutputs are stochastic (not deterministic)\n\nBoth likely require repeated testing and experimentation."
  },
  {
    "objectID": "lectures/9.3-Dimensionality.html#other-approaches",
    "href": "lectures/9.3-Dimensionality.html#other-approaches",
    "title": "Dimensionality",
    "section": "Other Approaches",
    "text": "Other Approaches\n\nFeature selection, including forwards/backwards (sklearn.feature_selection here)\nDecomposition (sklearn.decomposition here)\nOther types of manifold learning (sklearn.manifold here)\nRandom projection (sklearn.random_projection here)\nSupport Vector Machines (sklearn.svm here)\nEnsemble Methods (such as Random Forests: sklearn.ensemble.ExtraTreesClassifier and sklearn.ensemble.ExtraTreesRegressor here and here)"
  },
  {
    "objectID": "lectures/9.3-Dimensionality.html#resources",
    "href": "lectures/9.3-Dimensionality.html#resources",
    "title": "Dimensionality",
    "section": "Resources",
    "text": "Resources\n\nRethinking ‘distance’ in New York City Medium URL\nFive Boroughs for the 21\\[^{st}\\] Century Medium URL\nCurse of Dimensionality\nThe Curse of Dimensionality\nImportance of Feature Scaling\nUnderstanding PCA\nIntroduction to t-SNE in Python\nHow to Use t-SNE Effectively\nHow to tune the Hyperparameters of t-SNE\nUnderstanding UMAP (Compares to t-SNE)\nHow UMAP Works\n3 New Techniques for Data-Dimensionality Reduction in ML\nUMAP for Dimensionality Reduction (Video)\nA Bluffer’s Guide to Dimensionality Reduction (Video)"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#with-a-title",
    "href": "lectures/2.3-Python_the_Basics.html#with-a-title",
    "title": "Python, the Basics",
    "section": "With a title",
    "text": "With a title\n\nSome blockquote content, really, really, please?\n\nUpdated content with code border?\n\nAt heart, all programming is about:\n\nStoring data (or references to data) in variables,\nFeeding those variables into operations,\nAssigning the results to new variables,\nReporting back."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#section",
    "href": "lectures/2.3-Python_the_Basics.html#section",
    "title": "Python, the Basics",
    "section": "",
    "text": "Message starts as a string:\nmsg = '42'\ntype(msg)"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#section-1",
    "href": "lectures/2.3-Python_the_Basics.html#section-1",
    "title": "Python, the Basics",
    "section": "",
    "text": "But we can change it to an integer like this:\nmsg = '42'\ntype(msg)\nmsg = int(msg)\ntype(msg)"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#section-2",
    "href": "lectures/2.3-Python_the_Basics.html#section-2",
    "title": "Python, the Basics",
    "section": "",
    "text": "And back to a string:\nmsg = '42'\ntype(msg)\nmsg = int(msg)\ntype(msg)\nmsg = str(msg)\ntype(msg)"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#we-can-change-those-types-1",
    "href": "lectures/2.3-Python_the_Basics.html#we-can-change-those-types-1",
    "title": "Python, the Basics",
    "section": "We Can Change Those Types",
    "text": "We Can Change Those Types\nBut we can change it to an integer like this:\nmsg = '42'\ntype(msg) # str\nmsg = int(msg)\ntype(msg) # int"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#we-can-change-those-types-2",
    "href": "lectures/2.3-Python_the_Basics.html#we-can-change-those-types-2",
    "title": "Python, the Basics",
    "section": "We Can Change Those Types",
    "text": "We Can Change Those Types\nAnd back to a string:\nmsg = '42'\ntype(msg) # str\nmsg = int(msg)\ntype(msg) # int\nmsg = str(msg)\ntype(msg) # str"
  },
  {
    "objectID": "lectures/8.1-Data_Space.html#the-data-generating-process",
    "href": "lectures/8.1-Data_Space.html#the-data-generating-process",
    "title": "The Data Space",
    "section": "The Data Generating Process",
    "text": "The Data Generating Process\n\n\nSource: Are Statisticians Cold-Blooded Bosses?"
  },
  {
    "objectID": "lectures/8.1-Data_Space.html#the-data-generating-process-1",
    "href": "lectures/8.1-Data_Space.html#the-data-generating-process-1",
    "title": "The Data Space",
    "section": "The Data Generating Process",
    "text": "The Data Generating Process\n\n\nSource: Are Statisticians Cold-Blooded Bosses?"
  },
  {
    "objectID": "lectures/8.1-Data_Space.html#cashier-income-as-dgp",
    "href": "lectures/8.1-Data_Space.html#cashier-income-as-dgp",
    "title": "The Data Space",
    "section": "Cashier Income as DGP",
    "text": "Cashier Income as DGP\nQuestion: Retail cashier annual salaries have a Normal distribution with a mean equal to $25,000 and a standard deviation equal to $2,000. What is the probability that a randomly selected retail cashier earns more than $27,000?\nAnswer: 15.87%\nResult: All models are wrong, but some are useful (George Box)"
  },
  {
    "objectID": "lectures/8.1-Data_Space.html#house-prices-as-dgp",
    "href": "lectures/8.1-Data_Space.html#house-prices-as-dgp",
    "title": "The Data Space",
    "section": "House Prices as DGP",
    "text": "House Prices as DGP"
  },
  {
    "objectID": "lectures/8.1-Data_Space.html#so",
    "href": "lectures/8.1-Data_Space.html#so",
    "title": "The Data Space",
    "section": "So…",
    "text": "So…\nInstinctively, we know that Bill Gates’ wealth is much further from ‘normal’ than is his height. But how?\n\nHow can we compare income and height if they share no common units?\nHow can we compare the biodiversity of sites in the tropics with those of sub-Arctic areas given that there are different numbers of species to begin with?\n\nWe need ways to make different dimensions comparable, and we need ways to remove unit effects from distance measures."
  },
  {
    "objectID": "lectures/8.1-Data_Space.html#distance-in-1d",
    "href": "lectures/8.1-Data_Space.html#distance-in-1d",
    "title": "The Data Space",
    "section": "Distance in 1D",
    "text": "Distance in 1D\n\\[\nd(i,j) = |(i_{1}-j_{1})|\n\\]"
  },
  {
    "objectID": "lectures/8.1-Data_Space.html#distance-in-2d",
    "href": "lectures/8.1-Data_Space.html#distance-in-2d",
    "title": "The Data Space",
    "section": "Distance in 2D",
    "text": "Distance in 2D\n\\[\nd(i,j) = \\sqrt{(i_{1}-j_{1})^{2}+(i_{2}-j_{2})^{2}}\n\\]"
  },
  {
    "objectID": "lectures/8.1-Data_Space.html#distance-in-3d-or-more",
    "href": "lectures/8.1-Data_Space.html#distance-in-3d-or-more",
    "title": "The Data Space",
    "section": "Distance in 3D… or More",
    "text": "Distance in 3D… or More\nWe can keep adding dimensions…\n\\[\nd(i,j) = \\sqrt{(i_{1}-j_{1})^{2}+(i_{2}-j_{2})^{2}+(i_{3}-j_{3})^{2}}\n\\]\nYou continue adding dimensions indefinitely, but from here on out you are dealing with hyperspaces!"
  },
  {
    "objectID": "lectures/8.1-Data_Space.html#thinking-in-data-space",
    "href": "lectures/8.1-Data_Space.html#thinking-in-data-space",
    "title": "The Data Space",
    "section": "Thinking in Data Space",
    "text": "Thinking in Data Space\nWe can write the coordinates of an observation with 3 attributes (e.g. height, weight, income) as: \\[\nx_{i} = \\\\\\{ {x_{i1}, x_{i2}, x_{i3} } \\\\\\}\n\\]\nAnd we can think of something with 8 attributes (e.g. height, weight, income, age, year of birth, …) as occupying an 8-dimensional space (which, for obvious reasons, we can’t plot in one go)."
  },
  {
    "objectID": "lectures/8.1-Data_Space.html#two-propositions",
    "href": "lectures/8.1-Data_Space.html#two-propositions",
    "title": "The Data Space",
    "section": "Two Propositions",
    "text": "Two Propositions\n\nThat geographical space is no different from any other dimension in a data set.\nThat geographical space is still special when it comes to thinking about relationships."
  },
  {
    "objectID": "lectures/8.1-Data_Space.html#resources",
    "href": "lectures/8.1-Data_Space.html#resources",
    "title": "The Data Space",
    "section": "Resources",
    "text": "Resources"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#general-join-syntax",
    "href": "lectures/9.1-Linking_Data.html#general-join-syntax",
    "title": "Linking Data",
    "section": "General Join Syntax",
    "text": "General Join Syntax\nA join refers to the merging of two (or more) data tables using one (or more) matching columns:\npd.merge(df1, df2, on='SensorID')\n\nNote that if you want to use the index column which isn’t, technically, a column then you need to use left_index=True and right_index=True — where left is the first data set in the join.\nNote that the default behaviour is an inner join (i.e. defaults to how='inner')"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#inner-join",
    "href": "lectures/9.1-Linking_Data.html#inner-join",
    "title": "Linking Data",
    "section": "Inner Join",
    "text": "Inner Join\n\n\nData Set 1\n\n\n\nSensorID\nPlace\nOwner\n\n\n\n\n1 ⇒\nLHR\nBAA\n\n\n2 ✘\nLGA\nGIP\n\n\n3 ⇒\nSTA\nMAG\n\n\n4 ⇒\nLUT\nLuton LA\n\n\n5 ✘\nSEN\nStobart\n\n\n\n\nData Set 2\n\n\n\nSensorID\nParameter\nValue\n\n\n\n\n1 ⇐\nTemperature\n5ºC\n\n\n1 ⇐\nHumidity\n15%\n\n\n3 ⇐\nTemperature\n7ºC\n\n\n4 ⇐\nTemperature\n7ºC\n\n\n6 ✘\nHumidity\n18%"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#section",
    "href": "lectures/9.1-Linking_Data.html#section",
    "title": "Linking Data",
    "section": " ",
    "text": "Data Set 2\n\n\n\nSensorID\nParameter\nValue\n\n\n\n\n1 ⇐\nTemperature\n5ºC\n\n\n1 ⇐\nHumidity\n15%\n\n\n3 ⇐\nTemperature\n7ºC\n\n\n4 ⇐\nTemperature\n7ºC\n\n\n6 ✘\nHumidity\n18%"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#inner-join-result",
    "href": "lectures/9.1-Linking_Data.html#inner-join-result",
    "title": "Linking Data",
    "section": "Inner Join Result",
    "text": "Inner Join Result\nOn an Inner Join all non-matching rows are dropped:\npd.merge(df1, df2, \n         how = 'inner',\n         on  = 'SensorID')\n \n\n\n\nSensorID\nPlace\nOwner\nParameter\nValue\n\n\n\n\n1\nLHR\nBAA\nTemperature\n5ºC\n\n\n1\nLHR\nBAA\nHumidity\n15%\n\n\n3\nSTA\nMAG\nTemperature\n7ºC\n\n\n4\nLUT\nLuton LA\nTemperature\n7ºC"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#but-what-if",
    "href": "lectures/9.1-Linking_Data.html#but-what-if",
    "title": "Linking Data",
    "section": "But What If…",
    "text": "But What If…\nIf Data Set 2 had a SensorKey instead of a SensorID then:\npd.merge(df1, df2, \n         how      = 'inner',\n         left_on  = 'SensorID',\n         right_on = 'SensorKey')\n \nWe will get an ‘extra’ field:\n\n\n\nSensorID\nPlace\nOwner\nSensorKey\nParameter\nValue\n\n\n\n\n1\nLHR\nBAA\n1\nTemperature\n5ºC\n\n\n1\nLHR\nBAA\n1\nHumidity\n15%\n\n\n3\nSTA\nMAG\n3\nTemperature\n7ºC\n\n\n4\nLUT\nLuton LA\n4\nTemperature\n7ºC"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#outer-join",
    "href": "lectures/9.1-Linking_Data.html#outer-join",
    "title": "Linking Data",
    "section": "Outer Join",
    "text": "Outer Join\nOn an Outer Join all rows are retained, including ones with no match:\npd.merge(df1, df2,\n         how = 'outer',\n         on  = 'SensorID')\n \n\n\n\nSensorID\nPlace\nOwner\nParameter\nValue\n\n\n\n\n1\nLHR\nBAA\nTemp.\n5℃\n\n\n1\nLHR\nBAA\nHum.\n15%\n\n\n2\nLGA\nGIP\nNaN\nNaN\n\n\n3\nSTA\nMAG\nTemp.\n7℃\n\n\n4\nLUT\nLuton Borough\nNaN\nNaN\n\n\n5\nSEN\nStobart\nNaN\nNaN\n\n\n6\nNaN\nNaN\nHum.\n20%"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#left-join",
    "href": "lectures/9.1-Linking_Data.html#left-join",
    "title": "Linking Data",
    "section": "Left Join",
    "text": "Left Join\nOn a Left Join all rows on the left table are retained, including ones with no match, but unmatched right rows are dropped:\npd.merge(df1, df2, \n        how = 'left',\n        on  = 'SensorID')\n \n\n\n\nSensorID\nPlace\nOwner\nParameter\nValue\n\n\n\n\n1\nLHR\nBAA\nTemp.\n5℃\n\n\n1\nLHR\nBAA\nHum.\n15%\n\n\n2\nLGA\nGIP\nNaN\nNaN\n\n\n3\nSTA\nMAG\nTemp.\n7℃\n\n\n4\nLUT\nLuton Borough\nNaN\nNULL\n\n\n5\nSEN\nStobart\nNaN\nNaN"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#any-guesses",
    "href": "lectures/9.1-Linking_Data.html#any-guesses",
    "title": "Linking Data",
    "section": "Any Guesses?",
    "text": "Any Guesses?\n\nAny guesses for the fourth type of join?"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#append-concat",
    "href": "lectures/9.1-Linking_Data.html#append-concat",
    "title": "Linking Data",
    "section": "Append & Concat",
    "text": "Append & Concat\nPandas has two additional join-like functions:\n\nAppend: can be used to add a dict, Series, or DataFrame to the ‘bottom’ of an existing df. It’s not advisable to extend a df one row at a time (do bulk concatenations instead).\nConcat: can be used to concatenate two dfs together along either axis (rows or columns) “while performing optional set logic (union or intersection) of the indexes (if any) on the other axes.”"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#concat",
    "href": "lectures/9.1-Linking_Data.html#concat",
    "title": "Linking Data",
    "section": "Concat",
    "text": "Concat\ndf3 = pd.DataFrame.from_dict({\n    'SensorID': [2,3,8,9,10],\n    'Place': ['STA','LUT','BHX','MAN','INV'],\n    'Owner': ['BAA','Luton LA','???','???','???']\n})\npd.concat([df1, df3], ignore_index=True)\nOutputs:\n\n\n\n\nSensorID\nPlace\nOwner\n\n\n\n\n0\n1\nLHR\nBAA\n\n\n\\[\\vdots\\]\n\\[\\vdots\\]\n\\[\\vdots\\]\n\\[\\vdots\\]\n\n\n4\n5\nSEN\nStobart\n\n\n5\n2\nSTA\nBAA\n\n\n6\n3\nLUT\nGIP\n\n\n7\n8\nBHX\n???\n\n\n8\n9\nMAN\n???\n\n\n9\n10\nINV\n???"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#append",
    "href": "lectures/9.1-Linking_Data.html#append",
    "title": "Linking Data",
    "section": "Append",
    "text": "Append\nto_append = [\n    {'SensorID': 5, 'Parameter': 'Humidity', 'Value': 0.45},\n    {'SensorID': 5, 'Parameter': 'Humidity', 'Value': 0.31},\n    {'SensorID': 4, 'Parameter': 'Temperature', 'Value': 2},\n    {'SensorID': 3, 'Parameter': 'Temperature', 'Value': 3}]\ndf2.append(to_append)\nOutputs:\n\n\n\nSensorID\nParameter\nValue\n\n\n\n\n\n0\n1\nTemperature\n5.00\n\n\n\\[\\vdots\\]\n\\[\\vdots\\]\n\\[\\vdots\\]\n\\[\\vdots\\]\n\n\n4\n6\nHumidity\n0.18\n\n\n0\n5\nHumidity\n0.45\n\n\n1\n5\nHumidity\n0.31\n\n\n2\n4\nTemperature\n2.00\n\n\n3\n3\nTemperature\n3.00\n\n\n\n\nNote that a Dictionary-of-Lists would also work for an append and that appending a column that doesn’t exist (for vertical appends) will cause the column to be created while appending a row that doesn’t exist (for horizontal appends) with cause the row to be created."
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#merge-vs.-append-vs.-concat-vs.-join",
    "href": "lectures/9.1-Linking_Data.html#merge-vs.-append-vs.-concat-vs.-join",
    "title": "Linking Data",
    "section": "Merge vs. Append vs. Concat vs. Join?",
    "text": "Merge vs. Append vs. Concat vs. Join?\nAs usual, Stack Overflow to the rescue:\n\nA very high level difference is that merge() is used to combine two (or more) dataframes on the basis of values of common columns (indices can also be used, use left_index=True and/or right_index=True), and concat() is used to append one (or more) dataframes one below the other (or sideways, depending on whether the axis option is set to 0 or 1).\n\n\njoin() is used to merge 2 dataframes on the basis of the index; instead of using merge() with the option left_index=True we can use join().\n\nHint: axis=0 refers to the row index & axis=1 to the column index."
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#merge-vs.-append-vs.-concat-vs.-join-1",
    "href": "lectures/9.1-Linking_Data.html#merge-vs.-append-vs.-concat-vs.-join-1",
    "title": "Linking Data",
    "section": "Merge vs. Append vs. Concat vs. Join?",
    "text": "Merge vs. Append vs. Concat vs. Join?\nThese achieve the same thing, but they are not always equivalent:\npd.merge(df1, df2, left_index=True, right_index=True)\npd.concat([df1, df2], axis=1)\ndf1.join(df2)\nGenerally:\n\nConcat expects the number of columns in all data frames to match (if concatenating vertically) and the number of rows in all data frames to match (if concatenating horizontally). It does not deal well with linking.\nAppend assumes that either the columns or the rows will match.\nJoin is basically a functionality-restricted merge."
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#resources",
    "href": "lectures/9.1-Linking_Data.html#resources",
    "title": "Linking Data",
    "section": "Resources",
    "text": "Resources\n\nPandas Guide to Merges\nDatabase-style joining/merging\nPandas Concat\nPandas Append"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#data-sets-in-isolation-are-of-limited-use-but-the-ability-to-link-data-unlocks-all-kinds-of-new-possibilities",
    "href": "lectures/9.1-Linking_Data.html#data-sets-in-isolation-are-of-limited-use-but-the-ability-to-link-data-unlocks-all-kinds-of-new-possibilities",
    "title": "Linking Data",
    "section": "> Data sets in isolation are of limited use, but the ability to link data unlocks all kinds of new possibilities!",
    "text": "> Data sets in isolation are of limited use, but the ability to link data unlocks all kinds of new possibilities!"
  },
  {
    "objectID": "sessions/reading_week.html",
    "href": "sessions/reading_week.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "Changes before 22/23\n\n\n\n\n\nAdd Learning Outcomes for each week\n\nHighlight relevance of readings to Assessment 1 and overall Learning Outcomes.\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis week’s learning objectives are:\n\nGet caught up on any rmissed eading(s).\nStart to think about the research question(s) for your Policy Briefing.\nStart to think about how you might ask this question(s) of the data.\n\n\n\nYou should be doing the readings that will support your answers to Assessment #2. And, looking ahead to the Final Assessment and where you might find ideas or literature to support your thinking, I’d suggest having a browse of the Full Bibliography. This is a working document and I will add more items as and when I come across them or new works are published!\n\n\n\nIn addition to looking for relevant content in (D’Ignazio and Klein 2020) (URL), you will also want to check consider:\n\n(Elwood and Wilson 2017) DOI\n(Elwood and Leszczynski 2018) DOI\n(Bemt et al. 2018) DOI\n(Amoore 2019) DOI\n(Crawford and Finn 2015) DOI\n\n\n\n\n\nAmoore, L. 2019. “Doubt and the Algorithm: On the Partial Accounts of Machine Learning.” Theory, Culture, Society 36 (6):147–69. https://doi.org/https://doi.org/10.1177%2F0263276419851846.\n\n\nBemt, V. van den, J. Doornbos, L. Meijering, M. Plegt, and N. Theunissen. 2018. “Teaching Ethics When Working with Geocoded Data: A Novel Experiential Learning Approach.” Journal of Geography in Higher Education 42 (2):293–310. https://doi.org/https://doi.org/10.1080/03098265.2018.1436534.\n\n\nCrawford, K., and M. Finn. 2015. “The Limits of Crisis Data: Analytical and Ethical Challenges of Using Social and Mobile Data to Understand Disasters.” GeoJournal 80 (4):491–502. https://doi.org/https://doi.org/10.1007/s10708-014-9597-z.\n\n\nD’Ignazio, Catherine, and Lauren F. Klein. 2020. Data Feminism. MIT Press. https://bookbook.pubpub.org/data-feminism.\n\n\nElwood, S., and A. Leszczynski. 2018. “Feminist Digital Geographies.” Gender, Place and Culture 25 (5):629–44. https://doi.org/https://doi.org/10.1080/0966369X.2018.1465396.\n\n\nElwood, S., and M. Wilson. 2017. “Critical GIS Pedagogies Beyond ‘Week 10: Ethics.” International Journal of Geographical Information Science 31 (10):2098–2116. https://doi.org/https://doi.org/10.1080/13658816.2017.1334892."
  },
  {
    "objectID": "assessments/index.html",
    "href": "assessments/index.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "The overall assessment package is intended to test students’ comprehension of, and ability to integrate, technical skills with a broader understanding of, and reflection upon, computational approaches to urban research and spatial data science.\n\n\nThe assessments are grounded in a mixture of critical reflection and group work that map on to real-world data science challenges, including:\n\nExamining a data set to determine its suitability for tackling a selected analytical ‘problem’ or ‘challenge’;\nCollaboratively writing a data-led policy briefing involving high-quality code, analysis, and presentation (both writing for an audience and data visualisation) suitable for use by a policy- or decision-maker;\nReflecting on the process to better-understand why a project succeeded/failed so as to improve future outcomes.\n\n\n\n\nCollectively, these assessments seek to provide multiple opportunities to ‘shine’ both individually and as part of a group. You do not need to be the best programmer in the class in order to do well on these assessments. Indeed, focussing only on the programming is likely to result in a low mark because it missed the context in which data science and data analysis ‘work’. As a budding data scientist/analyst your job is just as much to understand your audience and their needs: you will work with clients who can’t really articulate what they want or why, so good project management often involves putting yourself in your client’s shoes and working out how to translate what they say they want into what they actually need.\nYou will therefore do poorly on the assessments if you do not do the readings, watch the pre-recorded lectures, or participate in discussions (both online and in-person during practicals and classes). These provide you with context for the work that is being done when you start typing and running code in a Jupyter Notebook. Code is the how. Context is the why."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#a-bit-of-perspective",
    "href": "lectures/1.1-Getting_Oriented.html#a-bit-of-perspective",
    "title": "Getting Oriented",
    "section": "A Bit of Perspective",
    "text": "A Bit of Perspective\n\n\nStudents face the risks of the de-skilling of geography and planning at one end, and being subsumed by data science at the other…"
  },
  {
    "objectID": "help.html",
    "href": "help.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "We all need help from time to time—and that’s what we’re here to provide—but the best way to ‘get help’ will always be taking steps to ‘help yourself’ first!\n\n\nWhen you are first learning to code there is no such thing as a stupid question. From time to time we all have lazy questions, which is what happens when we are frustrated and just want to know ‘the answer’ without putting in the work to clarify the problem. However, if any time you find yourself stuck on a particular problem there is a 100% chance that someone else in the class is having the same problem as well but hasn’t quite worked up the courage to ask. So please: ask.\n\n\n\nHere are four things that you can do to ‘help yourself’ first:\n\nUse Google–this is one course where saying “I googled it…” will be taken as a good sign! Probably the biggest difference between a good programmer and a new programmer is that the good one knows which terms to type into Google to get the answer that they need right away.\nUse Stack Overflow–as you become a better programmer you’ll start to understand how to frame your question in ways that produce the answer you need in the first couple of search results, but whether you’re a beginner or an expert Stack Overflow is your friend. True story: I have sometimes found answers that I provided (but didn’t remember giving) when trying to solve a problem.\nUse the work of others–beyond the readings that we’ve assigned to support the module’s learning outcomes there is a world of knowledge out there on which you can build! Use Google Scholar, Medium, and dedicated tutorial-type sites like Towards Data Science and Programming Historian to see if you can find others who have had similar challenges.\nUse the dedicated Slack channel–this provides a much richer experience than the Moodle Forum and should be your primary means of requesting help outside of scheduled teaching hours because you can get answers from other staff, other students, and the wider CASA community.\n\nYes, this is a lot of things to do when you want to know the answer to what feels like a simple question, but it’s an investment. If we just ‘give’ you the answer then chances are you’ll forget it as soon as your code starts running again; hwoever, if you’ve had to invest your time and energy in sorting through a whole range of answers (some useful, some not) then you have found it for yourself in a way that you’ll not soon forget. In fact, you’ve learned something about both how to frame questions and how to identify useful answers. That, frankly, is a much more valuable skill!\n\n\n\nLearning to code is like learning a language: you need to practice! Set yourself little problems or tasks and see if you can apply what you’ve learned in class to a problem in a different class, or to a friend’s problem, or just something you’re curious about! In the same way that practicing your Chinese or French with native speakers will help you to learn those languages, so will practicing your Python.\n\n\n\nIf you’ve gone through steps 1–4 and don’t feel any closer to a solution then it’s time to get our input! So the final steps:\n\nMake use of ‘Office Hours’–if you are struggling, then tell us! We can’t help you if we don’t know that you’re lost! That doesn’t mean that we can simply ‘give’ you the answers to challenging questions, but we will do everything that we can to support you in finding and understanding the answers.\nEmail us–Slack will usually be faster, but for personal questions or ones you’re just not comfortable asking in public, then email away!\nSign up for online classes–realistically, you will have a lot on your plate this year, but if you want or need more practice with Python then there is a wealth of options out there for ‘further study’ and ‘further practice’. Perhaps you’ll find a resource that speaks to you in a way that our module doesn’t!\n\n\n\n\nHowever tired you are, don’t send a stream of consciousness late-night email saying little more than “Hey, I’m stuck on this problem can you solve it?” Go to bed. Sleep on it. And if you’re still stuck in the morning it’s time for the email.\nWhat does a useful email look like? You might want to follow this overview of how to get a busy person to respond to your email:\n\nKeep it short.\nFormat it for readability and clarity.\nMake it clear what you want me to do.\nBe reasonable with your request.\nShow me why I should take the time to help you.\n\nThat last point isn’t quite as rude as it sounds! If you’ve gone through steps 1–4 above, then it’s actually easy to explain what you’ve done, what you’ve found, why you think things aren’t working, and whether you have any ideas for solving your issue! If you’ve done all this then your question will never be a result of laziness, so that suggests there’s something for us to learn about how we teach!\nIn academia there are a few more things I’d add:\n\nWhat module are you emailing me about?\nWhat is your student id? (Especially if it’s about Extenuating Circumstances or an Assessment)\nIs the question about a specific practical, lecture, or technical problem?\n\nAnd here is some additional insight into how to email your professor (without being annoying AF):\n\nSalutation: should I use “Dear”, “Hello” or “Hi”?\nHonourific: should I use “Mr”, “Ms”, “Dr” or “Professor”? Hint: don’t ever use Mr/Mrs/Ms.\nName: for god’s sake, please try not to get this wrong.\nExceptions: always look at how your Prof or TA responds to you for cues about how to respond to them.\nBe nice: treat your TA and Prof like human beings please!\nRemind me who you are: anything that allows me to place you and your question in context will help me to give you effective help.\nThe reason: tell me as precisely as possible why you are emailing me and what you want/hope to achieve by doing so.\nDo the legwork (this is the ‘show me why I should take the time to help’ thing above): if the answer is in the Syllabus or the recorded content then I may not answer your question. Or I may just write “It’s in the syllabus” and leave it at that. Show me that you’ve tried to answer your question yourself and give me a sense of what you’ve already tried. For instance, if your problem is technical then “I couldn’t install the software and it didn’t work” tells me nothing about your actual problem (see also information on asking a good tech question).\nWrap-up and Sign-off: is there a deadline (e.g. for a recommendation) or some other issue that I need to factor into my plans? Some recognition of thanks never goes amiss.\n\n\n\nIf we don’t reply to you then send a reminder! The trick is to send the reminder at the right time: if you are about to fail an assessment or are three weeks into the course and still can’t run the programming environment then your problem is urgent and you can send a reminder much sooner than if you’re wondering ‘if X would be a good topic for the final assessment’. As the Medium blogger puts it: “If it can wait a week, let it wait a week” (before following up).\n\n\n\nWe all have different ‘registers’ for speaking with other people: family (siblings vs. grandparents), friends (close friends vs. acquaintances), and so on. A professionally-written email is a vital work skill since most of the people that you will end up working for will use email over all the other channels available now. Sending an email like this will not get you the help you need:\n\nHey Bossman,\nAre U awake at 2am like me? LOL. Having problems and not going to finish the report in time for meeting tomorrow. Can you give me an extension?\nLater.\n\nIf you’re worried that you might not be hitting the right ‘tone’ then it’s ok to say so and to ask if you could write it better/differently in the future. This shows reflection and thought, which is all that we really want from our students anyway!\n\n\n\n\nFinally, we want you to think about how to learn what you need to learn. This is called meta-cognition—thinking about thinking—and it’s probably one of the most important study skills of all. So here you need to think about what you want from, for example, an assessment: do you just want to pass, or do you want to hit it out of the park?\nSome questions to ask yourself:\n\nFor an exam: What kinds of questions might it include?\nFor an essay/written piece: What kinds of topics have we covered in class?\nFor either: identify key terms, define those terms, and question the question (what might be the assumptions behind it?).\nWhich of resources would help me to study/prepare?\nHow should I make use of lecture notes, practice exam questions, textbook and other readings, instructor office hours, peer discussions, and tutoring. Write down why each resource would be useful and how you could use it to map out a study/writing plan.\nHow does my essay work as a story? I don’t necessarily mean a mystery or adventure novel, but a good essay has: a plot (what is sometimes called a narrative arc), characters (the key ideas from the literature, the data, the methods, the problems…), development (which character needs to be introduced first?), and a conclusion (what happened?). Have a look at the Tim Squirrel Guide where it talks about essay stucture.\nLooking at this literature/piece of research, is this the way that I would have done it? If I would have done it differently do I think that would have been more, or less, effective? Why or why not?\nLooking at my peers, is there someone who is doing really well who I could talk to about how they study, code, or write? How do they organise their time? How do they make use of the resources (literature, etc.)?\n\nIf all you want is a pass then organise yourself for a solid pass. If you want to excel then you’re going to have to do things a bit differently. There’s space for both approaches, but being clear about which you want will help you to prioritise the important stuff."
  },
  {
    "objectID": "attention.html",
    "href": "attention.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "Learning to code is hard work and I’m pretty sure that I’m a lot less exciting than your average YouTube star. So if I have to compete with YouTube (or with your friends on WhatsApp, Snapchat, WeChat, or whatever) for your attention then I’ll probably lose. And your computer and phone are designed to distract you because they are interested in engagement—they don’t care that you should be engaging with your studies, they want you to pick them up and use them. This is why you get so many notifications!\nThere is plenty of evidence to back this up:\n\nDistractions make learning harder\nNearly half of students distracted by technology\nThe effect of cellphones on attention and learning…\nDistractions make retaining info harder\nThe interrupted learner: how distractions during live and video lectures influence learning outcomes\n\nSo my recommendations for studying (online or offline) are:\n\nTurn off as many notifications as possible. On the Mac there is a ‘Do not disturb’ setting that you can enable in the Notifications section of your System Preferences. You can set it to turn on by time, and also add a setting short-cut to your ‘Notification Centre’ (upper-right corner of your desktop).\nUninstall messaging apps that you cannot turn off. If quitting the app disables notifications then that’s fine. But if you can’t disable notifications then I’d suggest uninstalling the messaging app entirely. Facebook Messenger, for instance, is a massive memory-hog and usually available on the desktop UI anyway.\nBlock access to distracting web sites. There are a number of tools that you can set up to block access to Facebook and other social media sites at set times of day. My Achilles heel is the news: if I’m struggling I tend to read news articles, so if I really need to make sure I don’t have the browser open to the home page of my favourite news sites.\nWork out a schedule. It’s a lot easier to avoid distractions if you have a routine that enables you to say “OK, I will work from 9–11 and then have a look at my email.” It is easier to be in control if you can give yourself rewards later. And if you get into a routine, as a parent probably suggested when you were an undergraduate (certainly mine did and I ignored them), then you’ll find that your ‘productivity’ improves dramatically. My mother was right, dammit.\n\n\n\n\nFrequent distraction is one problem, but (in a sense) getting enough distraction is another. As this piece suggests, it’s also important that you give yourself downtime between Zoom/Teams/whatever sessions. I will try to remember to bake these into our ‘lectures’, but you should also suggest breaks if you feel yourself flagging! This also applies, however, to your wider degree: Masters degrees are intense and you need to give yourself permission for a timeout… stepping away from the computer and going for a walk, doing some knitting, taking a weekend to visit another city or go on a camping trip, whatever floats your boat!\n\n\n\nAlthough the guidance below, from Tim Squirrel’s Guide is for undergraduate essays, and is about reading so that you can write an essay in history or philosophy, the advice works for all kinds of reading.\nMake sure that you cite the people who originally came up with the ideas you’re presenting, and ideally think about how you would differ from or improve upon or disagree with them. The only way you can do this (and consequently, the only way to get a decent mark) is to do some reading.\n\nLook at the reading list. If it’s incredibly long, you probably won’t want to (or be able) to read it all. However, that is not an excuse to not read any of it. Look through the list, identify if there are any readings marked as essential. Read them. If there aren’t any essential readings, pick a few which look interesting and relevant, then read them.\nRead some more. If the reading list is really short, you’ll need to go beyond it. If it’s long, this is still relevant. Look through the reference lists of the papers and books you’ve just read. See where their ideas came from. Mark out a few of the most promising-looking readings. Read them.\nThere is a difference between reading to understand the topic, and reading to reference. It is totally fine to use Wikipedia, lecture notes, etc to familiarise yourself with the key arguments and concepts. It is considerably less fine to cite them as your only sources.\nBooks. For undergraduate and masters study do not read whole books. It’s a waste of your time. You won’t remember any of it, it will drain all of your energy, and you only get one reference and viewpoint out of it. Read the intro and conclusion so that you get the gist of their argument. Pick a chapter from the contents page which looks like it’s relevant to your essay. Read that. As above, find relevant references and follow them up.\nArticles. Read the abstract first. Does it look like it’s relevant? If not, don’t waste your time. If it does, read it. Check the bibliography as above.\nRead critically. For the sake of all that is holy, read critically. This is absolutely essential. Don’t just stare at the pages and absorb them, bovine-like, for the purposes of regurgitation into your essay. Think about:\n\nThe central claim the author is making. Usually there is only one, perhaps two. Summarise it in one sentence if you can.\nWhat is the frame of their argument? When in history is it set? Who are the key actors? Are they responding to another author? If so, what is the argument they’re responding to? Try to position their argument in context. This allows you to:\nCritically assess the claims made. This obviously doesn’t just mean ‘say they’re wrong’. They might well be wrong, but you’ll need to find reasons for it. Generate a list of three reasons for each line of attack you want to take. Scrap the weakest two. If you think they’re right, why are they right? Are there other authors who corroborate their claims? Are there logical reasons to prefer their argument?\n\n\nMake sure you take notes on everything you read. Put page numbers in those notes. In fact, write down any potentially useful (and, ideally, flexible) quotes verbatim with the page numbers. Your goal is to read everything once even if you reference it repeatedly.\n\n\n\nAnother challenge for many students is that they want the ‘right’ answer to how to do things. There’s more academic literature on this, but for a thought-provoking look at why it might be a good idea for us to make your life hard then What IKEA and Our Education System Have in Common is an easy read.\nHere’s the summary:\n\nIKEA is easy, but you don’t learn anything.\nIKEA is about getting things done/finishing.\nIKEA is convenient, but it’s not creative.\nIKEA is standardised, but it is actually primed for hacking.\n\nBut there is a great response to the original post that adds nuance to this:\n\nThe thing to keep in mind, though, is that following recipies is how we learn skills to start with. If you were teaching someone to cook, for example, you wouldn’t throw the person into the kitchen and tell them to be creative with the food. Instead, you would teach them to follow recipies so that they can practice skills, and learn how to properly balance flavours and textures. Then, when they’ve mastered some recipies, you teach them how to mix up and re-combine recipies, and eventually come up with their own.\n\n\n\n\nFor the record, there are many things that can’t be solved by code or coders, but there are many things that can be tackled by learning to think like a programmer. This can include:\n\nUnderstanding: make sure that you actually understand the problem before you try to solve it. Try to explain it to someone else. Try to explain it to a pet or stuffed animal. Write it down. You may feel silly doing some of these things, but saying it/writing it forces you to organise your thoughts and often gives you the answer halfway through the explanation.\nPlanning: “Given X, what steps do I need to achieve Y?” You can start with comments, bullet points, or whatever format helps you to get the skeleton of an answer in place before you spend ages writing the first few lines of code or the first few lines of your essay. Markdown is good here!\nDividing: never try to solve a hard problem in one go. Break it down into little steps. Easy steps. Do the easiest one first (if you can separate it out from the first one). Check your solution works for that part. Take the next step. Check the two steps work together. Build from there. As programmers and Venture Capitalists would tell you: iterate! Same for an essay or written submission.\nUnsticking: try to stay curious rather than getting angry or frustrated. Debugging is a step-by-step process: comment things out, add print statements, break it back down again into the basics and gradually re-add pieces until you can see where it all breaks.\nPracticing: I like this quote “Practice. Practice. Practice. It’ll only be a matter of time before you recognize that ‘this problem could easily be solved with .’”\n\n\n“The art of debugging is figuring out what you really told your program to do rather than what you thought you told it to do.” — Andrew Singer\n\nOn a more practical level, here are common mistakes made by new programmers.\n\n\n\n\nWe will not give you the answer. This will be frustrating and annoying (particularly when you are already frustrated that something is not working) but by asking you questions we will try to teach you to solve problems for yourself.\nIf you keep asking us the same questions you will get increasingly abrupt answers. You should be scared to ask us the same question for the 20th time. You should not be scared to admit that you’re struggling with something.\nSometimes there is no answer! Every year the data changes. The policies and issues change. I teach things a little differently. This is much more like the real world and sometimes I don’t know what we’ll find when we start coding.\nYou need to work out how you learn best.\nYou will need to think critically about what you are doing. There’s a good article on How to write better essays which leads on to a blog on how to write better undergraduate essays (see above). You’re obviously not undergraduates any more and we expect more of you, but as a starting point this is a good one, especially for those of you who are new to the UK way of teaching and learning."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#principles",
    "href": "lectures/1.2-Tools_of_the_Trade.html#principles",
    "title": "Tools of the Trade",
    "section": "Principles",
    "text": "Principles\n\nSoftware should be free (as far as practicable).\nSoftware should be open (as far as practicable).\nSoftware should run on all platforms.\nSoftware should reflect what you will encounter in the ‘real world’."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#docker",
    "href": "lectures/1.2-Tools_of_the_Trade.html#docker",
    "title": "Tools of the Trade",
    "section": "Docker",
    "text": "Docker\n\nDocker “makes development efficient and predictable” because it is “used through the development lifecycle for fast, easy and portable application development”."
  },
  {
    "objectID": "lectures/8.2-Transformation.html#transformation-in-1d",
    "href": "lectures/8.2-Transformation.html#transformation-in-1d",
    "title": "Transformation",
    "section": "Transformation in 1D",
    "text": "Transformation in 1D"
  },
  {
    "objectID": "lectures/8.2-Transformation.html#transformation-in-1d-1",
    "href": "lectures/8.2-Transformation.html#transformation-in-1d-1",
    "title": "Transformation",
    "section": "Transformation in 1D",
    "text": "Transformation in 1D\n\\[\nx-\\bar{x}\n\\]\n\n\nHow is this any different?"
  },
  {
    "objectID": "lectures/8.2-Transformation.html#so",
    "href": "lectures/8.2-Transformation.html#so",
    "title": "Transformation",
    "section": "So…",
    "text": "So…\n\nTransformations are mathematical operations applied to every observation in a data set that preserve some of the relationships between them."
  },
  {
    "objectID": "lectures/8.2-Transformation.html#for-example",
    "href": "lectures/8.2-Transformation.html#for-example",
    "title": "Transformation",
    "section": "For Example",
    "text": "For Example\nIf we subtract the mean from everyone’s height then we can immediately tell if someone is taller or shorter than we would expect.\nIf we subtract the mean from everyone’s income then we cannot immediately tell if someone is earning more or less that we would expect.\nSo what is a useful transformation in one context, may not be in another!"
  },
  {
    "objectID": "lectures/8.2-Transformation.html#fleshing-this-out",
    "href": "lectures/8.2-Transformation.html#fleshing-this-out",
    "title": "Transformation",
    "section": "Fleshing This Out",
    "text": "Fleshing This Out\nQuestion: How can you tell if you did better than everyone else on the Quiz or on the Data Set Biography?\nAnswer: Just subtracting the mean is not enough because the distributions are not the same. For that we also need to standardise the data in some way.\n\\[\nz = \\dfrac{x-\\bar{x}}{\\sigma}\n\\]\nDivide through by the distribution!"
  },
  {
    "objectID": "lectures/8.2-Transformation.html#z-score-standardisation",
    "href": "lectures/8.2-Transformation.html#z-score-standardisation",
    "title": "Transformation",
    "section": "Z-Score Standardisation",
    "text": "Z-Score Standardisation\n\\[\n\\dfrac{x-\\bar{x}}{\\sigma}\n\\]\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(data)\nprint(scaler.mean_)\nscaler.transform(data)\nscaler.transform(<new data>)\n\nThe important thing to note is that if transform data that has not been fit and you get values outside the range used for fitting then you can no longer assume a standard normal."
  },
  {
    "objectID": "lectures/8.2-Transformation.html#interquartile-standardisation",
    "href": "lectures/8.2-Transformation.html#interquartile-standardisation",
    "title": "Transformation",
    "section": "Interquartile Standardisation",
    "text": "Interquartile Standardisation\n\\[\n\\dfrac{x_{i}-x_{Q2}}{x_{Q3}-x_{Q1}}\n\\]\nfrom sklearn.preprocessing import RobustScaler\ntrf = RobustScaler(\n        quantile_range=(25.0,75.0)).fit(<data>)\ntrf.transform(<data>)"
  },
  {
    "objectID": "lectures/8.2-Transformation.html#interdecile-standardisation",
    "href": "lectures/8.2-Transformation.html#interdecile-standardisation",
    "title": "Transformation",
    "section": "Interdecile Standardisation",
    "text": "Interdecile Standardisation\n\\[\n\\dfrac{x_{i}-x_{50^{th}}}{x_{90^{th}}-x_{10^{th}}}\n\\]\nprint(\"You've got this...\")\n\nWhy standardise:\n\nWe understand the properties of normal-ish distributions, and can simulate them easily.\nMore ‘power’ in the statistical tools available.\nMany analyses assume that ‘error’ is random and symmetric (homoscedastic, not skewed)."
  },
  {
    "objectID": "lectures/8.2-Transformation.html#group-standardisation",
    "href": "lectures/8.2-Transformation.html#group-standardisation",
    "title": "Transformation",
    "section": "Group Standardisation",
    "text": "Group Standardisation\n\\[\nx'_{a,i} = \\dfrac{x_{ai}}{\\sum_{g} r_{N,g} P_{a,g}}\n\\]\n\n\\[x_{a,i}\\] = Value of attribute i in area a.\n\\[P_{a,g}\\] = Population of group g in area a.\n\\[r_{N,g}\\] = National ration N of group g\n\\[\\sum\\] = Sum for all groups.\n\\[x'_{a,i}\\] = Standardised value of i in area a."
  },
  {
    "objectID": "lectures/8.2-Transformation.html#proportional-normalisation",
    "href": "lectures/8.2-Transformation.html#proportional-normalisation",
    "title": "Transformation",
    "section": "Proportional Normalisation",
    "text": "Proportional Normalisation\n$$\n$$\nimport numpy as np\ndata = <list>\ndata/np.sum(data)"
  },
  {
    "objectID": "lectures/8.2-Transformation.html#range-normalisation",
    "href": "lectures/8.2-Transformation.html#range-normalisation",
    "title": "Transformation",
    "section": "Range Normalisation",
    "text": "Range Normalisation\n$$\n$$\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(data)\nprint(scaler.data_max_)\nscaler.transform(data)\nscaler.transform(<new data>)\n\nNormalisation helps in several ways:\n\nScaling is important for comparability\nClustering is particularly sensitive to scale"
  },
  {
    "objectID": "lectures/8.2-Transformation.html#log-transformation",
    "href": "lectures/8.2-Transformation.html#log-transformation",
    "title": "Transformation",
    "section": "Log Transformation",
    "text": "Log Transformation\nRecall: logs are the inverse of exponentiation!\n\nSo if \\[10^{3} = 1,000\\] then \\[log_{10}(1,000) = 3\\].\nAnd if \\[10^{0} = 1\\] then \\[log_{10}(1) = 0\\]\n\nThe Natural Log (\\[e\\]) has certain advantages over other logs.\n\nWhy is this so common? Esp. in social sciences?"
  },
  {
    "objectID": "lectures/8.2-Transformation.html#why-log-transform",
    "href": "lectures/8.2-Transformation.html#why-log-transform",
    "title": "Transformation",
    "section": "Why Log Transform?",
    "text": "Why Log Transform?\nConsider:\n\nThe formula for the mean is \\[\\frac{\\sum{x}}{n}\\].\nThe formula for variance is \\[\\frac{(x-\\bar{x})^{2}}{n}\\].\n\nConsider what happens if \\[(x-\\bar{x})\\] is 10, 100, 1,000, or 10,000?"
  },
  {
    "objectID": "lectures/8.2-Transformation.html#other-transforms",
    "href": "lectures/8.2-Transformation.html#other-transforms",
    "title": "Transformation",
    "section": "Other Transforms…",
    "text": "Other Transforms…\n\nQuantile (maps the PDF of each feature to a uniform distribution)\nSquare Root (often with count data)\nArcsine/Angular (with percentages, proportions, text)\nRank (with care on extreme distributions)\nBox-Cox and Yeo-Johnson (arbitrary power transformations)\n\n\n\nTo report measures of central tendency it’s usually helpful to convert back to the original units.\nThe more extreme the transformation the less meaningful measures of dispersion.\nCorrelation can be significantly affected in either direction.\nCount data can be tricky because you should not have negative values (especially \\[-\\infty\\])."
  },
  {
    "objectID": "lectures/8.2-Transformation.html#when-transforms-dont-help",
    "href": "lectures/8.2-Transformation.html#when-transforms-dont-help",
    "title": "Transformation",
    "section": "When Transforms Don’t Help",
    "text": "When Transforms Don’t Help\nArbitrarily transforming data isn’t a panacea. ‘Robust’ tests can be another approach when all else fails and two common approaches are:\n\nWorking with the ‘Trimmed Mean’: cutting off, say, the top and bottom 5% of scores would start to remove skew and offer a more useful view of the central tendency of the data.\nBootstrapping: by taking many sub-samples (usually of \\[n-1\\] data points or similar) we can build a picture of how certain metrics vary."
  },
  {
    "objectID": "lectures/8.2-Transformation.html#one-last-note",
    "href": "lectures/8.2-Transformation.html#one-last-note",
    "title": "Transformation",
    "section": "One Last Note",
    "text": "One Last Note\n\nThe term normalization is used in many contexts, with distinct, but related, meanings. Basically, normalizing means transforming so as to render normal. When data are seen as vectors, normalizing means transforming the vector so that it has unit norm. When data are though of as random variables, normalizing means transforming to normal distribution. When the data are hypothesized to be normal, normalizing means transforming to unit variance.\n\nSource: Stack Exchange"
  },
  {
    "objectID": "lectures/8.2-Transformation.html#whats-wrong-with-this-map",
    "href": "lectures/8.2-Transformation.html#whats-wrong-with-this-map",
    "title": "Transformation",
    "section": "What’s Wrong with this Map?",
    "text": "What’s Wrong with this Map?"
  },
  {
    "objectID": "lectures/8.2-Transformation.html#thats-better",
    "href": "lectures/8.2-Transformation.html#thats-better",
    "title": "Transformation",
    "section": "That’s Better!",
    "text": "That’s Better!"
  },
  {
    "objectID": "lectures/8.2-Transformation.html#whats-a-projection",
    "href": "lectures/8.2-Transformation.html#whats-a-projection",
    "title": "Transformation",
    "section": "What’s a Projection?",
    "text": "What’s a Projection?\n\nSource"
  },
  {
    "objectID": "lectures/8.2-Transformation.html#resources",
    "href": "lectures/8.2-Transformation.html#resources",
    "title": "Transformation",
    "section": "Resources",
    "text": "Resources\n\nNormalisation vs Standardisation – Quantitative analysis\nTransforming Data with R\nData Transformation and Normality Testing\nIntroduction to Logarithms\nWhat is ‘e’ and where does it come from?\nLogarithms - What is e?\nsklearn API reference\nCompare effects of different scalers on data with outliers"
  },
  {
    "objectID": "lectures/8.3-Dimensionality.html#pca",
    "href": "lectures/8.3-Dimensionality.html#pca",
    "title": "Dimensionality",
    "section": "PCA",
    "text": "PCA\nWorkhorse dimensionality reduction method: simple, fast, and effective. Can be thought of as freely rotating axes to align with directions of maximum variance. I like this summary:\n\nPCA (Principal Components Analysis) gives us our ideal set of features. It creates a set of principal components that are rank ordered by variance (the first component has higher variance than the second, the second has higher variance than the third, and so on), uncorrelated (all components are orthogonal), and low in number (we can throw away the lower ranked components as they contain little signal).\n\nBut I particularly liked this exposition in Towards Data Science."
  },
  {
    "objectID": "lectures/8.3-Dimensionality.html#in-practice",
    "href": "lectures/8.3-Dimensionality.html#in-practice",
    "title": "Dimensionality",
    "section": "In Practice…",
    "text": "In Practice…\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(data)\nprint(pca.explained_variance_ratio_)\nprint(pca.singular_values_)\npca.transform(data)\nSee also: Kernel PCA for non-linear problems."
  },
  {
    "objectID": "lectures/8.3-Dimensionality.html#rtfm",
    "href": "lectures/8.3-Dimensionality.html#rtfm",
    "title": "Dimensionality",
    "section": "RT(F)M",
    "text": "RT(F)M\nWhy was I banging on about transformations?\n\nLinear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.\n\nI found a nice explanation of PCA using dinner conversation over several bottles of wine as an example on Stats.StackExhcange.com. There are many good illustrations of this process on stats.stackexchange.com."
  },
  {
    "objectID": "lectures/8.3-Dimensionality.html#other-considerations",
    "href": "lectures/8.3-Dimensionality.html#other-considerations",
    "title": "Dimensionality",
    "section": "Other Considerations",
    "text": "Other Considerations\n\nPCA is a form of unsupervised learning that does not take output labels into account. Other approaches (such as Linear Discriminant Analysis [note: not Latent Dirichlet Allocation]) consider the output as part of the transformation. PCA is also deterministic.\n\nSee this discussion."
  },
  {
    "objectID": "lectures/8.3-Dimensionality.html#t-sne",
    "href": "lectures/8.3-Dimensionality.html#t-sne",
    "title": "Dimensionality",
    "section": "t-SNE",
    "text": "t-SNE\nt-Distributed Stochastic Neighbour Embedding is best understood as a visualisation technique, not an analytical one. This is because it is probabilistic and not deterministic."
  },
  {
    "objectID": "lectures/8.3-Dimensionality.html#in-practice-1",
    "href": "lectures/8.3-Dimensionality.html#in-practice-1",
    "title": "Dimensionality",
    "section": "In Practice…",
    "text": "In Practice…\nfrom sklearn.manifold import TSNE\nTSNE().fit_transform(<data>)\nHowever, the choice of perplexity and n_iter at instantiation matters. In practice you will need to experiment with both.\nt-SNE is also much harder computationally and it may be preferrable on very high-D data sets to apply PCA first and then t-SNE to the reduced data set! The output could then be fed to a clustering algorithm to make predictions about where new observations belong, but do not confuse that with meaning.\nNote: I have also installed the Multicore-TSNE module in the Docker/Vagrant image."
  },
  {
    "objectID": "lectures/8.3-Dimensionality.html#umap",
    "href": "lectures/8.3-Dimensionality.html#umap",
    "title": "Dimensionality",
    "section": "UMAP",
    "text": "UMAP\nPromising (non-deterministic) alternative to PCA that supports non-linear reduction while preserving both local and global structure. Not currently installed in Docker/Vagrant, but available under umap-learn (conda install -c conda-forge umap-learn). See examples on umap.scikit-tda.org."
  },
  {
    "objectID": "lectures/8.3-Dimensionality.html#gotcha",
    "href": "lectures/8.3-Dimensionality.html#gotcha",
    "title": "Dimensionality",
    "section": "Gotcha!",
    "text": "Gotcha!\nBoth t-SNE and UMAP require very careful handling:\n\nHyperparameters matter\nCluster size means nothing\nCluster distances mean nothing\nClusters may mean nothing (low neighbour count/perplexity)\nOutputs are stochastic (not deterministic)\n\nBoth likely require repeated testing and experimentation."
  },
  {
    "objectID": "lectures/8.3-Dimensionality.html#other-approaches",
    "href": "lectures/8.3-Dimensionality.html#other-approaches",
    "title": "Dimensionality",
    "section": "Other Approaches",
    "text": "Other Approaches\n\nFeature selection, including forwards/backwards (sklearn.feature_selection here)\nDecomposition (sklearn.decomposition here)\nOther types of manifold learning (sklearn.manifold here)\nRandom projection (sklearn.random_projection here)\nSupport Vector Machines (sklearn.svm here)\nEnsemble Methods (such as Random Forests: sklearn.ensemble.ExtraTreesClassifier and sklearn.ensemble.ExtraTreesRegressor here and here)"
  },
  {
    "objectID": "lectures/8.3-Dimensionality.html#resources",
    "href": "lectures/8.3-Dimensionality.html#resources",
    "title": "Dimensionality",
    "section": "Resources",
    "text": "Resources\n\nRethinking ‘distance’ in New York City Medium URL\nFive Boroughs for the 21\\[^{st}\\] Century Medium URL\nCurse of Dimensionality\nThe Curse of Dimensionality\nImportance of Feature Scaling\nUnderstanding PCA\nIntroduction to t-SNE in Python\nHow to Use t-SNE Effectively\nHow to tune the Hyperparameters of t-SNE\nUnderstanding UMAP (Compares to t-SNE)\nHow UMAP Works\n3 New Techniques for Data-Dimensionality Reduction in ML\nUMAP for Dimensionality Reduction (Video)\nA Bluffer’s Guide to Dimensionality Reduction (Video)"
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#sjoin-vs.-join",
    "href": "lectures/9.2-Linking_Spatial_Data.html#sjoin-vs.-join",
    "title": "Linking Spatial Data",
    "section": "Sjoin vs. Join",
    "text": "Sjoin vs. Join\nSjoin adds an operator (['intersects','contains','within']) and example code can be found on GitHub.\ngdf = gpd.GeoDataFrame(df, \n            geometry=gpd.points_from_xy(df.longitude, df.latitude,\n            crs='epsg:4326')).to_crs('epsg:27700')\nhackney = boros[boros.NAME=='Hackney']\nrs = gpd.sjoin(gdf, hackney, op='within')"
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#combining-operators-how",
    "href": "lectures/9.2-Linking_Spatial_Data.html#combining-operators-how",
    "title": "Linking Spatial Data",
    "section": "Combining Operators & How",
    "text": "Combining Operators & How\nChanging how to left, right, or inner changes the join’s behaviour:\nrs = gpd.sjoin(gdf, hackney, how='left', op='within')\nrs.NAME.fillna('None', inplace=True)\nax = boros[boros.NAME=='Hackney'].plot(edgecolor='k', facecolor='none', figsize=(8,8))\nrs.plot(ax=ax, column='NAME', legend=True)"
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#merging-data",
    "href": "lectures/9.2-Linking_Spatial_Data.html#merging-data",
    "title": "Linking Spatial Data",
    "section": "Merging Data",
    "text": "Merging Data\nThese merge operators apply where a is the left set of features (in a GeoSeries or GeoDataFrame) and b is the right set:\n\nContains: Returns True if no points of b lie outside of a and at least one point of b lies inside a.\nWithin: Returns True if a’s boundary and interior intersect only with the interior of b (not its boundary or exterior).\nIntersects: Returns True if the boundary or interior of a intersects in any way with those of b.\n\nAll binary predicates are supported by features of GeoPandas, though only these three options are available in sjoin directly.\n\nBehaviour of operaions may vary with how you set up left and right tables, but you can probably think your way through it by asking: “Which features of x fall within features of y?” or “Do features of x contain y?” You will probably get it wrong at least a few times. That’s ok."
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#additional-spatial-operations",
    "href": "lectures/9.2-Linking_Spatial_Data.html#additional-spatial-operations",
    "title": "Linking Spatial Data",
    "section": "Additional Spatial Operations",
    "text": "Additional Spatial Operations\nThese operators apply to the GeoSeries where a is a GeoSeries and b is one or more spatial features:\n\nContains / Covers: Returns a Series of dtype('bool') with value True for each geometry in a that contains b. These are different.\nCrosses: An object is said to cross other if its interior intersects the interior of the other but does not contain it, and the dimension of the intersection is less than the dimension of the one or the other.\nTouches: Returns a Series indicating which elements of a touch a point on b.\nDistance: Returns a Series containing the distance from all a to some b.\nDisjoint: Returns a Series indicating which elements of a do not intersect with any b.\nGeom Equals / Geom Almost Equals: strict and loose tests of equality between a and b in terms of their geometry.\nBuffer, Simplify, Centroid, Representative Point: common transformations.\nRotate, Scale, Affine Transform, Skew, Translate: less common transformations.\nUnary Union: aggregation of all the geometries in a"
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#rtm",
    "href": "lectures/9.2-Linking_Spatial_Data.html#rtm",
    "title": "Linking Spatial Data",
    "section": "RTM",
    "text": "RTM\n\nIn particular, “contains” (and its converse “within”) has an aspect of its definition which may produce unexpected behaviour. This quirk can be expressed as “Polygons do not contain their boundary”. More precisely, the definition of contains is: Geometry A contains Geometry B iff no points of B lie in the exterior of A, and at least one point of the interior of B lies in the interior of A That last clause causes the trap – because of it, a LineString which is completely contained in the boundary of a Polygon is not considered to be contained in that Polygon! This behaviour could easily trip up someone who is simply trying to find all LineStrings which have no points outside a given Polygon. In fact, this is probably the most common usage of contains. For this reason it’s useful to define another predicate called covers, which has the intuitively expected semantics: Geometry A covers Geometry B iff no points of B lie in the exterior of A"
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#set-operations-with-overlay",
    "href": "lectures/9.2-Linking_Spatial_Data.html#set-operations-with-overlay",
    "title": "Linking Spatial Data",
    "section": "Set Operations with Overlay",
    "text": "Set Operations with Overlay\nIt is also possible to apply GIS-type ‘overlay’ operations:\n\nThese operations return indexes for gdf1 and gdf2 (either could be a NaN) together with a geometry and (usually?) columns from both data frames:\nrs_union = geopandas.overlay(gdf1, gdf2, how='union')\nThe set of operations includes: union, intersection, difference, symmetric_difference, and identity.\n\nA notebook example can be found here."
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#think-it-through",
    "href": "lectures/9.2-Linking_Spatial_Data.html#think-it-through",
    "title": "Linking Spatial Data",
    "section": "Think it Through",
    "text": "Think it Through\nAs your data grows in volume, the consequences of choosing the ‘wrong’ approach become more severe. Making a plan of attack becomes essential and it boils down to the following:\n\nSpatial joins are hard\nNon-spatial joins are easy\nKey-/Index-based joins are easiest\nAddition conditions to joins makes them harder.\n\nSo, when you have multiple joins…\n\nDo the easy ones first (they will run quickly on large data sets).\nDo the hard ones last (they will benefit most from the filtering process)."
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#whats-the-right-order",
    "href": "lectures/9.2-Linking_Spatial_Data.html#whats-the-right-order",
    "title": "Linking Spatial Data",
    "section": "What’s the ‘Right’ Order?",
    "text": "What’s the ‘Right’ Order?\nQ. Find me a nearby family-style Italian restaurant…\n{{Org-Chart}} - City = New York - Cuisine = Italian - Style = Family - Location = Within Neighbourhood"
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#mis-matched-scales",
    "href": "lectures/9.2-Linking_Spatial_Data.html#mis-matched-scales",
    "title": "Linking Spatial Data",
    "section": "Mis-matched Scales",
    "text": "Mis-matched Scales\nKeep in mind:\n\nWith complex geometries and mis-matched scales, converting the smaller geometry to centroids or representative points can speed things up a lot (within, contains, etc. become much simpler).\n\nAnd that:\n\nWith large data sets, rasterising the smaller and more ‘abundant’ geometry can speed things up a lot."
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#long-term-view",
    "href": "lectures/9.2-Linking_Spatial_Data.html#long-term-view",
    "title": "Linking Spatial Data",
    "section": "Long-Term View",
    "text": "Long-Term View\n\nUltimately, if you continue to work with large spatial data sets you’ll need to look into web services and spatial databases."
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#web-services",
    "href": "lectures/9.2-Linking_Spatial_Data.html#web-services",
    "title": "Linking Spatial Data",
    "section": "Web Services",
    "text": "Web Services\n\n\n\n\n\n\n\n\nAcronym\nMeans\nDoes\n\n\n\n\nWMS\nWeb Map Service\nTransfers map images within an area specified by bounding box; vector formats possible, but rarely used.\n\n\nWFS\nWeb Feature Service\nAllows interaction with features; so not about rendering maps directly, more about manipulation.\n\n\nWCS\nWeb Coverage Service\nTransfers data about objects covering a geographical area.\n\n\nOWS\nOpen Web Services\nSeems to be used by QGIS to serve data from a PC or server."
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#spatial-databases",
    "href": "lectures/9.2-Linking_Spatial_Data.html#spatial-databases",
    "title": "Linking Spatial Data",
    "section": "Spatial Databases",
    "text": "Spatial Databases\nThere are many flavours:\n\nOracle: good enterprise support; reasonably feature-rich.\nMySQL: free; was feature-poor (though this looks to have changed with MySQL8).\nPostreSQL: free; standards-setting/compliant; heavyweight (requires PostGIS).\nMicrosoft Access: um, no.\nSpatiaLite: free; standards-setting/compliant; lightweight\n\nGenerally:\n\nAd-hoc, modestly-sized, highly portable == SpatiaLite\nPermanent, large, weakly portable == Postgres+PostGIS"
  },
  {
    "objectID": "lectures/9.3-Grouping_Data.html#grouping-operations",
    "href": "lectures/9.3-Grouping_Data.html#grouping-operations",
    "title": "Grouping Data",
    "section": "Grouping Operations",
    "text": "Grouping Operations\nIn Pandas these follow a split / apply / combine approach:\n\n\nNote that, for simplicity, I’ve abbreviate the Local Authority names since this is just a simplified example: TH (Tower Hamlets), HAK (Hackney), W (Westminster)."
  },
  {
    "objectID": "lectures/9.3-Grouping_Data.html#in-practice",
    "href": "lectures/9.3-Grouping_Data.html#in-practice",
    "title": "Grouping Data",
    "section": "In Practice",
    "text": "In Practice\ngrouped_df = df.groupby(<fields>).<function>\nFor instance, if we had a Local Authority (LA) field:\ngrouped_df = df.groupby('LA').sum()\nUsing apply the function could be anything:\ndef norm_by_data(x): # x is a column from the grouped df\n    x['d1'] /= x['d2'].sum() \n    return x\n\ndf.groupby('LA').apply(norm_by_data)"
  },
  {
    "objectID": "lectures/9.3-Grouping_Data.html#grouping-by-arbitrary-mappings",
    "href": "lectures/9.3-Grouping_Data.html#grouping-by-arbitrary-mappings",
    "title": "Grouping Data",
    "section": "Grouping by Arbitrary Mappings",
    "text": "Grouping by Arbitrary Mappings\nmapping = {'HAK':'Inner', 'TH':'Outer', 'W':'Inner'}\ndf.set_index('LA', inplace=True)\ndf.groupby(mapping).sum()"
  },
  {
    "objectID": "lectures/9.3-Grouping_Data.html#pivot-tables",
    "href": "lectures/9.3-Grouping_Data.html#pivot-tables",
    "title": "Grouping Data",
    "section": "Pivot Tables",
    "text": "Pivot Tables\nA ‘special case’ of Group By features:\n\nCommonly-used in business to summarise data for reporting.\nGrouping (summarisation) happens along both axes (Group By operates only on one).\npandas.cut(<series>, <bins>) can be a useful feature here since it chops a continuous feature into bins suitable for grouping."
  },
  {
    "objectID": "lectures/9.3-Grouping_Data.html#in-practice-1",
    "href": "lectures/9.3-Grouping_Data.html#in-practice-1",
    "title": "Grouping Data",
    "section": "In Practice",
    "text": "In Practice\nage = pd.cut(titanic['age'], [0, 18, 80])\ntitanic.pivot_table('survived', ['sex', age], 'class')"
  },
  {
    "objectID": "lectures/9.3-Grouping_Data.html#deriving-measures-of-diversity",
    "href": "lectures/9.3-Grouping_Data.html#deriving-measures-of-diversity",
    "title": "Grouping Data",
    "section": "Deriving Measures of Diversity",
    "text": "Deriving Measures of Diversity\n\nOne of the benefits of grouping is that it enables us to derive measures of density and diversity; here are just a few… Location Quotient (LQ), Herfindah-Hirschman Index (HHI), Shanon Entropy.\n\n\nI like easy measures."
  },
  {
    "objectID": "lectures/9.3-Grouping_Data.html#location-quotient",
    "href": "lectures/9.3-Grouping_Data.html#location-quotient",
    "title": "Grouping Data",
    "section": "Location Quotient",
    "text": "Location Quotient\nThe LQ for industry i in zone z is the share of employment for i in z divided by the share of employment of i in the entire region R.\n\\[\nLQ_{zi} = \\dfrac{Emp_{zi}/Emp_{z}}{Emp_{Ri}/Emp_{R}}\n\\]\n\n\n\n \nHigh Local Share\nLow Local Share\n\n\n\n\nHigh Regional Share\n\\[\\approx 1\\]\n\\[< 1\\]\n\n\nLow Regional Share\n\\[> 1\\]\n\\[\\approx 1\\]\n\n\n\n\nIn other words, this is a type of standardisation that enables to compare the concentration of Investment Bankers with the concentration of Accountants, even if there are many more Accountants than Bankers! But this can also apply to the share of flats to whole-property lettings just as easily.\nNote that this is influenced by small sample sizes (e.g. the number of Fijians in Britain)."
  },
  {
    "objectID": "lectures/9.3-Grouping_Data.html#herfindahl-hirschman-index",
    "href": "lectures/9.3-Grouping_Data.html#herfindahl-hirschman-index",
    "title": "Grouping Data",
    "section": "Herfindahl-Hirschman index",
    "text": "Herfindahl-Hirschman index\nThe HHI for an industry i is the sum of squared market shares for each company in that industry:\n\\[\nH = \\sum_{i=1}^{N} s_{i}^{2}\n\\]\n\n\n\n\n\n\n\nConcentration Level\nHHI\n\n\n\n\nMonopolistic: one firm accounts for 100% of the market\n\\[1.0\\]\n\n\nOligopolistic: top five firms account for 60% of the market\n\\[\\approx 0.8\\]\n\n\nCompetitive: anything else?\n\\[< 0.5\\]?\n\n\n\n\nIf \\(s_{i} = 1\\) then \\(s_{i}^{2} = 1\\), while if \\(s_{i} = 0.5\\) then \\(s_{i}^{2} = 0.25\\) and \\(s_{i} = 0.1\\) then \\(s_{i}^{2} = 0.01\\).\nThis can be translated to compare, for instance, local and regional neighbourhood diversity: some cities are ethnically diverse in aggregate but highly segregated at the local level.\nNote that this is influenced by the number of ‘firms’ (or ethnicities or…)."
  },
  {
    "objectID": "lectures/9.3-Grouping_Data.html#shannon-entropy",
    "href": "lectures/9.3-Grouping_Data.html#shannon-entropy",
    "title": "Grouping Data",
    "section": "Shannon Entropy",
    "text": "Shannon Entropy\nShannon Entropy is an information-theoretic measure:\n\\[\nH(X) = - \\sum_{i=1}^{n} P(x_{i}) log P(x_{i})\n\\]\n\nI often think of this as ‘surprise’: a high entropy measure means that it’s hard to predict what will happen next. So randomness has high entropy. By extension, high concentration has low entropy (even if the result is surprising on the level of intuition: I wasn’t expecting to see that) because I can predict a 6 on the next roll of the dice fairly easy if all of my previous rolls were 6s."
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#git",
    "href": "lectures/1.3-Writing_Code.html#git",
    "title": "Writing Code",
    "section": "Git",
    "text": "Git\n\n\n\n\nVersion control allows us to:\n\nTrack changes to files with a high level of detail using commit.\npush these changes out to others.\npull down changes made by others.\nmerge and resolve conflicting changes.\nCreate a tag when a ‘milestones’ is reached.\nCreate a branch to add a feature.\nRetrieve specific versions or branches with a checkout."
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#github",
    "href": "lectures/1.3-Writing_Code.html#github",
    "title": "Writing Code",
    "section": "GitHub",
    "text": "GitHub\n\n\n\n\nGit is distributed, meaning that every computer is a potential server and a potential authority. Result: commits on a plane!\nBut how do people find and access your code if your ‘server’ is a home machine that goes to sleep at night? Result: GitHub.\nGitHub is ‘just’ a very large Git server with a lot of nice web-friendly features tacked on: create a web site, issue/bug tracking, promote your project…"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#additional-stylings",
    "href": "lectures/1.3-Writing_Code.html#additional-stylings",
    "title": "Writing Code",
    "section": "Additional Stylings",
    "text": "Additional Stylings\nFor those who know how to do it, you can also insert bits of real HTML and CSS (the ‘languages’ of web sites) as well.\n\nThis content has HTML formatting attached."
  },
  {
    "objectID": "lectures/2.1-Computers_in_Planning_and_Geography.html#data-science",
    "href": "lectures/2.1-Computers_in_Planning_and_Geography.html#data-science",
    "title": "Computers in Urban Studies",
    "section": "Data science?",
    "text": "Data science?\n\n\n\n\n\nIndustry-led\nSpatially ignorant (often)\nDisciplinarily greedy (often)"
  },
  {
    "objectID": "lectures/2.1-Computers_in_Planning_and_Geography.html#do-we-need-it",
    "href": "lectures/2.1-Computers_in_Planning_and_Geography.html#do-we-need-it",
    "title": "Computers in Urban Studies",
    "section": "Do We Need It?",
    "text": "Do We Need It?\n\n\n\nImage Source\n\nMore readings:\n\nO’Sullivan, D. and Manson, S.M. (2015) ‘Do physicists have geography envy? And what can geographers learn from it?’ Annals of the Association of American Geographers 105(4), DOI.\nScheider, S. and Nyamsuren, E. and Kruiger, H. and Xu, H. (2020) ‘Why georgaphic data science is not a science’, Geography Compass, DOI."
  }
]